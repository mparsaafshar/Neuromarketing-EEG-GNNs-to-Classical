{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9c6d87f-6ba2-4211-8585-9ba0df2b0280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from collections import Counter, defaultdict\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import (\n",
    "    GCNConv, GATConv, SAGEConv, GINConv,\n",
    "    global_mean_pool, global_max_pool, global_add_pool,\n",
    "    BatchNorm, LayerNorm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5191d128-a26b-45b0-8559-a333eca1a7d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       User  Page  Product  Bought  Electrode  Delta_Freq_1_FFT  \\\n",
      "0       S01     1        1       0          1        243.347126   \n",
      "1       S01     1        1       0          2        143.530647   \n",
      "2       S01     1        1       0          3        447.828155   \n",
      "3       S01     1        1       0          4        309.990625   \n",
      "4       S01     1        1       0          5         63.902792   \n",
      "...     ...   ...      ...     ...        ...               ...   \n",
      "112798  S44     6       24       0         15        192.445131   \n",
      "112799  S44     6       24       0         16        335.046856   \n",
      "112800  S44     6       24       0         17        403.756075   \n",
      "112801  S44     6       24       0         18         77.173027   \n",
      "112802  S44     6       24       0         19        125.191184   \n",
      "\n",
      "        Delta_Freq_1_PSD  Delta_Freq_2_FFT  Delta_Freq_2_PSD  \\\n",
      "0               1.375451        339.230580          1.291068   \n",
      "1               1.042604        333.600483          0.998860   \n",
      "2               3.812356        565.809115          3.337711   \n",
      "3               2.634113        504.127161          2.423049   \n",
      "4               1.776532        469.687793          1.677785   \n",
      "...                  ...               ...               ...   \n",
      "112798          1.222907        368.849864          1.082050   \n",
      "112799          2.191933        174.517866          2.051061   \n",
      "112800          5.470390        947.160027          5.055428   \n",
      "112801          1.925451        542.943799          1.845546   \n",
      "112802          2.121603        620.185277          2.154112   \n",
      "\n",
      "        Delta_Freq_3_FFT  ...  Gamma_Freq_41_PSD  Gamma_Freq_42_FFT  \\\n",
      "0             290.640450  ...           0.070590          54.517292   \n",
      "1             340.722568  ...           0.063099          52.852911   \n",
      "2             334.295466  ...           0.023202          27.811198   \n",
      "3             425.288820  ...           0.062490          14.241363   \n",
      "4             257.207099  ...           0.028001          14.985783   \n",
      "...                  ...  ...                ...                ...   \n",
      "112798        136.843275  ...           0.077479          49.779673   \n",
      "112799        547.072951  ...           0.142289         122.124689   \n",
      "112800        524.412079  ...           0.295327          95.353275   \n",
      "112801        310.543880  ...           0.107889         113.767225   \n",
      "112802        497.559088  ...           0.220713         175.001639   \n",
      "\n",
      "        Gamma_Freq_42_PSD  Gamma_Freq_43_FFT  Gamma_Freq_43_PSD  \\\n",
      "0                0.045636          13.747916           0.027914   \n",
      "1                0.041248          36.004803           0.024188   \n",
      "2                0.016476          25.976422           0.011473   \n",
      "3                0.039762          21.582407           0.022614   \n",
      "4                0.020053          19.824593           0.013198   \n",
      "...                   ...                ...                ...   \n",
      "112798           0.071274          39.016204           0.062780   \n",
      "112799           0.096789          41.904498           0.060926   \n",
      "112800           0.226786          96.607878           0.164588   \n",
      "112801           0.096013          31.741418           0.083360   \n",
      "112802           0.163987          90.370184           0.113471   \n",
      "\n",
      "        Gamma_Freq_44_FFT  Gamma_Freq_44_PSD  Gamma_Freq_45_FFT  \\\n",
      "0               13.973555           0.016752          29.535577   \n",
      "1                8.891116           0.012889           8.827381   \n",
      "2                4.544432           0.007999          23.212594   \n",
      "3               21.892419           0.011924          22.816483   \n",
      "4               28.884477           0.008007           8.409101   \n",
      "...                   ...                ...                ...   \n",
      "112798          74.812754           0.051129          21.169627   \n",
      "112799          34.530746           0.036427          45.827510   \n",
      "112800         116.651726           0.112049          80.397066   \n",
      "112801          65.914902           0.068172          41.546237   \n",
      "112802         107.321579           0.072302          25.080758   \n",
      "\n",
      "        Gamma_Freq_45_PSD  Unnamed: 103  \n",
      "0                0.010021           NaN  \n",
      "1                0.006376           NaN  \n",
      "2                0.005542           NaN  \n",
      "3                0.006237           NaN  \n",
      "4                0.004537           NaN  \n",
      "...                   ...           ...  \n",
      "112798           0.037269           NaN  \n",
      "112799           0.021316           NaN  \n",
      "112800           0.070663           NaN  \n",
      "112801           0.050590           NaN  \n",
      "112802           0.042051           NaN  \n",
      "\n",
      "[112803 rows x 104 columns]\n"
     ]
    }
   ],
   "source": [
    "EEG_Dataset = pd.read_csv(r\"EEG_Full_Spectrum_Features_All_Users.csv\")\n",
    "print(EEG_Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4befbdf9-6674-4aeb-922c-a2fe2f8216d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User                 0\n",
      "Page                 0\n",
      "Product              0\n",
      "Bought               0\n",
      "Electrode            0\n",
      "                    ..\n",
      "Gamma_Freq_43_PSD    0\n",
      "Gamma_Freq_44_FFT    0\n",
      "Gamma_Freq_44_PSD    0\n",
      "Gamma_Freq_45_FFT    0\n",
      "Gamma_Freq_45_PSD    0\n",
      "Length: 103, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "EEG_Dataset = EEG_Dataset.drop(columns=['Unnamed: 103'])\n",
    "print(EEG_Dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c25237f-de4c-4b86-95cc-dd5941a964d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       User  Page  Product  Bought  Electrode  Delta_Freq_1_FFT  \\\n",
      "0       S01     1        1       0          1        243.347126   \n",
      "1       S01     1        1       0          2        143.530647   \n",
      "2       S01     1        1       0          3        447.828155   \n",
      "3       S01     1        1       0          4        309.990625   \n",
      "4       S01     1        1       0          5         63.902792   \n",
      "...     ...   ...      ...     ...        ...               ...   \n",
      "112798  S44     6       24       0         15        192.445131   \n",
      "112799  S44     6       24       0         16        335.046856   \n",
      "112800  S44     6       24       0         17        403.756075   \n",
      "112801  S44     6       24       0         18         77.173027   \n",
      "112802  S44     6       24       0         19        125.191184   \n",
      "\n",
      "        Delta_Freq_1_PSD  Delta_Freq_2_FFT  Delta_Freq_2_PSD  \\\n",
      "0               1.375451        339.230580          1.291068   \n",
      "1               1.042604        333.600483          0.998860   \n",
      "2               3.812356        565.809115          3.337711   \n",
      "3               2.634113        504.127161          2.423049   \n",
      "4               1.776532        469.687793          1.677785   \n",
      "...                  ...               ...               ...   \n",
      "112798          1.222907        368.849864          1.082050   \n",
      "112799          2.191933        174.517866          2.051061   \n",
      "112800          5.470390        947.160027          5.055428   \n",
      "112801          1.925451        542.943799          1.845546   \n",
      "112802          2.121603        620.185277          2.154112   \n",
      "\n",
      "        Delta_Freq_3_FFT  ...  Gamma_Freq_41_PSD  Gamma_Freq_42_FFT  \\\n",
      "0             290.640450  ...           0.070590          54.517292   \n",
      "1             340.722568  ...           0.063099          52.852911   \n",
      "2             334.295466  ...           0.023202          27.811198   \n",
      "3             425.288820  ...           0.062490          14.241363   \n",
      "4             257.207099  ...           0.028001          14.985783   \n",
      "...                  ...  ...                ...                ...   \n",
      "112798        136.843275  ...           0.077479          49.779673   \n",
      "112799        547.072951  ...           0.142289         122.124689   \n",
      "112800        524.412079  ...           0.295327          95.353275   \n",
      "112801        310.543880  ...           0.107889         113.767225   \n",
      "112802        497.559088  ...           0.220713         175.001639   \n",
      "\n",
      "        Gamma_Freq_42_PSD  Gamma_Freq_43_FFT  Gamma_Freq_43_PSD  \\\n",
      "0                0.045636          13.747916           0.027914   \n",
      "1                0.041248          36.004803           0.024188   \n",
      "2                0.016476          25.976422           0.011473   \n",
      "3                0.039762          21.582407           0.022614   \n",
      "4                0.020053          19.824593           0.013198   \n",
      "...                   ...                ...                ...   \n",
      "112798           0.071274          39.016204           0.062780   \n",
      "112799           0.096789          41.904498           0.060926   \n",
      "112800           0.226786          96.607878           0.164588   \n",
      "112801           0.096013          31.741418           0.083360   \n",
      "112802           0.163987          90.370184           0.113471   \n",
      "\n",
      "        Gamma_Freq_44_FFT  Gamma_Freq_44_PSD  Gamma_Freq_45_FFT  \\\n",
      "0               13.973555           0.016752          29.535577   \n",
      "1                8.891116           0.012889           8.827381   \n",
      "2                4.544432           0.007999          23.212594   \n",
      "3               21.892419           0.011924          22.816483   \n",
      "4               28.884477           0.008007           8.409101   \n",
      "...                   ...                ...                ...   \n",
      "112798          74.812754           0.051129          21.169627   \n",
      "112799          34.530746           0.036427          45.827510   \n",
      "112800         116.651726           0.112049          80.397066   \n",
      "112801          65.914902           0.068172          41.546237   \n",
      "112802         107.321579           0.072302          25.080758   \n",
      "\n",
      "        Gamma_Freq_45_PSD  product_page  \n",
      "0                0.010021            11  \n",
      "1                0.006376            11  \n",
      "2                0.005542            11  \n",
      "3                0.006237            11  \n",
      "4                0.004537            11  \n",
      "...                   ...           ...  \n",
      "112798           0.037269           624  \n",
      "112799           0.021316           624  \n",
      "112800           0.070663           624  \n",
      "112801           0.050590           624  \n",
      "112802           0.042051           624  \n",
      "\n",
      "[112803 rows x 104 columns]\n"
     ]
    }
   ],
   "source": [
    "EEG_Dataset['product_page'] = EEG_Dataset['Page'].astype(str) + EEG_Dataset['Product'].astype(str)\n",
    "EEG_Dataset['product_page'] = EEG_Dataset['product_page'].astype(int)\n",
    "print(EEG_Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "101c2730-b774-4d4a-b81a-78ee1f04f764",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       User  product_page  Bought  Electrode  Delta_Freq_1_FFT  \\\n",
      "0       S01            11       0          1        243.347126   \n",
      "1       S01            11       0          2        143.530647   \n",
      "2       S01            11       0          3        447.828155   \n",
      "3       S01            11       0          4        309.990625   \n",
      "4       S01            11       0          5         63.902792   \n",
      "...     ...           ...     ...        ...               ...   \n",
      "112798  S44           624       0         15        192.445131   \n",
      "112799  S44           624       0         16        335.046856   \n",
      "112800  S44           624       0         17        403.756075   \n",
      "112801  S44           624       0         18         77.173027   \n",
      "112802  S44           624       0         19        125.191184   \n",
      "\n",
      "        Delta_Freq_1_PSD  Delta_Freq_2_FFT  Delta_Freq_2_PSD  \\\n",
      "0               1.375451        339.230580          1.291068   \n",
      "1               1.042604        333.600483          0.998860   \n",
      "2               3.812356        565.809115          3.337711   \n",
      "3               2.634113        504.127161          2.423049   \n",
      "4               1.776532        469.687793          1.677785   \n",
      "...                  ...               ...               ...   \n",
      "112798          1.222907        368.849864          1.082050   \n",
      "112799          2.191933        174.517866          2.051061   \n",
      "112800          5.470390        947.160027          5.055428   \n",
      "112801          1.925451        542.943799          1.845546   \n",
      "112802          2.121603        620.185277          2.154112   \n",
      "\n",
      "        Delta_Freq_3_FFT  Delta_Freq_3_PSD  ...  Gamma_Freq_41_FFT  \\\n",
      "0             290.640450          1.123420  ...          19.614381   \n",
      "1             340.722568          0.892034  ...          46.664184   \n",
      "2             334.295466          2.646345  ...          14.534089   \n",
      "3             425.288820          2.061939  ...          62.202099   \n",
      "4             257.207099          1.481409  ...          63.336172   \n",
      "...                  ...               ...  ...                ...   \n",
      "112798        136.843275          0.869743  ...          57.650508   \n",
      "112799        547.072951          1.783646  ...          41.017416   \n",
      "112800        524.412079          4.303470  ...         166.906364   \n",
      "112801        310.543880          1.647436  ...          91.776491   \n",
      "112802        497.559088          2.048103  ...          45.020932   \n",
      "\n",
      "        Gamma_Freq_41_PSD  Gamma_Freq_42_FFT  Gamma_Freq_42_PSD  \\\n",
      "0                0.070590          54.517292           0.045636   \n",
      "1                0.063099          52.852911           0.041248   \n",
      "2                0.023202          27.811198           0.016476   \n",
      "3                0.062490          14.241363           0.039762   \n",
      "4                0.028001          14.985783           0.020053   \n",
      "...                   ...                ...                ...   \n",
      "112798           0.077479          49.779673           0.071274   \n",
      "112799           0.142289         122.124689           0.096789   \n",
      "112800           0.295327          95.353275           0.226786   \n",
      "112801           0.107889         113.767225           0.096013   \n",
      "112802           0.220713         175.001639           0.163987   \n",
      "\n",
      "        Gamma_Freq_43_FFT  Gamma_Freq_43_PSD  Gamma_Freq_44_FFT  \\\n",
      "0               13.747916           0.027914          13.973555   \n",
      "1               36.004803           0.024188           8.891116   \n",
      "2               25.976422           0.011473           4.544432   \n",
      "3               21.582407           0.022614          21.892419   \n",
      "4               19.824593           0.013198          28.884477   \n",
      "...                   ...                ...                ...   \n",
      "112798          39.016204           0.062780          74.812754   \n",
      "112799          41.904498           0.060926          34.530746   \n",
      "112800          96.607878           0.164588         116.651726   \n",
      "112801          31.741418           0.083360          65.914902   \n",
      "112802          90.370184           0.113471         107.321579   \n",
      "\n",
      "        Gamma_Freq_44_PSD  Gamma_Freq_45_FFT  Gamma_Freq_45_PSD  \n",
      "0                0.016752          29.535577           0.010021  \n",
      "1                0.012889           8.827381           0.006376  \n",
      "2                0.007999          23.212594           0.005542  \n",
      "3                0.011924          22.816483           0.006237  \n",
      "4                0.008007           8.409101           0.004537  \n",
      "...                   ...                ...                ...  \n",
      "112798           0.051129          21.169627           0.037269  \n",
      "112799           0.036427          45.827510           0.021316  \n",
      "112800           0.112049          80.397066           0.070663  \n",
      "112801           0.068172          41.546237           0.050590  \n",
      "112802           0.072302          25.080758           0.042051  \n",
      "\n",
      "[112803 rows x 102 columns]\n"
     ]
    }
   ],
   "source": [
    "EEG_Dataset = EEG_Dataset.drop(columns=['Product', 'Page'])\n",
    "EEG_Dataset = EEG_Dataset[[EEG_Dataset.columns[0], 'product_page'] + EEG_Dataset.columns.drop(['product_page', EEG_Dataset.columns[0]]).tolist()]\n",
    "print(EEG_Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9641f579-35c8-429f-96b5-09f4398dc674",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     User  product_page  Bought  Alpha_Freq_10_FFT_Electrode_1  \\\n",
      "0     S01            11       0                     389.347719   \n",
      "1     S01            12       0                      17.319813   \n",
      "2     S01            13       0                     102.179924   \n",
      "3     S01            14       1                      50.225631   \n",
      "4     S01            15       0                      10.605075   \n",
      "...   ...           ...     ...                            ...   \n",
      "3247  S44           619       0                      61.262399   \n",
      "3248  S44           620       1                      19.770211   \n",
      "3249  S44           621       0                      38.829805   \n",
      "3250  S44           622       0                     172.130770   \n",
      "3251  S44           624       0                      25.977570   \n",
      "\n",
      "      Alpha_Freq_10_FFT_Electrode_2  Alpha_Freq_10_FFT_Electrode_3  \\\n",
      "0                        291.331736                     271.866244   \n",
      "1                          7.056958                      10.668018   \n",
      "2                        139.136277                     107.440642   \n",
      "3                          7.532174                       8.082533   \n",
      "4                         28.317680                      14.632045   \n",
      "...                             ...                            ...   \n",
      "3247                      41.008277                     117.278997   \n",
      "3248                      36.214037                      80.340693   \n",
      "3249                      87.969452                      98.757701   \n",
      "3250                     149.260883                      40.752011   \n",
      "3251                     107.755856                      24.512074   \n",
      "\n",
      "      Alpha_Freq_10_FFT_Electrode_4  Alpha_Freq_10_FFT_Electrode_5  \\\n",
      "0                        370.012184                     294.588292   \n",
      "1                         14.129221                      22.184167   \n",
      "2                         67.700625                     154.911032   \n",
      "3                         29.022488                      59.281816   \n",
      "4                         11.895777                      35.467673   \n",
      "...                             ...                            ...   \n",
      "3247                     197.373995                     171.325135   \n",
      "3248                      55.252712                      67.875373   \n",
      "3249                      98.820924                     138.793317   \n",
      "3250                      62.755765                     115.188726   \n",
      "3251                     135.271138                     160.428656   \n",
      "\n",
      "      Alpha_Freq_10_FFT_Electrode_6  Alpha_Freq_10_FFT_Electrode_7  ...  \\\n",
      "0                        163.023913                     275.125584  ...   \n",
      "1                         12.976338                      21.951340  ...   \n",
      "2                         71.797798                      82.641631  ...   \n",
      "3                         58.328360                      53.434501  ...   \n",
      "4                         18.639482                      23.225534  ...   \n",
      "...                             ...                            ...  ...   \n",
      "3247                      93.489057                     120.467622  ...   \n",
      "3248                     111.810745                      73.313722  ...   \n",
      "3249                      46.763453                      55.693246  ...   \n",
      "3250                     138.543294                     102.702133  ...   \n",
      "3251                     193.280485                      37.066653  ...   \n",
      "\n",
      "      Theta_Freq_8_PSD_Electrode_10  Theta_Freq_8_PSD_Electrode_11  \\\n",
      "0                          0.858478                       0.800493   \n",
      "1                          0.105492                       0.378344   \n",
      "2                          1.021204                       1.066130   \n",
      "3                          0.508811                       0.466100   \n",
      "4                          0.169072                       0.188846   \n",
      "...                             ...                            ...   \n",
      "3247                       0.990330                       0.429119   \n",
      "3248                       0.214363                       0.227323   \n",
      "3249                       6.321928                       6.309916   \n",
      "3250                       0.292498                       0.454693   \n",
      "3251                       0.797224                       0.588905   \n",
      "\n",
      "      Theta_Freq_8_PSD_Electrode_12  Theta_Freq_8_PSD_Electrode_13  \\\n",
      "0                          0.382462                       0.471643   \n",
      "1                          1.103498                       0.730954   \n",
      "2                          0.233821                       0.313982   \n",
      "3                          0.570128                       1.472353   \n",
      "4                          0.541452                       0.433949   \n",
      "...                             ...                            ...   \n",
      "3247                       0.837105                       0.271660   \n",
      "3248                       0.635669                       0.269850   \n",
      "3249                       0.145342                       0.101781   \n",
      "3250                       0.433617                       0.924500   \n",
      "3251                       0.632495                       0.383105   \n",
      "\n",
      "      Theta_Freq_8_PSD_Electrode_14  Theta_Freq_8_PSD_Electrode_15  \\\n",
      "0                          0.522279                       0.871569   \n",
      "1                          1.533291                       2.569766   \n",
      "2                          0.407564                       1.123489   \n",
      "3                          2.988221                       2.222447   \n",
      "4                          1.281652                       1.235423   \n",
      "...                             ...                            ...   \n",
      "3247                       0.230976                       0.227350   \n",
      "3248                       0.447332                       0.754279   \n",
      "3249                       0.043929                       0.166678   \n",
      "3250                       0.273253                       0.531934   \n",
      "3251                       0.274692                       0.224294   \n",
      "\n",
      "      Theta_Freq_8_PSD_Electrode_16  Theta_Freq_8_PSD_Electrode_17  \\\n",
      "0                          0.655299                       0.467571   \n",
      "1                          0.194050                       0.558652   \n",
      "2                          0.810551                       0.694516   \n",
      "3                          0.258775                       0.592465   \n",
      "4                          0.723745                       0.242880   \n",
      "...                             ...                            ...   \n",
      "3247                       1.416618                       0.310928   \n",
      "3248                       0.367608                       0.077309   \n",
      "3249                       2.180129                       3.729330   \n",
      "3250                       0.249450                       0.540119   \n",
      "3251                       1.003437                       0.764837   \n",
      "\n",
      "      Theta_Freq_8_PSD_Electrode_18  Theta_Freq_8_PSD_Electrode_19  \n",
      "0                          0.468216                       0.254179  \n",
      "1                          0.486367                       0.229822  \n",
      "2                          0.598272                       0.151814  \n",
      "3                          1.613849                       0.468974  \n",
      "4                          0.946793                       0.209386  \n",
      "...                             ...                            ...  \n",
      "3247                       0.890001                       0.138706  \n",
      "3248                       0.803914                       0.140373  \n",
      "3249                       0.083023                       0.166684  \n",
      "3250                       0.575125                       0.266264  \n",
      "3251                       0.190247                       0.365994  \n",
      "\n",
      "[3252 rows x 1865 columns]\n"
     ]
    }
   ],
   "source": [
    "electrode_columns = [col for col in EEG_Dataset.columns if col not in ['User', 'product_page', 'Bought', 'Electrode']]\n",
    "\n",
    "df_pivoted = EEG_Dataset.pivot_table(index=['User', 'product_page', 'Bought'], columns='Electrode', values=electrode_columns, aggfunc='first')\n",
    "\n",
    "df_pivoted.columns = [f\"{col[0]}_Electrode_{col[1]}\" for col in df_pivoted.columns]\n",
    "\n",
    "df_pivoted = df_pivoted.reset_index()\n",
    "\n",
    "print(df_pivoted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0754f7c8-203b-45fb-a111-5f8b25661c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivoted.to_csv(r'EEG_Full_Spectrum_Features_All_Users1_pivoted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fdbae94-c436-4131-87ed-844497f69181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>product_page</th>\n",
       "      <th>Bought</th>\n",
       "      <th>Alpha_Freq_10_FFT_Electrode_1</th>\n",
       "      <th>Alpha_Freq_10_FFT_Electrode_2</th>\n",
       "      <th>Alpha_Freq_10_FFT_Electrode_3</th>\n",
       "      <th>Alpha_Freq_10_FFT_Electrode_4</th>\n",
       "      <th>Alpha_Freq_10_FFT_Electrode_5</th>\n",
       "      <th>Alpha_Freq_10_FFT_Electrode_6</th>\n",
       "      <th>Alpha_Freq_10_FFT_Electrode_7</th>\n",
       "      <th>...</th>\n",
       "      <th>Theta_Freq_8_PSD_Electrode_10</th>\n",
       "      <th>Theta_Freq_8_PSD_Electrode_11</th>\n",
       "      <th>Theta_Freq_8_PSD_Electrode_12</th>\n",
       "      <th>Theta_Freq_8_PSD_Electrode_13</th>\n",
       "      <th>Theta_Freq_8_PSD_Electrode_14</th>\n",
       "      <th>Theta_Freq_8_PSD_Electrode_15</th>\n",
       "      <th>Theta_Freq_8_PSD_Electrode_16</th>\n",
       "      <th>Theta_Freq_8_PSD_Electrode_17</th>\n",
       "      <th>Theta_Freq_8_PSD_Electrode_18</th>\n",
       "      <th>Theta_Freq_8_PSD_Electrode_19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S01</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>389.347719</td>\n",
       "      <td>291.331736</td>\n",
       "      <td>271.866244</td>\n",
       "      <td>370.012184</td>\n",
       "      <td>294.588292</td>\n",
       "      <td>163.023913</td>\n",
       "      <td>275.125584</td>\n",
       "      <td>...</td>\n",
       "      <td>0.858478</td>\n",
       "      <td>0.800493</td>\n",
       "      <td>0.382462</td>\n",
       "      <td>0.471643</td>\n",
       "      <td>0.522279</td>\n",
       "      <td>0.871569</td>\n",
       "      <td>0.655299</td>\n",
       "      <td>0.467571</td>\n",
       "      <td>0.468216</td>\n",
       "      <td>0.254179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S01</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>17.319813</td>\n",
       "      <td>7.056958</td>\n",
       "      <td>10.668018</td>\n",
       "      <td>14.129221</td>\n",
       "      <td>22.184167</td>\n",
       "      <td>12.976338</td>\n",
       "      <td>21.951340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105492</td>\n",
       "      <td>0.378344</td>\n",
       "      <td>1.103498</td>\n",
       "      <td>0.730954</td>\n",
       "      <td>1.533291</td>\n",
       "      <td>2.569766</td>\n",
       "      <td>0.194050</td>\n",
       "      <td>0.558652</td>\n",
       "      <td>0.486367</td>\n",
       "      <td>0.229822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S01</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>102.179924</td>\n",
       "      <td>139.136277</td>\n",
       "      <td>107.440642</td>\n",
       "      <td>67.700625</td>\n",
       "      <td>154.911032</td>\n",
       "      <td>71.797798</td>\n",
       "      <td>82.641631</td>\n",
       "      <td>...</td>\n",
       "      <td>1.021204</td>\n",
       "      <td>1.066130</td>\n",
       "      <td>0.233821</td>\n",
       "      <td>0.313982</td>\n",
       "      <td>0.407564</td>\n",
       "      <td>1.123489</td>\n",
       "      <td>0.810551</td>\n",
       "      <td>0.694516</td>\n",
       "      <td>0.598272</td>\n",
       "      <td>0.151814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S01</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>50.225631</td>\n",
       "      <td>7.532174</td>\n",
       "      <td>8.082533</td>\n",
       "      <td>29.022488</td>\n",
       "      <td>59.281816</td>\n",
       "      <td>58.328360</td>\n",
       "      <td>53.434501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.508811</td>\n",
       "      <td>0.466100</td>\n",
       "      <td>0.570128</td>\n",
       "      <td>1.472353</td>\n",
       "      <td>2.988221</td>\n",
       "      <td>2.222447</td>\n",
       "      <td>0.258775</td>\n",
       "      <td>0.592465</td>\n",
       "      <td>1.613849</td>\n",
       "      <td>0.468974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S01</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>10.605075</td>\n",
       "      <td>28.317680</td>\n",
       "      <td>14.632045</td>\n",
       "      <td>11.895777</td>\n",
       "      <td>35.467673</td>\n",
       "      <td>18.639482</td>\n",
       "      <td>23.225534</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169072</td>\n",
       "      <td>0.188846</td>\n",
       "      <td>0.541452</td>\n",
       "      <td>0.433949</td>\n",
       "      <td>1.281652</td>\n",
       "      <td>1.235423</td>\n",
       "      <td>0.723745</td>\n",
       "      <td>0.242880</td>\n",
       "      <td>0.946793</td>\n",
       "      <td>0.209386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1865 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  User  product_page  Bought  Alpha_Freq_10_FFT_Electrode_1  \\\n",
       "0  S01            11       0                     389.347719   \n",
       "1  S01            12       0                      17.319813   \n",
       "2  S01            13       0                     102.179924   \n",
       "3  S01            14       1                      50.225631   \n",
       "4  S01            15       0                      10.605075   \n",
       "\n",
       "   Alpha_Freq_10_FFT_Electrode_2  Alpha_Freq_10_FFT_Electrode_3  \\\n",
       "0                     291.331736                     271.866244   \n",
       "1                       7.056958                      10.668018   \n",
       "2                     139.136277                     107.440642   \n",
       "3                       7.532174                       8.082533   \n",
       "4                      28.317680                      14.632045   \n",
       "\n",
       "   Alpha_Freq_10_FFT_Electrode_4  Alpha_Freq_10_FFT_Electrode_5  \\\n",
       "0                     370.012184                     294.588292   \n",
       "1                      14.129221                      22.184167   \n",
       "2                      67.700625                     154.911032   \n",
       "3                      29.022488                      59.281816   \n",
       "4                      11.895777                      35.467673   \n",
       "\n",
       "   Alpha_Freq_10_FFT_Electrode_6  Alpha_Freq_10_FFT_Electrode_7  ...  \\\n",
       "0                     163.023913                     275.125584  ...   \n",
       "1                      12.976338                      21.951340  ...   \n",
       "2                      71.797798                      82.641631  ...   \n",
       "3                      58.328360                      53.434501  ...   \n",
       "4                      18.639482                      23.225534  ...   \n",
       "\n",
       "   Theta_Freq_8_PSD_Electrode_10  Theta_Freq_8_PSD_Electrode_11  \\\n",
       "0                       0.858478                       0.800493   \n",
       "1                       0.105492                       0.378344   \n",
       "2                       1.021204                       1.066130   \n",
       "3                       0.508811                       0.466100   \n",
       "4                       0.169072                       0.188846   \n",
       "\n",
       "   Theta_Freq_8_PSD_Electrode_12  Theta_Freq_8_PSD_Electrode_13  \\\n",
       "0                       0.382462                       0.471643   \n",
       "1                       1.103498                       0.730954   \n",
       "2                       0.233821                       0.313982   \n",
       "3                       0.570128                       1.472353   \n",
       "4                       0.541452                       0.433949   \n",
       "\n",
       "   Theta_Freq_8_PSD_Electrode_14  Theta_Freq_8_PSD_Electrode_15  \\\n",
       "0                       0.522279                       0.871569   \n",
       "1                       1.533291                       2.569766   \n",
       "2                       0.407564                       1.123489   \n",
       "3                       2.988221                       2.222447   \n",
       "4                       1.281652                       1.235423   \n",
       "\n",
       "   Theta_Freq_8_PSD_Electrode_16  Theta_Freq_8_PSD_Electrode_17  \\\n",
       "0                       0.655299                       0.467571   \n",
       "1                       0.194050                       0.558652   \n",
       "2                       0.810551                       0.694516   \n",
       "3                       0.258775                       0.592465   \n",
       "4                       0.723745                       0.242880   \n",
       "\n",
       "   Theta_Freq_8_PSD_Electrode_18  Theta_Freq_8_PSD_Electrode_19  \n",
       "0                       0.468216                       0.254179  \n",
       "1                       0.486367                       0.229822  \n",
       "2                       0.598272                       0.151814  \n",
       "3                       1.613849                       0.468974  \n",
       "4                       0.946793                       0.209386  \n",
       "\n",
       "[5 rows x 1865 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pivoted.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04638995-85e7-406c-b499-6904018b018c",
   "metadata": {},
   "source": [
    "Graph Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "720d3197-325e-433b-9fae-0e2e3d2ff05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n",
    "        super(BaselineGCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.lin = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return self.lin(x)\n",
    "\n",
    "class BaselineGAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_heads=4, dropout=0.5):\n",
    "        super(BaselineGAT, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=num_heads, concat=True)\n",
    "        self.conv2 = GATConv(hidden_channels * num_heads, hidden_channels, heads=1, concat=True)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.lin = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return self.lin(x)\n",
    "\n",
    "class BaselineSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n",
    "        super(BaselineSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.lin = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return self.lin(x)\n",
    "\n",
    "class ResidualGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n",
    "        super(ResidualGCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.skip_proj = nn.Linear(in_channels, hidden_channels) if in_channels != hidden_channels else nn.Identity()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.lin = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # First GCN layer\n",
    "        h1 = F.relu(self.conv1(x, edge_index))\n",
    "        h1 = self.dropout(h1)\n",
    "        skip = self.skip_proj(x)\n",
    "        h2 = self.conv2(h1, edge_index)\n",
    "        h2 = self.dropout(h2)\n",
    "        h = F.relu(h2 + skip)\n",
    "        h = global_mean_pool(h, batch)\n",
    "        return self.lin(h)\n",
    "\n",
    "class HybridModel(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_heads=4, dropout=0.5):\n",
    "        super(HybridModel, self).__init__()\n",
    "        self.mlp1 = nn.Linear(in_channels, hidden_channels)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.gcn1 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.gat = GATConv(hidden_channels, hidden_channels, heads=num_heads, concat=True)\n",
    "        self.gcn2 = GCNConv(hidden_channels * num_heads, hidden_channels)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.mlp2 = nn.Linear(hidden_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.bn1(self.mlp1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.gcn1(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = F.elu(self.gat(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.gcn2(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.bn2(self.mlp2(x))\n",
    "        return x\n",
    "\n",
    "        \n",
    "class RegularizedGNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n",
    "        super(RegularizedGNN, self).__init__()\n",
    "        self.input_dropout = nn.Dropout(dropout)\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.norm1 = LayerNorm(hidden_channels)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels, heads=1)\n",
    "        self.norm2 = LayerNorm(hidden_channels)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels // 2, out_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.input_dropout(x)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.norm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class LightweightGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n",
    "        super(LightweightGCN, self).__init__()\n",
    "        self.conv = GCNConv(in_channels, hidden_channels)\n",
    "        self.norm = LayerNorm(hidden_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv(x, edge_index)\n",
    "        x = self.norm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class BalancedGAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n",
    "        super(BalancedGAT, self).__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(in_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.conv = GATConv(hidden_channels, hidden_channels, heads=1)\n",
    "        self.norm = LayerNorm(hidden_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.proj(x)\n",
    "        x = self.conv(x, edge_index)\n",
    "        x = self.norm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class MultiGNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_heads=4, dropout=0.3):\n",
    "        super(MultiGNN, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.gcn = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.gat = GATConv(hidden_channels, hidden_channels // num_heads, heads=num_heads)\n",
    "        self.sage = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.norm1 = LayerNorm(hidden_channels)\n",
    "        self.norm2 = LayerNorm(hidden_channels)\n",
    "        self.norm3 = LayerNorm(hidden_channels)\n",
    "        self.skip_proj = nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_channels * 3, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels, hidden_channels // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels // 2, out_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.encoder(x)\n",
    "        gcn_out = self.gcn(x, edge_index)\n",
    "        gcn_out = self.norm1(gcn_out)\n",
    "        gcn_out = F.relu(gcn_out)\n",
    "        gat_out = self.gat(x, edge_index)\n",
    "        gat_out = self.norm2(gat_out)\n",
    "        gat_out = F.relu(gat_out)\n",
    "        sage_out = self.sage(x, edge_index)\n",
    "        sage_out = self.norm3(sage_out)\n",
    "        sage_out = F.relu(sage_out)\n",
    "        skip = self.skip_proj(x)\n",
    "        gcn_out = gcn_out + skip\n",
    "        gat_out = gat_out + skip\n",
    "        sage_out = sage_out + skip\n",
    "        gcn_pooled = global_mean_pool(gcn_out, batch)\n",
    "        gat_pooled = global_max_pool(gat_out, batch)\n",
    "        sage_pooled = global_add_pool(sage_out, batch)\n",
    "        combined = torch.cat([gcn_pooled, gat_pooled, sage_pooled], dim=1)\n",
    "        out = self.classifier(combined)\n",
    "        return out\n",
    "\n",
    "class ResidualAttentionGNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_heads=4, dropout=0.3):\n",
    "        super(ResidualAttentionGNN, self).__init__()\n",
    "        self.input_transform = nn.Sequential(\n",
    "            nn.Linear(in_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.gat1 = GATConv(hidden_channels, hidden_channels // num_heads, heads=num_heads)\n",
    "        self.gat2 = GATConv(hidden_channels, hidden_channels // num_heads, heads=num_heads)\n",
    "        self.norm1 = LayerNorm(hidden_channels)\n",
    "        self.norm2 = LayerNorm(hidden_channels)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_channels // 2, 1)\n",
    "        )\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels, hidden_channels // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels // 2, out_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        h = self.input_transform(x)\n",
    "        h1 = self.gat1(h, edge_index)\n",
    "        h1 = self.norm1(h1)\n",
    "        h1 = F.relu(h1)\n",
    "        h1 = h1 + h  \n",
    "        h2 = self.gat2(h1, edge_index)\n",
    "        h2 = self.norm2(h2)\n",
    "        h2 = F.relu(h2)\n",
    "        h2 = h2 + h1  \n",
    "        attention_weights = self.attention(h2)\n",
    "        attention_weights = F.softmax(attention_weights, dim=0)\n",
    "        h2 = h2 * attention_weights\n",
    "        pooled = global_mean_pool(h2, batch)\n",
    "        out = self.output(pooled)\n",
    "        return out\n",
    "\n",
    "class DeepGNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3, dropout=0.3):\n",
    "        super(DeepGNN, self).__init__()\n",
    "        self.input_proj = nn.Linear(in_channels, hidden_channels)\n",
    "        self.gnn_layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.skip_connections = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            self.gnn_layers.append(nn.ModuleList([\n",
    "                GCNConv(hidden_channels, hidden_channels),\n",
    "                GATConv(hidden_channels, hidden_channels // 4, heads=4),\n",
    "                SAGEConv(hidden_channels, hidden_channels)\n",
    "            ]))\n",
    "            self.norms.append(nn.ModuleList([\n",
    "                LayerNorm(hidden_channels),\n",
    "                LayerNorm(hidden_channels),\n",
    "                LayerNorm(hidden_channels)\n",
    "            ]))\n",
    "            if i > 0:\n",
    "                self.skip_connections.append(nn.Linear(hidden_channels, hidden_channels))\n",
    "        \n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels, hidden_channels // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels // 2, out_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        h = self.input_proj(x)\n",
    "        \n",
    "        for i, (gnn_layer, norm_layer) in enumerate(zip(self.gnn_layers, self.norms)):\n",
    "            gcn_out = gnn_layer[0](h, edge_index)\n",
    "            gat_out = gnn_layer[1](h, edge_index)\n",
    "            sage_out = gnn_layer[2](h, edge_index)\n",
    "            \n",
    "            gcn_out = F.relu(norm_layer[0](gcn_out))\n",
    "            gat_out = F.relu(norm_layer[1](gat_out))\n",
    "            sage_out = F.relu(norm_layer[2](sage_out))\n",
    "            \n",
    "            h_new = (gcn_out + gat_out + sage_out) / 3\n",
    "            \n",
    "            if i > 0:\n",
    "                h_new = h_new + self.skip_connections[i-1](h)\n",
    "            \n",
    "            h = h_new\n",
    "        \n",
    "        pooled = global_mean_pool(h, batch)\n",
    "        out = self.output(pooled)\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb696d54-e11d-4e5d-86bd-0448ecc405b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading Data ===\n",
      "Using device: cuda\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF ALL MODELS - MAJORITY VOTE RESULTS\n",
      "================================================================================\n",
      "\n",
      "=== Testing BaselineGCN Model ===\n",
      "\n",
      "===== BaselineGCN - Fold 1 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=5.0702, Acc=0.5351 | Val Loss=1.5738, Acc=0.4146\n",
      "Epoch 02: Train Loss=1.2840, Acc=0.5298 | Val Loss=0.9197, Acc=0.6334\n",
      "Epoch 03: Train Loss=0.8571, Acc=0.5764 | Val Loss=0.7472, Acc=0.6449\n",
      "Epoch 04: Train Loss=0.7218, Acc=0.6332 | Val Loss=0.6998, Acc=0.6104\n",
      "Epoch 05: Train Loss=0.6950, Acc=0.6077 | Val Loss=0.6772, Acc=0.6910\n",
      "Epoch 06: Train Loss=0.6798, Acc=0.6394 | Val Loss=0.6740, Acc=0.6641\n",
      "Epoch 07: Train Loss=0.6745, Acc=0.6505 | Val Loss=0.6915, Acc=0.7063\n",
      "Epoch 08: Train Loss=0.6782, Acc=0.6692 | Val Loss=0.6955, Acc=0.5605\n",
      "Epoch 09: Train Loss=0.6758, Acc=0.6606 | Val Loss=0.6884, Acc=0.6814\n",
      "Epoch 10: Train Loss=0.6790, Acc=0.6659 | Val Loss=0.6906, Acc=0.6679\n",
      "Epoch 11: Train Loss=0.6661, Acc=0.6885 | Val Loss=0.6900, Acc=0.7351\n",
      "Epoch 12: Train Loss=0.6666, Acc=0.6841 | Val Loss=0.6706, Acc=0.6756\n",
      "Epoch 13: Train Loss=0.6648, Acc=0.6505 | Val Loss=0.6765, Acc=0.6142\n",
      "Epoch 14: Train Loss=0.6611, Acc=0.6745 | Val Loss=0.6801, Acc=0.7121\n",
      "Epoch 15: Train Loss=0.6616, Acc=0.6750 | Val Loss=0.7018, Acc=0.6718\n",
      "Epoch 16: Train Loss=0.6603, Acc=0.6870 | Val Loss=0.6833, Acc=0.5720\n",
      "Epoch 17: Train Loss=0.6609, Acc=0.6644 | Val Loss=0.6826, Acc=0.7198\n",
      "Epoch 18: Train Loss=0.6492, Acc=0.6716 | Val Loss=0.6759, Acc=0.6564\n",
      "Epoch 19: Train Loss=0.6508, Acc=0.6841 | Val Loss=0.6865, Acc=0.6046\n",
      "Epoch 20: Train Loss=0.6519, Acc=0.6870 | Val Loss=0.7077, Acc=0.6641\n",
      "Epoch 21: Train Loss=0.6466, Acc=0.6769 | Val Loss=0.6831, Acc=0.6660\n",
      "Epoch 22: Train Loss=0.6483, Acc=0.6760 | Val Loss=0.6802, Acc=0.6545\n",
      "Epoch 23: Train Loss=0.6483, Acc=0.6827 | Val Loss=0.6776, Acc=0.6008\n",
      "Epoch 24: Train Loss=0.6409, Acc=0.6471 | Val Loss=0.6893, Acc=0.6392\n",
      "Epoch 25: Train Loss=0.6450, Acc=0.6731 | Val Loss=0.6858, Acc=0.6488\n",
      "Epoch 26: Train Loss=0.6477, Acc=0.6918 | Val Loss=0.6856, Acc=0.6468\n",
      "Epoch 27: Train Loss=0.6367, Acc=0.6837 | Val Loss=0.6849, Acc=0.6699\n",
      "Early stopping triggered after 27 epochs\n",
      "\n",
      "Fold 1 Test Metrics:\n",
      "  Accuracy:  0.6375\n",
      "  Precision: 0.7086\n",
      "  Recall:    0.6375\n",
      "  F1 Score:  0.6642\n",
      "\n",
      "Fold 1 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8237    0.6893    0.7505       515\n",
      "           1     0.2727    0.4412    0.3371       136\n",
      "\n",
      "    accuracy                         0.6375       651\n",
      "   macro avg     0.5482    0.5652    0.5438       651\n",
      "weighted avg     0.7086    0.6375    0.6642       651\n",
      "\n",
      "\n",
      "===== BaselineGCN - Fold 2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=5.6403, Acc=0.5615 | Val Loss=1.6359, Acc=0.6327\n",
      "Epoch 02: Train Loss=1.6237, Acc=0.5817 | Val Loss=0.9230, Acc=0.5308\n",
      "Epoch 03: Train Loss=0.9117, Acc=0.5659 | Val Loss=0.7067, Acc=0.5500\n",
      "Epoch 04: Train Loss=0.7451, Acc=0.5808 | Val Loss=0.7023, Acc=0.7173\n",
      "Epoch 05: Train Loss=0.6840, Acc=0.6279 | Val Loss=0.6842, Acc=0.6942\n",
      "Epoch 06: Train Loss=0.6997, Acc=0.6562 | Val Loss=0.6803, Acc=0.6538\n",
      "Epoch 07: Train Loss=0.6869, Acc=0.6404 | Val Loss=0.6838, Acc=0.7692\n",
      "Epoch 08: Train Loss=0.6748, Acc=0.6611 | Val Loss=0.6909, Acc=0.5635\n",
      "Epoch 09: Train Loss=0.6785, Acc=0.6813 | Val Loss=0.6847, Acc=0.5481\n",
      "Epoch 10: Train Loss=0.6722, Acc=0.6361 | Val Loss=0.6766, Acc=0.6365\n",
      "Epoch 11: Train Loss=0.6663, Acc=0.6755 | Val Loss=0.6812, Acc=0.6692\n",
      "Epoch 12: Train Loss=0.6708, Acc=0.6697 | Val Loss=0.7080, Acc=0.6250\n",
      "Epoch 13: Train Loss=0.6623, Acc=0.6981 | Val Loss=0.6885, Acc=0.6308\n",
      "Epoch 14: Train Loss=0.6573, Acc=0.6837 | Val Loss=0.6784, Acc=0.6596\n",
      "Epoch 15: Train Loss=0.6556, Acc=0.7192 | Val Loss=0.6840, Acc=0.6385\n",
      "Epoch 16: Train Loss=0.6547, Acc=0.6865 | Val Loss=0.6812, Acc=0.6731\n",
      "Epoch 17: Train Loss=0.6512, Acc=0.7029 | Val Loss=0.6819, Acc=0.7212\n",
      "Epoch 18: Train Loss=0.6547, Acc=0.7106 | Val Loss=0.6859, Acc=0.6654\n",
      "Epoch 19: Train Loss=0.6426, Acc=0.7111 | Val Loss=0.7100, Acc=0.6981\n",
      "Epoch 20: Train Loss=0.6527, Acc=0.7245 | Val Loss=0.6994, Acc=0.6885\n",
      "Epoch 21: Train Loss=0.6403, Acc=0.7139 | Val Loss=0.7006, Acc=0.6385\n",
      "Epoch 22: Train Loss=0.6381, Acc=0.6995 | Val Loss=0.7048, Acc=0.6808\n",
      "Epoch 23: Train Loss=0.6374, Acc=0.7240 | Val Loss=0.6961, Acc=0.6712\n",
      "Epoch 24: Train Loss=0.6385, Acc=0.6952 | Val Loss=0.6924, Acc=0.6865\n",
      "Epoch 25: Train Loss=0.6342, Acc=0.7317 | Val Loss=0.6980, Acc=0.6750\n",
      "Early stopping triggered after 25 epochs\n",
      "\n",
      "Fold 2 Test Metrics:\n",
      "  Accuracy:  0.6498\n",
      "  Precision: 0.7069\n",
      "  Recall:    0.6498\n",
      "  F1 Score:  0.6724\n",
      "\n",
      "Fold 2 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8210    0.7126    0.7630       515\n",
      "           1     0.2745    0.4118    0.3294       136\n",
      "\n",
      "    accuracy                         0.6498       651\n",
      "   macro avg     0.5478    0.5622    0.5462       651\n",
      "weighted avg     0.7069    0.6498    0.6724       651\n",
      "\n",
      "\n",
      "===== BaselineGCN - Fold 3 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=3.4372, Acc=0.5750 | Val Loss=1.6433, Acc=0.4712\n",
      "Epoch 02: Train Loss=1.3736, Acc=0.5813 | Val Loss=0.9175, Acc=0.6365\n",
      "Epoch 03: Train Loss=0.9076, Acc=0.5966 | Val Loss=0.7244, Acc=0.6635\n",
      "Epoch 04: Train Loss=0.7660, Acc=0.6476 | Val Loss=0.7036, Acc=0.6981\n",
      "Epoch 05: Train Loss=0.7039, Acc=0.6683 | Val Loss=0.6760, Acc=0.7365\n",
      "Epoch 06: Train Loss=0.6835, Acc=0.6683 | Val Loss=0.6590, Acc=0.6846\n",
      "Epoch 07: Train Loss=0.6741, Acc=0.6490 | Val Loss=0.6674, Acc=0.6231\n",
      "Epoch 08: Train Loss=0.6710, Acc=0.6433 | Val Loss=0.6600, Acc=0.7135\n",
      "Epoch 09: Train Loss=0.6709, Acc=0.6736 | Val Loss=0.6729, Acc=0.6231\n",
      "Epoch 10: Train Loss=0.6735, Acc=0.6510 | Val Loss=0.6767, Acc=0.6462\n",
      "Epoch 11: Train Loss=0.6685, Acc=0.6587 | Val Loss=0.6555, Acc=0.7558\n",
      "Epoch 12: Train Loss=0.6609, Acc=0.6976 | Val Loss=0.6679, Acc=0.6962\n",
      "Epoch 13: Train Loss=0.6595, Acc=0.7053 | Val Loss=0.6617, Acc=0.7346\n",
      "Epoch 14: Train Loss=0.6564, Acc=0.6923 | Val Loss=0.6962, Acc=0.6019\n",
      "Epoch 15: Train Loss=0.6681, Acc=0.6803 | Val Loss=0.6624, Acc=0.7308\n",
      "Epoch 16: Train Loss=0.6609, Acc=0.6687 | Val Loss=0.6544, Acc=0.6923\n",
      "Epoch 17: Train Loss=0.6594, Acc=0.7072 | Val Loss=0.6567, Acc=0.6712\n",
      "Epoch 18: Train Loss=0.6481, Acc=0.6861 | Val Loss=0.6544, Acc=0.6942\n",
      "Epoch 19: Train Loss=0.6511, Acc=0.6933 | Val Loss=0.6621, Acc=0.6596\n",
      "Epoch 20: Train Loss=0.6482, Acc=0.6904 | Val Loss=0.6690, Acc=0.7250\n",
      "Epoch 21: Train Loss=0.6549, Acc=0.7082 | Val Loss=0.6673, Acc=0.7154\n",
      "Epoch 22: Train Loss=0.6357, Acc=0.7183 | Val Loss=0.6710, Acc=0.6904\n",
      "Epoch 23: Train Loss=0.6447, Acc=0.6966 | Val Loss=0.6582, Acc=0.6712\n",
      "Epoch 24: Train Loss=0.6367, Acc=0.6923 | Val Loss=0.6660, Acc=0.7135\n",
      "Epoch 25: Train Loss=0.6337, Acc=0.7178 | Val Loss=0.6676, Acc=0.7115\n",
      "Epoch 26: Train Loss=0.6338, Acc=0.7058 | Val Loss=0.6724, Acc=0.7173\n",
      "Epoch 27: Train Loss=0.6332, Acc=0.7053 | Val Loss=0.6699, Acc=0.7154\n",
      "Epoch 28: Train Loss=0.6407, Acc=0.7091 | Val Loss=0.6760, Acc=0.7192\n",
      "Epoch 29: Train Loss=0.6306, Acc=0.7231 | Val Loss=0.6870, Acc=0.7192\n",
      "Epoch 30: Train Loss=0.6221, Acc=0.7231 | Val Loss=0.6855, Acc=0.7192\n",
      "Epoch 31: Train Loss=0.6265, Acc=0.7260 | Val Loss=0.6937, Acc=0.7192\n",
      "Epoch 32: Train Loss=0.6304, Acc=0.7173 | Val Loss=0.6844, Acc=0.7173\n",
      "Epoch 33: Train Loss=0.6200, Acc=0.7337 | Val Loss=0.6899, Acc=0.7231\n",
      "Early stopping triggered after 33 epochs\n",
      "\n",
      "Fold 3 Test Metrics:\n",
      "  Accuracy:  0.6790\n",
      "  Precision: 0.7091\n",
      "  Recall:    0.6790\n",
      "  F1 Score:  0.6922\n",
      "\n",
      "Fold 3 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8201    0.7612    0.7895       515\n",
      "           1     0.2890    0.3676    0.3236       136\n",
      "\n",
      "    accuracy                         0.6790       651\n",
      "   macro avg     0.5546    0.5644    0.5566       651\n",
      "weighted avg     0.7091    0.6790    0.6922       651\n",
      "\n",
      "\n",
      "===== BaselineGCN - Fold 4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=12.4381, Acc=0.4673 | Val Loss=1.2834, Acc=0.6846\n",
      "Epoch 02: Train Loss=1.7321, Acc=0.5495 | Val Loss=1.1381, Acc=0.6942\n",
      "Epoch 03: Train Loss=1.1092, Acc=0.5841 | Val Loss=0.8331, Acc=0.6308\n",
      "Epoch 04: Train Loss=0.8729, Acc=0.5615 | Val Loss=0.7679, Acc=0.4981\n",
      "Epoch 05: Train Loss=0.8024, Acc=0.5486 | Val Loss=0.7252, Acc=0.5058\n",
      "Epoch 06: Train Loss=0.7117, Acc=0.5923 | Val Loss=0.6892, Acc=0.6269\n",
      "Epoch 07: Train Loss=0.6980, Acc=0.6096 | Val Loss=0.6998, Acc=0.5442\n",
      "Epoch 08: Train Loss=0.6945, Acc=0.6173 | Val Loss=0.6950, Acc=0.5788\n",
      "Epoch 09: Train Loss=0.6614, Acc=0.6519 | Val Loss=0.6800, Acc=0.6346\n",
      "Epoch 10: Train Loss=0.6649, Acc=0.6293 | Val Loss=0.6773, Acc=0.6654\n",
      "Epoch 11: Train Loss=0.6644, Acc=0.6433 | Val Loss=0.6812, Acc=0.6750\n",
      "Epoch 12: Train Loss=0.6626, Acc=0.6394 | Val Loss=0.6819, Acc=0.6808\n",
      "Epoch 13: Train Loss=0.6627, Acc=0.6822 | Val Loss=0.6795, Acc=0.6673\n",
      "Epoch 14: Train Loss=0.6491, Acc=0.6779 | Val Loss=0.6756, Acc=0.6962\n",
      "Epoch 15: Train Loss=0.6530, Acc=0.6644 | Val Loss=0.6777, Acc=0.6154\n",
      "Epoch 16: Train Loss=0.6540, Acc=0.6764 | Val Loss=0.6795, Acc=0.6808\n",
      "Epoch 17: Train Loss=0.6517, Acc=0.6774 | Val Loss=0.6852, Acc=0.6712\n",
      "Epoch 18: Train Loss=0.6490, Acc=0.6813 | Val Loss=0.6800, Acc=0.6269\n",
      "Epoch 19: Train Loss=0.6448, Acc=0.6827 | Val Loss=0.6709, Acc=0.6865\n",
      "Epoch 20: Train Loss=0.6501, Acc=0.6846 | Val Loss=0.6868, Acc=0.6750\n",
      "Epoch 21: Train Loss=0.6476, Acc=0.7000 | Val Loss=0.6752, Acc=0.6577\n",
      "Epoch 22: Train Loss=0.6342, Acc=0.6918 | Val Loss=0.6797, Acc=0.6635\n",
      "Epoch 23: Train Loss=0.6296, Acc=0.7154 | Val Loss=0.6788, Acc=0.6750\n",
      "Epoch 24: Train Loss=0.6361, Acc=0.6918 | Val Loss=0.6718, Acc=0.6846\n",
      "Epoch 25: Train Loss=0.6420, Acc=0.7038 | Val Loss=0.6788, Acc=0.6769\n",
      "Epoch 26: Train Loss=0.6290, Acc=0.7038 | Val Loss=0.6741, Acc=0.6865\n",
      "Epoch 27: Train Loss=0.6297, Acc=0.7139 | Val Loss=0.6716, Acc=0.6904\n",
      "Epoch 28: Train Loss=0.6275, Acc=0.6962 | Val Loss=0.6719, Acc=0.7000\n",
      "Epoch 29: Train Loss=0.6230, Acc=0.7173 | Val Loss=0.6748, Acc=0.6635\n",
      "Epoch 30: Train Loss=0.6239, Acc=0.6923 | Val Loss=0.6734, Acc=0.6692\n",
      "Epoch 31: Train Loss=0.6183, Acc=0.6976 | Val Loss=0.6718, Acc=0.6923\n",
      "Epoch 32: Train Loss=0.6240, Acc=0.7063 | Val Loss=0.6745, Acc=0.6981\n",
      "Epoch 33: Train Loss=0.6173, Acc=0.7144 | Val Loss=0.6769, Acc=0.6558\n",
      "Epoch 34: Train Loss=0.6132, Acc=0.6937 | Val Loss=0.6753, Acc=0.6962\n",
      "Early stopping triggered after 34 epochs\n",
      "\n",
      "Fold 4 Test Metrics:\n",
      "  Accuracy:  0.6421\n",
      "  Precision: 0.7023\n",
      "  Recall:    0.6421\n",
      "  F1 Score:  0.6659\n",
      "\n",
      "Fold 4 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8176    0.7049    0.7570       515\n",
      "           1     0.2657    0.4044    0.3207       136\n",
      "\n",
      "    accuracy                         0.6421       651\n",
      "   macro avg     0.5416    0.5546    0.5389       651\n",
      "weighted avg     0.7023    0.6421    0.6659       651\n",
      "\n",
      "\n",
      "===== BaselineGCN - Fold 5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=6.1159, Acc=0.5188 | Val Loss=1.3972, Acc=0.5423\n",
      "Epoch 02: Train Loss=1.4892, Acc=0.5505 | Val Loss=0.9141, Acc=0.7269\n",
      "Epoch 03: Train Loss=0.9378, Acc=0.5885 | Val Loss=0.7536, Acc=0.6135\n",
      "Epoch 04: Train Loss=0.7515, Acc=0.5697 | Val Loss=0.7357, Acc=0.5808\n",
      "Epoch 05: Train Loss=0.7456, Acc=0.5538 | Val Loss=0.7250, Acc=0.4615\n",
      "Epoch 06: Train Loss=0.6943, Acc=0.5332 | Val Loss=0.7042, Acc=0.5788\n",
      "Epoch 07: Train Loss=0.6894, Acc=0.5538 | Val Loss=0.6923, Acc=0.5885\n",
      "Epoch 08: Train Loss=0.6880, Acc=0.5663 | Val Loss=0.7076, Acc=0.5558\n",
      "Epoch 09: Train Loss=0.6624, Acc=0.5822 | Val Loss=0.6985, Acc=0.5596\n",
      "Epoch 10: Train Loss=0.6566, Acc=0.5620 | Val Loss=0.7183, Acc=0.6000\n",
      "Epoch 11: Train Loss=0.6601, Acc=0.5750 | Val Loss=0.7030, Acc=0.5327\n",
      "Epoch 12: Train Loss=0.6561, Acc=0.5894 | Val Loss=0.7114, Acc=0.6250\n",
      "Epoch 13: Train Loss=0.6557, Acc=0.5947 | Val Loss=0.6993, Acc=0.5096\n",
      "Epoch 14: Train Loss=0.6577, Acc=0.5625 | Val Loss=0.7127, Acc=0.6192\n",
      "Epoch 15: Train Loss=0.6482, Acc=0.5841 | Val Loss=0.7085, Acc=0.5885\n",
      "Epoch 16: Train Loss=0.6448, Acc=0.5813 | Val Loss=0.7101, Acc=0.5827\n",
      "Epoch 17: Train Loss=0.6456, Acc=0.5894 | Val Loss=0.7284, Acc=0.6115\n",
      "Epoch 18: Train Loss=0.6486, Acc=0.5995 | Val Loss=0.7224, Acc=0.5827\n",
      "Epoch 19: Train Loss=0.6346, Acc=0.6014 | Val Loss=0.7228, Acc=0.6038\n",
      "Epoch 20: Train Loss=0.6428, Acc=0.6034 | Val Loss=0.7182, Acc=0.6019\n",
      "Epoch 21: Train Loss=0.6399, Acc=0.6029 | Val Loss=0.7202, Acc=0.6096\n",
      "Epoch 22: Train Loss=0.6285, Acc=0.6096 | Val Loss=0.7259, Acc=0.6115\n",
      "Early stopping triggered after 22 epochs\n",
      "\n",
      "Fold 5 Test Metrics:\n",
      "  Accuracy:  0.5684\n",
      "  Precision: 0.7160\n",
      "  Recall:    0.5684\n",
      "  F1 Score:  0.6087\n",
      "\n",
      "Fold 5 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8362    0.5650    0.6744       515\n",
      "           1     0.2607    0.5809    0.3599       136\n",
      "\n",
      "    accuracy                         0.5684       651\n",
      "   macro avg     0.5485    0.5730    0.5172       651\n",
      "weighted avg     0.7160    0.5684    0.6087       651\n",
      "\n",
      "\n",
      "===== Final Combined Results (Majority Vote) for BaselineGCN =====\n",
      "Final Accuracy:  0.6375\n",
      "Final Precision: 0.7037\n",
      "Final Recall:    0.6375\n",
      "Final F1 Score:  0.6630\n",
      "\n",
      "Final Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8192    0.6951    0.7521       515\n",
      "           1     0.2664    0.4191    0.3257       136\n",
      "\n",
      "    accuracy                         0.6375       651\n",
      "   macro avg     0.5428    0.5571    0.5389       651\n",
      "weighted avg     0.7037    0.6375    0.6630       651\n",
      "\n",
      "\n",
      "BaselineGCN - Final Majority Vote Results:\n",
      "  Accuracy:  0.6375\n",
      "  Precision: 0.7037\n",
      "  Recall:    0.6375\n",
      "  F1 Score:  0.6630\n",
      "\n",
      "=== Testing BaselineGAT Model ===\n",
      "\n",
      "===== BaselineGAT - Fold 1 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=13.3149, Acc=0.5649 | Val Loss=4.9287, Acc=0.7025\n",
      "Epoch 02: Train Loss=4.3486, Acc=0.5317 | Val Loss=2.3102, Acc=0.7524\n",
      "Epoch 03: Train Loss=1.7493, Acc=0.5457 | Val Loss=0.8125, Acc=0.4856\n",
      "Epoch 04: Train Loss=0.9717, Acc=0.5404 | Val Loss=1.0205, Acc=0.2246\n",
      "Epoch 05: Train Loss=0.8667, Acc=0.4995 | Val Loss=0.7158, Acc=0.7793\n",
      "Epoch 06: Train Loss=0.7899, Acc=0.5322 | Val Loss=0.7111, Acc=0.7274\n",
      "Epoch 07: Train Loss=0.7603, Acc=0.5813 | Val Loss=0.6993, Acc=0.4894\n",
      "Epoch 08: Train Loss=0.7208, Acc=0.5529 | Val Loss=0.7133, Acc=0.5144\n",
      "Epoch 09: Train Loss=0.7409, Acc=0.5740 | Val Loss=0.6842, Acc=0.7083\n",
      "Epoch 10: Train Loss=0.7350, Acc=0.6101 | Val Loss=0.6889, Acc=0.7179\n",
      "Epoch 11: Train Loss=0.7182, Acc=0.6106 | Val Loss=0.6747, Acc=0.7217\n",
      "Epoch 12: Train Loss=0.6894, Acc=0.6351 | Val Loss=0.6729, Acc=0.6622\n",
      "Epoch 13: Train Loss=0.6837, Acc=0.6495 | Val Loss=0.6794, Acc=0.6699\n",
      "Epoch 14: Train Loss=0.6918, Acc=0.6630 | Val Loss=0.6736, Acc=0.6583\n",
      "Epoch 15: Train Loss=0.6946, Acc=0.6356 | Val Loss=0.6677, Acc=0.6795\n",
      "Epoch 16: Train Loss=0.6974, Acc=0.6139 | Val Loss=0.6732, Acc=0.7294\n",
      "Epoch 17: Train Loss=0.6903, Acc=0.6510 | Val Loss=0.6781, Acc=0.7159\n",
      "Epoch 18: Train Loss=0.6736, Acc=0.6861 | Val Loss=0.6739, Acc=0.7063\n",
      "Epoch 19: Train Loss=0.6672, Acc=0.6899 | Val Loss=0.6739, Acc=0.7313\n",
      "Epoch 20: Train Loss=0.6799, Acc=0.6769 | Val Loss=0.6751, Acc=0.7332\n",
      "Epoch 21: Train Loss=0.6690, Acc=0.6692 | Val Loss=0.6745, Acc=0.6929\n",
      "Epoch 22: Train Loss=0.6709, Acc=0.6755 | Val Loss=0.6726, Acc=0.7274\n",
      "Epoch 23: Train Loss=0.6589, Acc=0.6885 | Val Loss=0.6715, Acc=0.7198\n",
      "Epoch 24: Train Loss=0.6603, Acc=0.7139 | Val Loss=0.6698, Acc=0.7255\n",
      "Epoch 25: Train Loss=0.6649, Acc=0.6909 | Val Loss=0.6691, Acc=0.6987\n",
      "Epoch 26: Train Loss=0.6665, Acc=0.6760 | Val Loss=0.6688, Acc=0.7274\n",
      "Epoch 27: Train Loss=0.6597, Acc=0.6909 | Val Loss=0.6690, Acc=0.7044\n",
      "Epoch 28: Train Loss=0.6624, Acc=0.6904 | Val Loss=0.6689, Acc=0.7121\n",
      "Epoch 29: Train Loss=0.6516, Acc=0.7159 | Val Loss=0.6713, Acc=0.7044\n",
      "Epoch 30: Train Loss=0.6506, Acc=0.6913 | Val Loss=0.6696, Acc=0.7140\n",
      "Early stopping triggered after 30 epochs\n",
      "\n",
      "Fold 1 Test Metrics:\n",
      "  Accuracy:  0.6851\n",
      "  Precision: 0.7039\n",
      "  Recall:    0.6851\n",
      "  F1 Score:  0.6938\n",
      "\n",
      "Fold 1 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8150    0.7786    0.7964       515\n",
      "           1     0.2830    0.3309    0.3051       136\n",
      "\n",
      "    accuracy                         0.6851       651\n",
      "   macro avg     0.5490    0.5548    0.5508       651\n",
      "weighted avg     0.7039    0.6851    0.6938       651\n",
      "\n",
      "\n",
      "===== BaselineGAT - Fold 2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=9.4037, Acc=0.5303 | Val Loss=2.6631, Acc=0.5365\n",
      "Epoch 02: Train Loss=3.4746, Acc=0.5553 | Val Loss=1.6569, Acc=0.3423\n",
      "Epoch 03: Train Loss=1.7996, Acc=0.5529 | Val Loss=0.8548, Acc=0.4385\n",
      "Epoch 04: Train Loss=0.8782, Acc=0.5596 | Val Loss=0.7112, Acc=0.4096\n",
      "Epoch 05: Train Loss=0.7716, Acc=0.5577 | Val Loss=0.7207, Acc=0.7481\n",
      "Epoch 06: Train Loss=0.7456, Acc=0.5957 | Val Loss=0.6994, Acc=0.7750\n",
      "Epoch 07: Train Loss=0.7083, Acc=0.6356 | Val Loss=0.7419, Acc=0.7846\n",
      "Epoch 08: Train Loss=0.7060, Acc=0.6601 | Val Loss=0.6884, Acc=0.7115\n",
      "Epoch 09: Train Loss=0.6889, Acc=0.6399 | Val Loss=0.6834, Acc=0.7327\n",
      "Epoch 10: Train Loss=0.6900, Acc=0.6548 | Val Loss=0.6836, Acc=0.6577\n",
      "Epoch 11: Train Loss=0.6948, Acc=0.6591 | Val Loss=0.6877, Acc=0.6942\n",
      "Epoch 12: Train Loss=0.6893, Acc=0.6726 | Val Loss=0.6922, Acc=0.7288\n",
      "Epoch 13: Train Loss=0.6879, Acc=0.6572 | Val Loss=0.6772, Acc=0.6038\n",
      "Epoch 14: Train Loss=0.6610, Acc=0.6663 | Val Loss=0.6675, Acc=0.6212\n",
      "Epoch 15: Train Loss=0.6576, Acc=0.6813 | Val Loss=0.6717, Acc=0.5808\n",
      "Epoch 16: Train Loss=0.6634, Acc=0.6784 | Val Loss=0.6893, Acc=0.6404\n",
      "Epoch 17: Train Loss=0.6637, Acc=0.6736 | Val Loss=0.6744, Acc=0.6827\n",
      "Epoch 18: Train Loss=0.6652, Acc=0.6841 | Val Loss=0.6752, Acc=0.6673\n",
      "Epoch 19: Train Loss=0.6573, Acc=0.6813 | Val Loss=0.6699, Acc=0.6769\n",
      "Epoch 20: Train Loss=0.6503, Acc=0.7139 | Val Loss=0.6792, Acc=0.6308\n",
      "Epoch 21: Train Loss=0.6466, Acc=0.6817 | Val Loss=0.6690, Acc=0.6904\n",
      "Epoch 22: Train Loss=0.6493, Acc=0.6875 | Val Loss=0.6689, Acc=0.6635\n",
      "Epoch 23: Train Loss=0.6373, Acc=0.6904 | Val Loss=0.6708, Acc=0.6519\n",
      "Epoch 24: Train Loss=0.6438, Acc=0.6779 | Val Loss=0.6691, Acc=0.6519\n",
      "Epoch 25: Train Loss=0.6440, Acc=0.6875 | Val Loss=0.6708, Acc=0.6731\n",
      "Epoch 26: Train Loss=0.6427, Acc=0.6769 | Val Loss=0.6706, Acc=0.6769\n",
      "Epoch 27: Train Loss=0.6383, Acc=0.7106 | Val Loss=0.6721, Acc=0.6692\n",
      "Epoch 28: Train Loss=0.6339, Acc=0.6736 | Val Loss=0.6726, Acc=0.6423\n",
      "Epoch 29: Train Loss=0.6387, Acc=0.6740 | Val Loss=0.6763, Acc=0.6404\n",
      "Early stopping triggered after 29 epochs\n",
      "\n",
      "Fold 2 Test Metrics:\n",
      "  Accuracy:  0.5899\n",
      "  Precision: 0.7077\n",
      "  Recall:    0.5899\n",
      "  F1 Score:  0.6270\n",
      "\n",
      "Fold 2 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8263    0.6097    0.7017       515\n",
      "           1     0.2583    0.5147    0.3440       136\n",
      "\n",
      "    accuracy                         0.5899       651\n",
      "   macro avg     0.5423    0.5622    0.5228       651\n",
      "weighted avg     0.7077    0.5899    0.6270       651\n",
      "\n",
      "\n",
      "===== BaselineGAT - Fold 3 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=12.8970, Acc=0.5582 | Val Loss=7.4752, Acc=0.2635\n",
      "Epoch 02: Train Loss=4.6171, Acc=0.5587 | Val Loss=1.7783, Acc=0.4077\n",
      "Epoch 03: Train Loss=2.1146, Acc=0.5298 | Val Loss=1.0326, Acc=0.3808\n",
      "Epoch 04: Train Loss=0.9655, Acc=0.5332 | Val Loss=0.8737, Acc=0.6269\n",
      "Epoch 05: Train Loss=0.7585, Acc=0.5510 | Val Loss=0.7869, Acc=0.2231\n",
      "Epoch 06: Train Loss=0.7159, Acc=0.5601 | Val Loss=0.6577, Acc=0.6596\n",
      "Epoch 07: Train Loss=0.6929, Acc=0.6034 | Val Loss=0.7031, Acc=0.6769\n",
      "Epoch 08: Train Loss=0.7063, Acc=0.6192 | Val Loss=0.6851, Acc=0.5827\n",
      "Epoch 09: Train Loss=0.6951, Acc=0.6293 | Val Loss=0.6771, Acc=0.6308\n",
      "Epoch 10: Train Loss=0.6798, Acc=0.5957 | Val Loss=0.6803, Acc=0.6923\n",
      "Epoch 11: Train Loss=0.6678, Acc=0.6019 | Val Loss=0.6876, Acc=0.7212\n",
      "Epoch 12: Train Loss=0.6847, Acc=0.5962 | Val Loss=0.6719, Acc=0.6346\n",
      "Epoch 13: Train Loss=0.6682, Acc=0.5846 | Val Loss=0.6661, Acc=0.5981\n",
      "Epoch 14: Train Loss=0.6723, Acc=0.6024 | Val Loss=0.6786, Acc=0.4288\n",
      "Epoch 15: Train Loss=0.6654, Acc=0.5649 | Val Loss=0.6871, Acc=0.4462\n",
      "Epoch 16: Train Loss=0.6707, Acc=0.5837 | Val Loss=0.6583, Acc=0.6308\n",
      "Epoch 17: Train Loss=0.6666, Acc=0.5894 | Val Loss=0.6747, Acc=0.4808\n",
      "Epoch 18: Train Loss=0.6564, Acc=0.6298 | Val Loss=0.6727, Acc=0.5635\n",
      "Epoch 19: Train Loss=0.6468, Acc=0.6125 | Val Loss=0.6914, Acc=0.4942\n",
      "Epoch 20: Train Loss=0.6485, Acc=0.6139 | Val Loss=0.6727, Acc=0.5731\n",
      "Epoch 21: Train Loss=0.6423, Acc=0.6159 | Val Loss=0.6450, Acc=0.5923\n",
      "Epoch 22: Train Loss=0.6500, Acc=0.5990 | Val Loss=0.6613, Acc=0.5385\n",
      "Epoch 23: Train Loss=0.6448, Acc=0.5966 | Val Loss=0.6684, Acc=0.5981\n",
      "Epoch 24: Train Loss=0.6333, Acc=0.6144 | Val Loss=0.6589, Acc=0.5923\n",
      "Epoch 25: Train Loss=0.6346, Acc=0.6207 | Val Loss=0.6669, Acc=0.5212\n",
      "Epoch 26: Train Loss=0.6331, Acc=0.5986 | Val Loss=0.6658, Acc=0.5231\n",
      "Epoch 27: Train Loss=0.6384, Acc=0.6125 | Val Loss=0.6614, Acc=0.5558\n",
      "Epoch 28: Train Loss=0.6344, Acc=0.6312 | Val Loss=0.6622, Acc=0.5962\n",
      "Epoch 29: Train Loss=0.6292, Acc=0.6111 | Val Loss=0.6585, Acc=0.5577\n",
      "Epoch 30: Train Loss=0.6212, Acc=0.6216 | Val Loss=0.6650, Acc=0.5038\n",
      "Epoch 31: Train Loss=0.6251, Acc=0.6130 | Val Loss=0.6667, Acc=0.4962\n",
      "Epoch 32: Train Loss=0.6209, Acc=0.6226 | Val Loss=0.6666, Acc=0.4942\n",
      "Epoch 33: Train Loss=0.6179, Acc=0.6365 | Val Loss=0.6722, Acc=0.4904\n",
      "Epoch 34: Train Loss=0.6274, Acc=0.6178 | Val Loss=0.6685, Acc=0.4904\n",
      "Epoch 35: Train Loss=0.6256, Acc=0.6404 | Val Loss=0.6658, Acc=0.4846\n",
      "Epoch 36: Train Loss=0.6160, Acc=0.6457 | Val Loss=0.6663, Acc=0.4827\n",
      "Early stopping triggered after 36 epochs\n",
      "\n",
      "Fold 3 Test Metrics:\n",
      "  Accuracy:  0.4547\n",
      "  Precision: 0.6984\n",
      "  Recall:    0.4547\n",
      "  F1 Score:  0.4943\n",
      "\n",
      "Fold 3 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8226    0.3961    0.5347       515\n",
      "           1     0.2283    0.6765    0.3414       136\n",
      "\n",
      "    accuracy                         0.4547       651\n",
      "   macro avg     0.5254    0.5363    0.4381       651\n",
      "weighted avg     0.6984    0.4547    0.4943       651\n",
      "\n",
      "\n",
      "===== BaselineGAT - Fold 4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=11.8155, Acc=0.5611 | Val Loss=3.0003, Acc=0.4731\n",
      "Epoch 02: Train Loss=3.5342, Acc=0.5236 | Val Loss=1.3854, Acc=0.6154\n",
      "Epoch 03: Train Loss=1.6262, Acc=0.5409 | Val Loss=0.8127, Acc=0.4577\n",
      "Epoch 04: Train Loss=0.8523, Acc=0.4841 | Val Loss=0.6929, Acc=0.6538\n",
      "Epoch 05: Train Loss=0.7778, Acc=0.5019 | Val Loss=0.7065, Acc=0.2577\n",
      "Epoch 06: Train Loss=0.7310, Acc=0.5577 | Val Loss=0.6929, Acc=0.5923\n",
      "Epoch 07: Train Loss=0.7145, Acc=0.5981 | Val Loss=0.6873, Acc=0.7423\n",
      "Epoch 08: Train Loss=0.7084, Acc=0.6279 | Val Loss=0.6858, Acc=0.6115\n",
      "Epoch 09: Train Loss=0.7026, Acc=0.6240 | Val Loss=0.6992, Acc=0.4635\n",
      "Epoch 10: Train Loss=0.7000, Acc=0.5995 | Val Loss=0.6851, Acc=0.6923\n",
      "Epoch 11: Train Loss=0.6985, Acc=0.6582 | Val Loss=0.6829, Acc=0.7173\n",
      "Epoch 12: Train Loss=0.6878, Acc=0.6370 | Val Loss=0.6853, Acc=0.7269\n",
      "Epoch 13: Train Loss=0.6912, Acc=0.6870 | Val Loss=0.6893, Acc=0.6981\n",
      "Epoch 14: Train Loss=0.6808, Acc=0.6202 | Val Loss=0.6940, Acc=0.7615\n",
      "Epoch 15: Train Loss=0.6797, Acc=0.6697 | Val Loss=0.6893, Acc=0.7596\n",
      "Epoch 16: Train Loss=0.6612, Acc=0.6894 | Val Loss=0.6782, Acc=0.7250\n",
      "Epoch 17: Train Loss=0.6626, Acc=0.6827 | Val Loss=0.6803, Acc=0.7327\n",
      "Epoch 18: Train Loss=0.6664, Acc=0.6793 | Val Loss=0.6821, Acc=0.7173\n",
      "Epoch 19: Train Loss=0.6623, Acc=0.6678 | Val Loss=0.6794, Acc=0.6846\n",
      "Epoch 20: Train Loss=0.6569, Acc=0.6779 | Val Loss=0.6833, Acc=0.6788\n",
      "Epoch 21: Train Loss=0.6568, Acc=0.6755 | Val Loss=0.6794, Acc=0.6731\n",
      "Epoch 22: Train Loss=0.6521, Acc=0.6779 | Val Loss=0.6769, Acc=0.7038\n",
      "Epoch 23: Train Loss=0.6505, Acc=0.6947 | Val Loss=0.6775, Acc=0.6846\n",
      "Epoch 24: Train Loss=0.6560, Acc=0.6620 | Val Loss=0.6802, Acc=0.6865\n",
      "Epoch 25: Train Loss=0.6438, Acc=0.6942 | Val Loss=0.6777, Acc=0.6769\n",
      "Epoch 26: Train Loss=0.6391, Acc=0.6851 | Val Loss=0.6759, Acc=0.6635\n",
      "Epoch 27: Train Loss=0.6508, Acc=0.6740 | Val Loss=0.6787, Acc=0.6731\n",
      "Epoch 28: Train Loss=0.6428, Acc=0.6817 | Val Loss=0.6797, Acc=0.6615\n",
      "Epoch 29: Train Loss=0.6475, Acc=0.6760 | Val Loss=0.6782, Acc=0.6788\n",
      "Epoch 30: Train Loss=0.6418, Acc=0.6707 | Val Loss=0.6795, Acc=0.6577\n",
      "Epoch 31: Train Loss=0.6296, Acc=0.6755 | Val Loss=0.6787, Acc=0.6635\n",
      "Epoch 32: Train Loss=0.6365, Acc=0.6837 | Val Loss=0.6803, Acc=0.6692\n",
      "Epoch 33: Train Loss=0.6382, Acc=0.6880 | Val Loss=0.6815, Acc=0.6519\n",
      "Epoch 34: Train Loss=0.6378, Acc=0.6909 | Val Loss=0.6807, Acc=0.6462\n",
      "Epoch 35: Train Loss=0.6295, Acc=0.6841 | Val Loss=0.6816, Acc=0.6385\n",
      "Epoch 36: Train Loss=0.6299, Acc=0.6856 | Val Loss=0.6807, Acc=0.6462\n",
      "Epoch 37: Train Loss=0.6352, Acc=0.6870 | Val Loss=0.6802, Acc=0.6346\n",
      "Epoch 38: Train Loss=0.6277, Acc=0.6851 | Val Loss=0.6811, Acc=0.6385\n",
      "Epoch 39: Train Loss=0.6318, Acc=0.6798 | Val Loss=0.6813, Acc=0.6308\n",
      "Epoch 40: Train Loss=0.6382, Acc=0.6659 | Val Loss=0.6810, Acc=0.6308\n",
      "Epoch 41: Train Loss=0.6325, Acc=0.6548 | Val Loss=0.6814, Acc=0.6231\n",
      "Early stopping triggered after 41 epochs\n",
      "\n",
      "Fold 4 Test Metrics:\n",
      "  Accuracy:  0.6237\n",
      "  Precision: 0.7132\n",
      "  Recall:    0.6237\n",
      "  F1 Score:  0.6547\n",
      "\n",
      "Fold 4 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8293    0.6602    0.7351       515\n",
      "           1     0.2739    0.4853    0.3501       136\n",
      "\n",
      "    accuracy                         0.6237       651\n",
      "   macro avg     0.5516    0.5727    0.5426       651\n",
      "weighted avg     0.7132    0.6237    0.6547       651\n",
      "\n",
      "\n",
      "===== BaselineGAT - Fold 5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=11.4434, Acc=0.5462 | Val Loss=2.6323, Acc=0.6019\n",
      "Epoch 02: Train Loss=3.2860, Acc=0.5298 | Val Loss=1.0511, Acc=0.6712\n",
      "Epoch 03: Train Loss=1.0727, Acc=0.5380 | Val Loss=0.7372, Acc=0.7000\n",
      "Epoch 04: Train Loss=0.7974, Acc=0.5663 | Val Loss=0.7171, Acc=0.3673\n",
      "Epoch 05: Train Loss=0.7432, Acc=0.6072 | Val Loss=0.7425, Acc=0.3500\n",
      "Epoch 06: Train Loss=0.7405, Acc=0.5707 | Val Loss=0.7019, Acc=0.7654\n",
      "Epoch 07: Train Loss=0.7103, Acc=0.6000 | Val Loss=0.6777, Acc=0.6077\n",
      "Epoch 08: Train Loss=0.7054, Acc=0.6149 | Val Loss=0.6891, Acc=0.7615\n",
      "Epoch 09: Train Loss=0.6975, Acc=0.6399 | Val Loss=0.6903, Acc=0.7808\n",
      "Epoch 10: Train Loss=0.6987, Acc=0.6317 | Val Loss=0.6758, Acc=0.6865\n",
      "Epoch 11: Train Loss=0.6969, Acc=0.6139 | Val Loss=0.6696, Acc=0.7731\n",
      "Epoch 12: Train Loss=0.7047, Acc=0.6553 | Val Loss=0.6621, Acc=0.6654\n",
      "Epoch 13: Train Loss=0.6929, Acc=0.6865 | Val Loss=0.6567, Acc=0.6962\n",
      "Epoch 14: Train Loss=0.6983, Acc=0.6865 | Val Loss=0.6797, Acc=0.7635\n",
      "Epoch 15: Train Loss=0.6919, Acc=0.6736 | Val Loss=0.6860, Acc=0.7038\n",
      "Epoch 16: Train Loss=0.6875, Acc=0.7082 | Val Loss=0.6662, Acc=0.7058\n",
      "Epoch 17: Train Loss=0.6693, Acc=0.7125 | Val Loss=0.6660, Acc=0.7135\n",
      "Epoch 18: Train Loss=0.6720, Acc=0.7115 | Val Loss=0.6685, Acc=0.7019\n",
      "Epoch 19: Train Loss=0.6806, Acc=0.6885 | Val Loss=0.6713, Acc=0.7077\n",
      "Epoch 20: Train Loss=0.6761, Acc=0.6923 | Val Loss=0.6668, Acc=0.7154\n",
      "Epoch 21: Train Loss=0.6637, Acc=0.7159 | Val Loss=0.6754, Acc=0.7385\n",
      "Epoch 22: Train Loss=0.6851, Acc=0.6880 | Val Loss=0.6724, Acc=0.7192\n",
      "Epoch 23: Train Loss=0.6624, Acc=0.6875 | Val Loss=0.6696, Acc=0.7250\n",
      "Epoch 24: Train Loss=0.6605, Acc=0.7048 | Val Loss=0.6748, Acc=0.7365\n",
      "Epoch 25: Train Loss=0.6730, Acc=0.6962 | Val Loss=0.6696, Acc=0.7288\n",
      "Epoch 26: Train Loss=0.6722, Acc=0.6841 | Val Loss=0.6743, Acc=0.7481\n",
      "Epoch 27: Train Loss=0.6474, Acc=0.7063 | Val Loss=0.6691, Acc=0.7212\n",
      "Epoch 28: Train Loss=0.6541, Acc=0.7005 | Val Loss=0.6676, Acc=0.7212\n",
      "Early stopping triggered after 28 epochs\n",
      "\n",
      "Fold 5 Test Metrics:\n",
      "  Accuracy:  0.6836\n",
      "  Precision: 0.7142\n",
      "  Recall:    0.6836\n",
      "  F1 Score:  0.6969\n",
      "\n",
      "Fold 5 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8239    0.7631    0.7923       515\n",
      "           1     0.2989    0.3824    0.3355       136\n",
      "\n",
      "    accuracy                         0.6836       651\n",
      "   macro avg     0.5614    0.5727    0.5639       651\n",
      "weighted avg     0.7142    0.6836    0.6969       651\n",
      "\n",
      "\n",
      "===== Final Combined Results (Majority Vote) for BaselineGAT =====\n",
      "Final Accuracy:  0.6313\n",
      "Final Precision: 0.7079\n",
      "Final Recall:    0.6313\n",
      "Final F1 Score:  0.6595\n",
      "\n",
      "Final Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8235    0.6796    0.7447       515\n",
      "           1     0.2699    0.4485    0.3370       136\n",
      "\n",
      "    accuracy                         0.6313       651\n",
      "   macro avg     0.5467    0.5641    0.5408       651\n",
      "weighted avg     0.7079    0.6313    0.6595       651\n",
      "\n",
      "\n",
      "BaselineGAT - Final Majority Vote Results:\n",
      "  Accuracy:  0.6313\n",
      "  Precision: 0.7079\n",
      "  Recall:    0.6313\n",
      "  F1 Score:  0.6595\n",
      "\n",
      "=== Testing BaselineSAGE Model ===\n",
      "\n",
      "===== BaselineSAGE - Fold 1 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=2.1447, Acc=0.5399 | Val Loss=0.7381, Acc=0.6027\n",
      "Epoch 02: Train Loss=0.7701, Acc=0.5894 | Val Loss=0.7019, Acc=0.6545\n",
      "Epoch 03: Train Loss=0.6928, Acc=0.6370 | Val Loss=0.6742, Acc=0.6852\n",
      "Epoch 04: Train Loss=0.6766, Acc=0.6885 | Val Loss=0.6808, Acc=0.6833\n",
      "Epoch 05: Train Loss=0.6739, Acc=0.6981 | Val Loss=0.6831, Acc=0.7409\n",
      "Epoch 06: Train Loss=0.6793, Acc=0.6111 | Val Loss=0.6816, Acc=0.6142\n",
      "Epoch 07: Train Loss=0.6669, Acc=0.5928 | Val Loss=0.6897, Acc=0.5624\n",
      "Epoch 08: Train Loss=0.6625, Acc=0.5995 | Val Loss=0.6760, Acc=0.6084\n",
      "Epoch 09: Train Loss=0.6553, Acc=0.6394 | Val Loss=0.6743, Acc=0.6296\n",
      "Epoch 10: Train Loss=0.6592, Acc=0.6212 | Val Loss=0.6824, Acc=0.5739\n",
      "Epoch 11: Train Loss=0.6570, Acc=0.6115 | Val Loss=0.6970, Acc=0.5835\n",
      "Epoch 12: Train Loss=0.6577, Acc=0.5923 | Val Loss=0.6751, Acc=0.6065\n",
      "Epoch 13: Train Loss=0.6451, Acc=0.6058 | Val Loss=0.6971, Acc=0.6334\n",
      "Epoch 14: Train Loss=0.6531, Acc=0.6380 | Val Loss=0.6769, Acc=0.5777\n",
      "Epoch 15: Train Loss=0.6455, Acc=0.6096 | Val Loss=0.6697, Acc=0.6008\n",
      "Epoch 16: Train Loss=0.6470, Acc=0.6188 | Val Loss=0.6819, Acc=0.6008\n",
      "Epoch 17: Train Loss=0.6538, Acc=0.6245 | Val Loss=0.6820, Acc=0.6430\n",
      "Epoch 18: Train Loss=0.6440, Acc=0.6149 | Val Loss=0.6797, Acc=0.6046\n",
      "Epoch 19: Train Loss=0.6406, Acc=0.6212 | Val Loss=0.6820, Acc=0.6027\n",
      "Epoch 20: Train Loss=0.6386, Acc=0.6332 | Val Loss=0.6819, Acc=0.5950\n",
      "Epoch 21: Train Loss=0.6356, Acc=0.6337 | Val Loss=0.6844, Acc=0.6200\n",
      "Epoch 22: Train Loss=0.6403, Acc=0.6231 | Val Loss=0.6845, Acc=0.6065\n",
      "Epoch 23: Train Loss=0.6419, Acc=0.6337 | Val Loss=0.6874, Acc=0.6257\n",
      "Epoch 24: Train Loss=0.6338, Acc=0.6375 | Val Loss=0.6863, Acc=0.6219\n",
      "Epoch 25: Train Loss=0.6357, Acc=0.6486 | Val Loss=0.6907, Acc=0.6065\n",
      "Epoch 26: Train Loss=0.6315, Acc=0.6337 | Val Loss=0.6851, Acc=0.6161\n",
      "Epoch 27: Train Loss=0.6348, Acc=0.6375 | Val Loss=0.6818, Acc=0.6142\n",
      "Epoch 28: Train Loss=0.6386, Acc=0.6409 | Val Loss=0.6816, Acc=0.6008\n",
      "Epoch 29: Train Loss=0.6358, Acc=0.6389 | Val Loss=0.6838, Acc=0.6104\n",
      "Epoch 30: Train Loss=0.6320, Acc=0.6404 | Val Loss=0.6839, Acc=0.6065\n",
      "Early stopping triggered after 30 epochs\n",
      "\n",
      "Fold 1 Test Metrics:\n",
      "  Accuracy:  0.6037\n",
      "  Precision: 0.7161\n",
      "  Recall:    0.6037\n",
      "  F1 Score:  0.6391\n",
      "\n",
      "Fold 1 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8338    0.6233    0.7133       515\n",
      "           1     0.2707    0.5294    0.3582       136\n",
      "\n",
      "    accuracy                         0.6037       651\n",
      "   macro avg     0.5522    0.5764    0.5358       651\n",
      "weighted avg     0.7161    0.6037    0.6391       651\n",
      "\n",
      "\n",
      "===== BaselineSAGE - Fold 2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=2.4742, Acc=0.5452 | Val Loss=0.7221, Acc=0.5750\n",
      "Epoch 02: Train Loss=0.7531, Acc=0.5644 | Val Loss=0.6689, Acc=0.6077\n",
      "Epoch 03: Train Loss=0.6979, Acc=0.5365 | Val Loss=0.6616, Acc=0.6000\n",
      "Epoch 04: Train Loss=0.6801, Acc=0.5981 | Val Loss=0.6590, Acc=0.6423\n",
      "Epoch 05: Train Loss=0.6748, Acc=0.5303 | Val Loss=0.6660, Acc=0.5288\n",
      "Epoch 06: Train Loss=0.6732, Acc=0.5793 | Val Loss=0.6784, Acc=0.6885\n",
      "Epoch 07: Train Loss=0.6613, Acc=0.6332 | Val Loss=0.6555, Acc=0.6404\n",
      "Epoch 08: Train Loss=0.6732, Acc=0.6260 | Val Loss=0.6636, Acc=0.6442\n",
      "Epoch 09: Train Loss=0.6710, Acc=0.6149 | Val Loss=0.6570, Acc=0.6635\n",
      "Epoch 10: Train Loss=0.6617, Acc=0.6197 | Val Loss=0.6580, Acc=0.6500\n",
      "Epoch 11: Train Loss=0.6655, Acc=0.6337 | Val Loss=0.6556, Acc=0.6212\n",
      "Epoch 12: Train Loss=0.6681, Acc=0.5865 | Val Loss=0.6652, Acc=0.6423\n",
      "Epoch 13: Train Loss=0.6541, Acc=0.6332 | Val Loss=0.6607, Acc=0.6500\n",
      "Epoch 14: Train Loss=0.6484, Acc=0.6404 | Val Loss=0.6653, Acc=0.6404\n",
      "Epoch 15: Train Loss=0.6528, Acc=0.6327 | Val Loss=0.6627, Acc=0.6462\n",
      "Epoch 16: Train Loss=0.6556, Acc=0.6442 | Val Loss=0.6638, Acc=0.6519\n",
      "Epoch 17: Train Loss=0.6502, Acc=0.6529 | Val Loss=0.6661, Acc=0.6096\n",
      "Epoch 18: Train Loss=0.6464, Acc=0.6500 | Val Loss=0.6640, Acc=0.6442\n",
      "Epoch 19: Train Loss=0.6448, Acc=0.6562 | Val Loss=0.6649, Acc=0.6442\n",
      "Epoch 20: Train Loss=0.6530, Acc=0.6486 | Val Loss=0.6668, Acc=0.6462\n",
      "Epoch 21: Train Loss=0.6506, Acc=0.6524 | Val Loss=0.6659, Acc=0.6404\n",
      "Epoch 22: Train Loss=0.6460, Acc=0.6471 | Val Loss=0.6678, Acc=0.6577\n",
      "Early stopping triggered after 22 epochs\n",
      "\n",
      "Fold 2 Test Metrics:\n",
      "  Accuracy:  0.6114\n",
      "  Precision: 0.7121\n",
      "  Recall:    0.6114\n",
      "  F1 Score:  0.6449\n",
      "\n",
      "Fold 2 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8291    0.6408    0.7229       515\n",
      "           1     0.2688    0.5000    0.3496       136\n",
      "\n",
      "    accuracy                         0.6114       651\n",
      "   macro avg     0.5490    0.5704    0.5363       651\n",
      "weighted avg     0.7121    0.6114    0.6449       651\n",
      "\n",
      "\n",
      "===== BaselineSAGE - Fold 3 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=1.6282, Acc=0.5231 | Val Loss=0.6732, Acc=0.6058\n",
      "Epoch 02: Train Loss=0.7280, Acc=0.5909 | Val Loss=0.6740, Acc=0.6154\n",
      "Epoch 03: Train Loss=0.7036, Acc=0.6990 | Val Loss=0.6874, Acc=0.7558\n",
      "Epoch 04: Train Loss=0.6931, Acc=0.7014 | Val Loss=0.6627, Acc=0.7712\n",
      "Epoch 05: Train Loss=0.6782, Acc=0.7260 | Val Loss=0.6583, Acc=0.7788\n",
      "Epoch 06: Train Loss=0.6798, Acc=0.7486 | Val Loss=0.6633, Acc=0.7788\n",
      "Epoch 07: Train Loss=0.6725, Acc=0.7572 | Val Loss=0.6747, Acc=0.7827\n",
      "Epoch 08: Train Loss=0.6770, Acc=0.7332 | Val Loss=0.6474, Acc=0.7538\n",
      "Epoch 09: Train Loss=0.6603, Acc=0.7332 | Val Loss=0.6496, Acc=0.7442\n",
      "Epoch 10: Train Loss=0.6623, Acc=0.7236 | Val Loss=0.6649, Acc=0.7750\n",
      "Epoch 11: Train Loss=0.6663, Acc=0.7486 | Val Loss=0.6564, Acc=0.7654\n",
      "Epoch 12: Train Loss=0.6655, Acc=0.7260 | Val Loss=0.6546, Acc=0.7769\n",
      "Epoch 13: Train Loss=0.6689, Acc=0.7514 | Val Loss=0.6642, Acc=0.7365\n",
      "Epoch 14: Train Loss=0.6572, Acc=0.7418 | Val Loss=0.6622, Acc=0.7635\n",
      "Epoch 15: Train Loss=0.6594, Acc=0.7442 | Val Loss=0.6548, Acc=0.7500\n",
      "Epoch 16: Train Loss=0.6553, Acc=0.7370 | Val Loss=0.6508, Acc=0.7500\n",
      "Epoch 17: Train Loss=0.6563, Acc=0.7438 | Val Loss=0.6526, Acc=0.7654\n",
      "Epoch 18: Train Loss=0.6503, Acc=0.7389 | Val Loss=0.6614, Acc=0.7635\n",
      "Epoch 19: Train Loss=0.6436, Acc=0.7284 | Val Loss=0.6570, Acc=0.7519\n",
      "Epoch 20: Train Loss=0.6454, Acc=0.7361 | Val Loss=0.6611, Acc=0.7654\n",
      "Epoch 21: Train Loss=0.6385, Acc=0.7423 | Val Loss=0.6559, Acc=0.7462\n",
      "Epoch 22: Train Loss=0.6384, Acc=0.7216 | Val Loss=0.6629, Acc=0.7519\n",
      "Epoch 23: Train Loss=0.6392, Acc=0.7293 | Val Loss=0.6597, Acc=0.7385\n",
      "Early stopping triggered after 23 epochs\n",
      "\n",
      "Fold 3 Test Metrics:\n",
      "  Accuracy:  0.6912\n",
      "  Precision: 0.7050\n",
      "  Recall:    0.6912\n",
      "  F1 Score:  0.6977\n",
      "\n",
      "Fold 3 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8153    0.7883    0.8016       515\n",
      "           1     0.2876    0.3235    0.3045       136\n",
      "\n",
      "    accuracy                         0.6912       651\n",
      "   macro avg     0.5514    0.5559    0.5530       651\n",
      "weighted avg     0.7050    0.6912    0.6977       651\n",
      "\n",
      "\n",
      "===== BaselineSAGE - Fold 4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=1.4094, Acc=0.5755 | Val Loss=0.7236, Acc=0.7212\n",
      "Epoch 02: Train Loss=0.6924, Acc=0.6317 | Val Loss=0.6784, Acc=0.7481\n",
      "Epoch 03: Train Loss=0.6752, Acc=0.7149 | Val Loss=0.6737, Acc=0.7250\n",
      "Epoch 04: Train Loss=0.6690, Acc=0.7250 | Val Loss=0.6724, Acc=0.7288\n",
      "Epoch 05: Train Loss=0.6687, Acc=0.7322 | Val Loss=0.6722, Acc=0.7077\n",
      "Epoch 06: Train Loss=0.6594, Acc=0.7524 | Val Loss=0.6777, Acc=0.7212\n",
      "Epoch 07: Train Loss=0.6570, Acc=0.7524 | Val Loss=0.6848, Acc=0.7038\n",
      "Epoch 08: Train Loss=0.6570, Acc=0.7385 | Val Loss=0.6756, Acc=0.7096\n",
      "Epoch 09: Train Loss=0.6477, Acc=0.7380 | Val Loss=0.6827, Acc=0.7115\n",
      "Epoch 10: Train Loss=0.6495, Acc=0.7351 | Val Loss=0.6715, Acc=0.7212\n",
      "Epoch 11: Train Loss=0.6507, Acc=0.7500 | Val Loss=0.6720, Acc=0.6981\n",
      "Epoch 12: Train Loss=0.6417, Acc=0.7442 | Val Loss=0.6810, Acc=0.6981\n",
      "Epoch 13: Train Loss=0.6372, Acc=0.7442 | Val Loss=0.6743, Acc=0.6962\n",
      "Epoch 14: Train Loss=0.6374, Acc=0.7490 | Val Loss=0.6743, Acc=0.6962\n",
      "Epoch 15: Train Loss=0.6390, Acc=0.7361 | Val Loss=0.6733, Acc=0.7212\n",
      "Epoch 16: Train Loss=0.6347, Acc=0.7577 | Val Loss=0.6793, Acc=0.7077\n",
      "Epoch 17: Train Loss=0.6336, Acc=0.7500 | Val Loss=0.6728, Acc=0.6981\n",
      "Epoch 18: Train Loss=0.6349, Acc=0.7553 | Val Loss=0.6786, Acc=0.7058\n",
      "Epoch 19: Train Loss=0.6397, Acc=0.7433 | Val Loss=0.6792, Acc=0.6981\n",
      "Epoch 20: Train Loss=0.6332, Acc=0.7524 | Val Loss=0.6789, Acc=0.6981\n",
      "Epoch 21: Train Loss=0.6296, Acc=0.7505 | Val Loss=0.6753, Acc=0.7000\n",
      "Epoch 22: Train Loss=0.6294, Acc=0.7524 | Val Loss=0.6758, Acc=0.6942\n",
      "Epoch 23: Train Loss=0.6295, Acc=0.7490 | Val Loss=0.6763, Acc=0.7115\n",
      "Epoch 24: Train Loss=0.6263, Acc=0.7466 | Val Loss=0.6767, Acc=0.7000\n",
      "Epoch 25: Train Loss=0.6241, Acc=0.7457 | Val Loss=0.6759, Acc=0.6981\n",
      "Early stopping triggered after 25 epochs\n",
      "\n",
      "Fold 4 Test Metrics:\n",
      "  Accuracy:  0.6743\n",
      "  Precision: 0.7025\n",
      "  Recall:    0.6743\n",
      "  F1 Score:  0.6869\n",
      "\n",
      "Fold 4 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8150    0.7612    0.7871       515\n",
      "           1     0.2765    0.3456    0.3072       136\n",
      "\n",
      "    accuracy                         0.6743       651\n",
      "   macro avg     0.5457    0.5534    0.5472       651\n",
      "weighted avg     0.7025    0.6743    0.6869       651\n",
      "\n",
      "\n",
      "===== BaselineSAGE - Fold 5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=2.1848, Acc=0.5375 | Val Loss=0.7650, Acc=0.5404\n",
      "Epoch 02: Train Loss=0.7593, Acc=0.5865 | Val Loss=0.6762, Acc=0.6673\n",
      "Epoch 03: Train Loss=0.6994, Acc=0.5534 | Val Loss=0.6748, Acc=0.5250\n",
      "Epoch 04: Train Loss=0.6744, Acc=0.6077 | Val Loss=0.6769, Acc=0.7308\n",
      "Epoch 05: Train Loss=0.6780, Acc=0.5986 | Val Loss=0.6646, Acc=0.6519\n",
      "Epoch 06: Train Loss=0.6662, Acc=0.6361 | Val Loss=0.6802, Acc=0.6154\n",
      "Epoch 07: Train Loss=0.6713, Acc=0.6091 | Val Loss=0.6622, Acc=0.6442\n",
      "Epoch 08: Train Loss=0.6679, Acc=0.5885 | Val Loss=0.6730, Acc=0.4923\n",
      "Epoch 09: Train Loss=0.6716, Acc=0.6077 | Val Loss=0.6735, Acc=0.6423\n",
      "Epoch 10: Train Loss=0.6630, Acc=0.5918 | Val Loss=0.6823, Acc=0.6250\n",
      "Epoch 11: Train Loss=0.6592, Acc=0.5990 | Val Loss=0.6634, Acc=0.6269\n",
      "Epoch 12: Train Loss=0.6592, Acc=0.6260 | Val Loss=0.6636, Acc=0.6135\n",
      "Epoch 13: Train Loss=0.6488, Acc=0.6452 | Val Loss=0.6624, Acc=0.6231\n",
      "Epoch 14: Train Loss=0.6535, Acc=0.6486 | Val Loss=0.6581, Acc=0.6577\n",
      "Epoch 15: Train Loss=0.6528, Acc=0.6404 | Val Loss=0.6636, Acc=0.6462\n",
      "Epoch 16: Train Loss=0.6499, Acc=0.6389 | Val Loss=0.6645, Acc=0.6481\n",
      "Epoch 17: Train Loss=0.6461, Acc=0.6495 | Val Loss=0.6634, Acc=0.6500\n",
      "Epoch 18: Train Loss=0.6431, Acc=0.6389 | Val Loss=0.6634, Acc=0.6423\n",
      "Epoch 19: Train Loss=0.6402, Acc=0.6452 | Val Loss=0.6678, Acc=0.6231\n",
      "Epoch 20: Train Loss=0.6395, Acc=0.6255 | Val Loss=0.6778, Acc=0.6712\n",
      "Epoch 21: Train Loss=0.6440, Acc=0.6543 | Val Loss=0.6674, Acc=0.6250\n",
      "Epoch 22: Train Loss=0.6368, Acc=0.6452 | Val Loss=0.6725, Acc=0.6846\n",
      "Epoch 23: Train Loss=0.6398, Acc=0.6519 | Val Loss=0.6706, Acc=0.6346\n",
      "Epoch 24: Train Loss=0.6296, Acc=0.6500 | Val Loss=0.6710, Acc=0.6538\n",
      "Epoch 25: Train Loss=0.6300, Acc=0.6558 | Val Loss=0.6701, Acc=0.6558\n",
      "Epoch 26: Train Loss=0.6350, Acc=0.6462 | Val Loss=0.6743, Acc=0.6462\n",
      "Epoch 27: Train Loss=0.6444, Acc=0.6553 | Val Loss=0.6702, Acc=0.6462\n",
      "Epoch 28: Train Loss=0.6351, Acc=0.6591 | Val Loss=0.6742, Acc=0.6269\n",
      "Epoch 29: Train Loss=0.6301, Acc=0.6466 | Val Loss=0.6732, Acc=0.6346\n",
      "Early stopping triggered after 29 epochs\n",
      "\n",
      "Fold 5 Test Metrics:\n",
      "  Accuracy:  0.6022\n",
      "  Precision: 0.7138\n",
      "  Recall:    0.6022\n",
      "  F1 Score:  0.6377\n",
      "\n",
      "Fold 5 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8316    0.6233    0.7125       515\n",
      "           1     0.2679    0.5221    0.3541       136\n",
      "\n",
      "    accuracy                         0.6022       651\n",
      "   macro avg     0.5498    0.5727    0.5333       651\n",
      "weighted avg     0.7138    0.6022    0.6377       651\n",
      "\n",
      "\n",
      "===== Final Combined Results (Majority Vote) for BaselineSAGE =====\n",
      "Final Accuracy:  0.6283\n",
      "Final Precision: 0.7100\n",
      "Final Recall:    0.6283\n",
      "Final F1 Score:  0.6576\n",
      "\n",
      "Final Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8258    0.6718    0.7409       515\n",
      "           1     0.2716    0.4632    0.3424       136\n",
      "\n",
      "    accuracy                         0.6283       651\n",
      "   macro avg     0.5487    0.5675    0.5416       651\n",
      "weighted avg     0.7100    0.6283    0.6576       651\n",
      "\n",
      "\n",
      "BaselineSAGE - Final Majority Vote Results:\n",
      "  Accuracy:  0.6283\n",
      "  Precision: 0.7100\n",
      "  Recall:    0.6283\n",
      "  F1 Score:  0.6576\n",
      "\n",
      "=== Testing ResidualGCN Model ===\n",
      "\n",
      "===== ResidualGCN - Fold 1 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=4.2119, Acc=0.5269 | Val Loss=2.6386, Acc=0.6526\n",
      "Epoch 02: Train Loss=1.8353, Acc=0.5832 | Val Loss=1.5126, Acc=0.6180\n",
      "Epoch 03: Train Loss=1.0820, Acc=0.5793 | Val Loss=0.9851, Acc=0.6795\n",
      "Epoch 04: Train Loss=0.8449, Acc=0.6375 | Val Loss=0.8188, Acc=0.5681\n",
      "Epoch 05: Train Loss=0.7422, Acc=0.6087 | Val Loss=0.7337, Acc=0.6065\n",
      "Epoch 06: Train Loss=0.6968, Acc=0.6841 | Val Loss=0.6922, Acc=0.6353\n",
      "Epoch 07: Train Loss=0.6838, Acc=0.6611 | Val Loss=0.8030, Acc=0.5163\n",
      "Epoch 08: Train Loss=0.6762, Acc=0.6788 | Val Loss=0.6961, Acc=0.7198\n",
      "Epoch 09: Train Loss=0.6666, Acc=0.7106 | Val Loss=0.7101, Acc=0.6027\n",
      "Epoch 10: Train Loss=0.6665, Acc=0.6760 | Val Loss=0.7125, Acc=0.7390\n",
      "Epoch 11: Train Loss=0.6573, Acc=0.7091 | Val Loss=0.7104, Acc=0.7006\n",
      "Epoch 12: Train Loss=0.6565, Acc=0.6990 | Val Loss=0.6913, Acc=0.6987\n",
      "Epoch 13: Train Loss=0.6520, Acc=0.7178 | Val Loss=0.6901, Acc=0.6641\n",
      "Epoch 14: Train Loss=0.6559, Acc=0.7178 | Val Loss=0.6865, Acc=0.7179\n",
      "Epoch 15: Train Loss=0.6482, Acc=0.7365 | Val Loss=0.6837, Acc=0.7083\n",
      "Epoch 16: Train Loss=0.6406, Acc=0.7014 | Val Loss=0.6852, Acc=0.7140\n",
      "Epoch 17: Train Loss=0.6307, Acc=0.7380 | Val Loss=0.6904, Acc=0.7102\n",
      "Epoch 18: Train Loss=0.6262, Acc=0.7053 | Val Loss=0.6848, Acc=0.6795\n",
      "Epoch 19: Train Loss=0.6129, Acc=0.7356 | Val Loss=0.7020, Acc=0.5816\n",
      "Epoch 20: Train Loss=0.6103, Acc=0.7188 | Val Loss=0.6948, Acc=0.6564\n",
      "Epoch 21: Train Loss=0.6111, Acc=0.7245 | Val Loss=0.6995, Acc=0.6737\n",
      "Epoch 22: Train Loss=0.6027, Acc=0.7216 | Val Loss=0.7048, Acc=0.6622\n",
      "Epoch 23: Train Loss=0.5910, Acc=0.7385 | Val Loss=0.7105, Acc=0.6852\n",
      "Epoch 24: Train Loss=0.5842, Acc=0.7409 | Val Loss=0.7162, Acc=0.6699\n",
      "Epoch 25: Train Loss=0.5876, Acc=0.7351 | Val Loss=0.7175, Acc=0.6795\n",
      "Epoch 26: Train Loss=0.5823, Acc=0.7452 | Val Loss=0.7161, Acc=0.6660\n",
      "Epoch 27: Train Loss=0.5781, Acc=0.7433 | Val Loss=0.7290, Acc=0.6814\n",
      "Epoch 28: Train Loss=0.5810, Acc=0.7457 | Val Loss=0.7189, Acc=0.6180\n",
      "Epoch 29: Train Loss=0.5729, Acc=0.7433 | Val Loss=0.7271, Acc=0.6699\n",
      "Epoch 30: Train Loss=0.5737, Acc=0.7413 | Val Loss=0.7306, Acc=0.6795\n",
      "Early stopping triggered after 30 epochs\n",
      "\n",
      "Fold 1 Test Metrics:\n",
      "  Accuracy:  0.6513\n",
      "  Precision: 0.6915\n",
      "  Recall:    0.6513\n",
      "  F1 Score:  0.6687\n",
      "\n",
      "Fold 1 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8077    0.7340    0.7691       515\n",
      "           1     0.2514    0.3382    0.2884       136\n",
      "\n",
      "    accuracy                         0.6513       651\n",
      "   macro avg     0.5295    0.5361    0.5287       651\n",
      "weighted avg     0.6915    0.6513    0.6687       651\n",
      "\n",
      "\n",
      "===== ResidualGCN - Fold 2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=4.0722, Acc=0.5817 | Val Loss=3.1922, Acc=0.7712\n",
      "Epoch 02: Train Loss=1.8867, Acc=0.5889 | Val Loss=2.4783, Acc=0.7750\n",
      "Epoch 03: Train Loss=1.3452, Acc=0.5803 | Val Loss=0.8872, Acc=0.4654\n",
      "Epoch 04: Train Loss=0.9183, Acc=0.6082 | Val Loss=0.8937, Acc=0.4558\n",
      "Epoch 05: Train Loss=0.7736, Acc=0.5990 | Val Loss=0.7197, Acc=0.4269\n",
      "Epoch 06: Train Loss=0.7356, Acc=0.5918 | Val Loss=0.7090, Acc=0.4808\n",
      "Epoch 07: Train Loss=0.6857, Acc=0.6264 | Val Loss=0.7165, Acc=0.6346\n",
      "Epoch 08: Train Loss=0.6713, Acc=0.6673 | Val Loss=0.7473, Acc=0.6115\n",
      "Epoch 09: Train Loss=0.6586, Acc=0.6399 | Val Loss=0.6814, Acc=0.5654\n",
      "Epoch 10: Train Loss=0.6562, Acc=0.6745 | Val Loss=0.7231, Acc=0.5635\n",
      "Epoch 11: Train Loss=0.6481, Acc=0.6697 | Val Loss=0.7169, Acc=0.5673\n",
      "Epoch 12: Train Loss=0.6479, Acc=0.6745 | Val Loss=0.6914, Acc=0.5808\n",
      "Epoch 13: Train Loss=0.6488, Acc=0.6865 | Val Loss=0.6930, Acc=0.6135\n",
      "Epoch 14: Train Loss=0.6326, Acc=0.6918 | Val Loss=0.6891, Acc=0.5962\n",
      "Epoch 15: Train Loss=0.6330, Acc=0.6976 | Val Loss=0.6971, Acc=0.5846\n",
      "Epoch 16: Train Loss=0.6249, Acc=0.7091 | Val Loss=0.7138, Acc=0.6192\n",
      "Epoch 17: Train Loss=0.6232, Acc=0.7120 | Val Loss=0.6978, Acc=0.5808\n",
      "Epoch 18: Train Loss=0.6193, Acc=0.7038 | Val Loss=0.7080, Acc=0.6346\n",
      "Epoch 19: Train Loss=0.6193, Acc=0.7264 | Val Loss=0.7103, Acc=0.6404\n",
      "Epoch 20: Train Loss=0.6195, Acc=0.7260 | Val Loss=0.7138, Acc=0.5808\n",
      "Epoch 21: Train Loss=0.6214, Acc=0.6962 | Val Loss=0.7117, Acc=0.6058\n",
      "Epoch 22: Train Loss=0.6079, Acc=0.7043 | Val Loss=0.7197, Acc=0.6558\n",
      "Epoch 23: Train Loss=0.6152, Acc=0.7183 | Val Loss=0.7167, Acc=0.6462\n",
      "Epoch 24: Train Loss=0.6126, Acc=0.7236 | Val Loss=0.7247, Acc=0.6135\n",
      "Early stopping triggered after 24 epochs\n",
      "\n",
      "Fold 2 Test Metrics:\n",
      "  Accuracy:  0.6252\n",
      "  Precision: 0.6893\n",
      "  Recall:    0.6252\n",
      "  F1 Score:  0.6508\n",
      "\n",
      "Fold 2 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8073    0.6913    0.7448       515\n",
      "           1     0.2429    0.3750    0.2948       136\n",
      "\n",
      "    accuracy                         0.6252       651\n",
      "   macro avg     0.5251    0.5331    0.5198       651\n",
      "weighted avg     0.6893    0.6252    0.6508       651\n",
      "\n",
      "\n",
      "===== ResidualGCN - Fold 3 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=5.8333, Acc=0.5505 | Val Loss=3.0678, Acc=0.5481\n",
      "Epoch 02: Train Loss=2.0280, Acc=0.5442 | Val Loss=1.7585, Acc=0.4962\n",
      "Epoch 03: Train Loss=1.0846, Acc=0.5611 | Val Loss=0.7519, Acc=0.7500\n",
      "Epoch 04: Train Loss=0.7519, Acc=0.5865 | Val Loss=0.6803, Acc=0.7212\n",
      "Epoch 05: Train Loss=0.6837, Acc=0.6418 | Val Loss=0.6675, Acc=0.7731\n",
      "Epoch 06: Train Loss=0.6742, Acc=0.6986 | Val Loss=0.6571, Acc=0.7712\n",
      "Epoch 07: Train Loss=0.6725, Acc=0.6769 | Val Loss=0.6657, Acc=0.7827\n",
      "Epoch 08: Train Loss=0.6600, Acc=0.6894 | Val Loss=0.6747, Acc=0.7923\n",
      "Epoch 09: Train Loss=0.6640, Acc=0.6899 | Val Loss=0.6733, Acc=0.7904\n",
      "Epoch 10: Train Loss=0.6592, Acc=0.7077 | Val Loss=0.6628, Acc=0.7827\n",
      "Epoch 11: Train Loss=0.6555, Acc=0.6865 | Val Loss=0.6688, Acc=0.7846\n",
      "Epoch 12: Train Loss=0.6515, Acc=0.7024 | Val Loss=0.6595, Acc=0.7865\n",
      "Epoch 13: Train Loss=0.6580, Acc=0.6928 | Val Loss=0.6773, Acc=0.7923\n",
      "Epoch 14: Train Loss=0.6524, Acc=0.7014 | Val Loss=0.6619, Acc=0.7865\n",
      "Epoch 15: Train Loss=0.6495, Acc=0.6942 | Val Loss=0.6675, Acc=0.7865\n",
      "Epoch 16: Train Loss=0.6441, Acc=0.6986 | Val Loss=0.6735, Acc=0.7865\n",
      "Epoch 17: Train Loss=0.6412, Acc=0.7077 | Val Loss=0.6689, Acc=0.7808\n",
      "Epoch 18: Train Loss=0.6363, Acc=0.7087 | Val Loss=0.6594, Acc=0.7827\n",
      "Epoch 19: Train Loss=0.6429, Acc=0.7000 | Val Loss=0.6725, Acc=0.7885\n",
      "Epoch 20: Train Loss=0.6339, Acc=0.7115 | Val Loss=0.6719, Acc=0.7827\n",
      "Epoch 21: Train Loss=0.6314, Acc=0.7038 | Val Loss=0.6724, Acc=0.7865\n",
      "Early stopping triggered after 21 epochs\n",
      "\n",
      "Fold 3 Test Metrics:\n",
      "  Accuracy:  0.7757\n",
      "  Precision: 0.7106\n",
      "  Recall:    0.7757\n",
      "  F1 Score:  0.7223\n",
      "\n",
      "Fold 3 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8010    0.9534    0.8706       515\n",
      "           1     0.3684    0.1029    0.1609       136\n",
      "\n",
      "    accuracy                         0.7757       651\n",
      "   macro avg     0.5847    0.5282    0.5157       651\n",
      "weighted avg     0.7106    0.7757    0.7223       651\n",
      "\n",
      "\n",
      "===== ResidualGCN - Fold 4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=6.0817, Acc=0.5029 | Val Loss=3.2184, Acc=0.6788\n",
      "Epoch 02: Train Loss=1.7975, Acc=0.5630 | Val Loss=1.7253, Acc=0.6577\n",
      "Epoch 03: Train Loss=1.2695, Acc=0.5832 | Val Loss=1.0155, Acc=0.4904\n",
      "Epoch 04: Train Loss=0.8797, Acc=0.6346 | Val Loss=0.8583, Acc=0.6692\n",
      "Epoch 05: Train Loss=0.7287, Acc=0.6601 | Val Loss=0.6960, Acc=0.7058\n",
      "Epoch 06: Train Loss=0.6996, Acc=0.7188 | Val Loss=0.7766, Acc=0.6654\n",
      "Epoch 07: Train Loss=0.6803, Acc=0.6952 | Val Loss=0.7163, Acc=0.6788\n",
      "Epoch 08: Train Loss=0.6706, Acc=0.7149 | Val Loss=0.6875, Acc=0.7365\n",
      "Epoch 09: Train Loss=0.6625, Acc=0.7317 | Val Loss=0.6830, Acc=0.7481\n",
      "Epoch 10: Train Loss=0.6495, Acc=0.6971 | Val Loss=0.6853, Acc=0.6712\n",
      "Epoch 11: Train Loss=0.6476, Acc=0.7115 | Val Loss=0.6835, Acc=0.7442\n",
      "Epoch 12: Train Loss=0.6460, Acc=0.7226 | Val Loss=0.6886, Acc=0.6865\n",
      "Epoch 13: Train Loss=0.6431, Acc=0.7149 | Val Loss=0.7040, Acc=0.6481\n",
      "Epoch 14: Train Loss=0.6443, Acc=0.7163 | Val Loss=0.6751, Acc=0.7423\n",
      "Epoch 15: Train Loss=0.6389, Acc=0.7202 | Val Loss=0.6877, Acc=0.6500\n",
      "Epoch 16: Train Loss=0.6230, Acc=0.7245 | Val Loss=0.6981, Acc=0.7077\n",
      "Epoch 17: Train Loss=0.6215, Acc=0.7173 | Val Loss=0.6923, Acc=0.6692\n",
      "Epoch 18: Train Loss=0.6123, Acc=0.7226 | Val Loss=0.7082, Acc=0.6615\n",
      "Epoch 19: Train Loss=0.6117, Acc=0.7212 | Val Loss=0.7289, Acc=0.6904\n",
      "Epoch 20: Train Loss=0.6109, Acc=0.7356 | Val Loss=0.7079, Acc=0.6538\n",
      "Epoch 21: Train Loss=0.6000, Acc=0.7471 | Val Loss=0.7198, Acc=0.6827\n",
      "Epoch 22: Train Loss=0.5922, Acc=0.7327 | Val Loss=0.7310, Acc=0.6962\n",
      "Epoch 23: Train Loss=0.5827, Acc=0.7587 | Val Loss=0.7221, Acc=0.6692\n",
      "Epoch 24: Train Loss=0.5812, Acc=0.7380 | Val Loss=0.7305, Acc=0.6654\n",
      "Epoch 25: Train Loss=0.5899, Acc=0.7466 | Val Loss=0.7226, Acc=0.6712\n",
      "Epoch 26: Train Loss=0.5849, Acc=0.7510 | Val Loss=0.7298, Acc=0.6577\n",
      "Epoch 27: Train Loss=0.5820, Acc=0.7486 | Val Loss=0.7277, Acc=0.6692\n",
      "Epoch 28: Train Loss=0.5665, Acc=0.7582 | Val Loss=0.7315, Acc=0.6731\n",
      "Epoch 29: Train Loss=0.5662, Acc=0.7702 | Val Loss=0.7308, Acc=0.6673\n",
      "Early stopping triggered after 29 epochs\n",
      "\n",
      "Fold 4 Test Metrics:\n",
      "  Accuracy:  0.6590\n",
      "  Precision: 0.6913\n",
      "  Recall:    0.6590\n",
      "  F1 Score:  0.6734\n",
      "\n",
      "Fold 4 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8071    0.7476    0.7762       515\n",
      "           1     0.2529    0.3235    0.2839       136\n",
      "\n",
      "    accuracy                         0.6590       651\n",
      "   macro avg     0.5300    0.5356    0.5300       651\n",
      "weighted avg     0.6913    0.6590    0.6734       651\n",
      "\n",
      "\n",
      "===== ResidualGCN - Fold 5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=4.8725, Acc=0.5346 | Val Loss=1.8715, Acc=0.4462\n",
      "Epoch 02: Train Loss=1.6675, Acc=0.5495 | Val Loss=1.2141, Acc=0.6154\n",
      "Epoch 03: Train Loss=0.9152, Acc=0.5380 | Val Loss=0.7178, Acc=0.5827\n",
      "Epoch 04: Train Loss=0.7297, Acc=0.5827 | Val Loss=0.7377, Acc=0.6769\n",
      "Epoch 05: Train Loss=0.6858, Acc=0.6067 | Val Loss=0.6814, Acc=0.4058\n",
      "Epoch 06: Train Loss=0.6759, Acc=0.6409 | Val Loss=0.6851, Acc=0.7365\n",
      "Epoch 07: Train Loss=0.6714, Acc=0.6750 | Val Loss=0.6739, Acc=0.7481\n",
      "Epoch 08: Train Loss=0.6806, Acc=0.6885 | Val Loss=0.6648, Acc=0.7500\n",
      "Epoch 09: Train Loss=0.6607, Acc=0.6683 | Val Loss=0.6819, Acc=0.7596\n",
      "Epoch 10: Train Loss=0.6585, Acc=0.7106 | Val Loss=0.6618, Acc=0.6769\n",
      "Epoch 11: Train Loss=0.6652, Acc=0.7139 | Val Loss=0.6790, Acc=0.7635\n",
      "Epoch 12: Train Loss=0.6579, Acc=0.6995 | Val Loss=0.7217, Acc=0.7481\n",
      "Epoch 13: Train Loss=0.6768, Acc=0.7125 | Val Loss=0.6724, Acc=0.7462\n",
      "Epoch 14: Train Loss=0.6548, Acc=0.7005 | Val Loss=0.6741, Acc=0.7519\n",
      "Epoch 15: Train Loss=0.6624, Acc=0.6928 | Val Loss=0.6772, Acc=0.7692\n",
      "Epoch 16: Train Loss=0.6545, Acc=0.7144 | Val Loss=0.6822, Acc=0.7058\n",
      "Epoch 17: Train Loss=0.6460, Acc=0.7067 | Val Loss=0.6835, Acc=0.7712\n",
      "Epoch 18: Train Loss=0.6476, Acc=0.6952 | Val Loss=0.6859, Acc=0.7596\n",
      "Epoch 19: Train Loss=0.6544, Acc=0.7236 | Val Loss=0.6784, Acc=0.7269\n",
      "Epoch 20: Train Loss=0.6382, Acc=0.6798 | Val Loss=0.6752, Acc=0.6769\n",
      "Epoch 21: Train Loss=0.6371, Acc=0.6856 | Val Loss=0.6877, Acc=0.7442\n",
      "Epoch 22: Train Loss=0.6426, Acc=0.6865 | Val Loss=0.6794, Acc=0.6423\n",
      "Epoch 23: Train Loss=0.6525, Acc=0.6904 | Val Loss=0.6811, Acc=0.7327\n",
      "Epoch 24: Train Loss=0.6222, Acc=0.7067 | Val Loss=0.6825, Acc=0.6981\n",
      "Epoch 25: Train Loss=0.6253, Acc=0.7005 | Val Loss=0.6736, Acc=0.6923\n",
      "Early stopping triggered after 25 epochs\n",
      "\n",
      "Fold 5 Test Metrics:\n",
      "  Accuracy:  0.6344\n",
      "  Precision: 0.6848\n",
      "  Recall:    0.6344\n",
      "  F1 Score:  0.6556\n",
      "\n",
      "Fold 5 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8031    0.7126    0.7551       515\n",
      "           1     0.2371    0.3382    0.2788       136\n",
      "\n",
      "    accuracy                         0.6344       651\n",
      "   macro avg     0.5201    0.5254    0.5170       651\n",
      "weighted avg     0.6848    0.6344    0.6556       651\n",
      "\n",
      "\n",
      "===== Final Combined Results (Majority Vote) for ResidualGCN =====\n",
      "Final Accuracy:  0.6912\n",
      "Final Precision: 0.6887\n",
      "Final Recall:    0.6912\n",
      "Final F1 Score:  0.6900\n",
      "\n",
      "Final Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8031    0.8078    0.8054       515\n",
      "           1     0.2556    0.2500    0.2528       136\n",
      "\n",
      "    accuracy                         0.6912       651\n",
      "   macro avg     0.5294    0.5289    0.5291       651\n",
      "weighted avg     0.6887    0.6912    0.6900       651\n",
      "\n",
      "\n",
      "ResidualGCN - Final Majority Vote Results:\n",
      "  Accuracy:  0.6912\n",
      "  Precision: 0.6887\n",
      "  Recall:    0.6912\n",
      "  F1 Score:  0.6900\n",
      "\n",
      "=== Testing HybridModel Model ===\n",
      "\n",
      "===== HybridModel - Fold 1 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=0.6869, Acc=0.5952 | Val Loss=0.6680, Acc=0.6449\n",
      "Epoch 02: Train Loss=0.6606, Acc=0.6500 | Val Loss=0.6741, Acc=0.6257\n",
      "Epoch 03: Train Loss=0.6621, Acc=0.6562 | Val Loss=0.6656, Acc=0.6545\n",
      "Epoch 04: Train Loss=0.6520, Acc=0.6697 | Val Loss=0.6675, Acc=0.6660\n",
      "Epoch 05: Train Loss=0.6574, Acc=0.6736 | Val Loss=0.6652, Acc=0.6756\n",
      "Epoch 06: Train Loss=0.6560, Acc=0.6750 | Val Loss=0.6670, Acc=0.6296\n",
      "Epoch 07: Train Loss=0.6552, Acc=0.6591 | Val Loss=0.6731, Acc=0.6488\n",
      "Epoch 08: Train Loss=0.6487, Acc=0.6687 | Val Loss=0.6694, Acc=0.6526\n",
      "Epoch 09: Train Loss=0.6530, Acc=0.6760 | Val Loss=0.6731, Acc=0.5912\n",
      "Epoch 10: Train Loss=0.6522, Acc=0.6630 | Val Loss=0.6728, Acc=0.6315\n",
      "Epoch 11: Train Loss=0.6447, Acc=0.6774 | Val Loss=0.6665, Acc=0.6449\n",
      "Epoch 12: Train Loss=0.6421, Acc=0.6856 | Val Loss=0.6688, Acc=0.6545\n",
      "Epoch 13: Train Loss=0.6373, Acc=0.6981 | Val Loss=0.6712, Acc=0.6545\n",
      "Epoch 14: Train Loss=0.6275, Acc=0.6966 | Val Loss=0.6815, Acc=0.6392\n",
      "Epoch 15: Train Loss=0.6376, Acc=0.6731 | Val Loss=0.6818, Acc=0.6104\n",
      "Epoch 16: Train Loss=0.6289, Acc=0.6837 | Val Loss=0.6839, Acc=0.6296\n",
      "Epoch 17: Train Loss=0.6380, Acc=0.6909 | Val Loss=0.6744, Acc=0.6411\n",
      "Epoch 18: Train Loss=0.6232, Acc=0.6832 | Val Loss=0.6835, Acc=0.6488\n",
      "Epoch 19: Train Loss=0.6239, Acc=0.6899 | Val Loss=0.6790, Acc=0.6411\n",
      "Epoch 20: Train Loss=0.6291, Acc=0.6798 | Val Loss=0.6797, Acc=0.6257\n",
      "Early stopping triggered after 20 epochs\n",
      "\n",
      "Fold 1 Test Metrics:\n",
      "  Accuracy:  0.6006\n",
      "  Precision: 0.7150\n",
      "  Recall:    0.6006\n",
      "  F1 Score:  0.6365\n",
      "\n",
      "Fold 1 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8329    0.6194    0.7105       515\n",
      "           1     0.2687    0.5294    0.3564       136\n",
      "\n",
      "    accuracy                         0.6006       651\n",
      "   macro avg     0.5508    0.5744    0.5335       651\n",
      "weighted avg     0.7150    0.6006    0.6365       651\n",
      "\n",
      "\n",
      "===== HybridModel - Fold 2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=0.6920, Acc=0.5904 | Val Loss=0.6593, Acc=0.6692\n",
      "Epoch 02: Train Loss=0.6633, Acc=0.6519 | Val Loss=0.6700, Acc=0.5981\n",
      "Epoch 03: Train Loss=0.6530, Acc=0.6500 | Val Loss=0.6700, Acc=0.6423\n",
      "Epoch 04: Train Loss=0.6605, Acc=0.6683 | Val Loss=0.6625, Acc=0.6654\n",
      "Epoch 05: Train Loss=0.6539, Acc=0.6760 | Val Loss=0.6682, Acc=0.6904\n",
      "Epoch 06: Train Loss=0.6462, Acc=0.6851 | Val Loss=0.6699, Acc=0.6846\n",
      "Epoch 07: Train Loss=0.6498, Acc=0.6832 | Val Loss=0.6695, Acc=0.6731\n",
      "Epoch 08: Train Loss=0.6486, Acc=0.6880 | Val Loss=0.6695, Acc=0.6538\n",
      "Epoch 09: Train Loss=0.6460, Acc=0.6784 | Val Loss=0.6798, Acc=0.6404\n",
      "Epoch 10: Train Loss=0.6468, Acc=0.6832 | Val Loss=0.6767, Acc=0.6673\n",
      "Epoch 11: Train Loss=0.6401, Acc=0.6798 | Val Loss=0.6814, Acc=0.6942\n",
      "Epoch 12: Train Loss=0.6396, Acc=0.6947 | Val Loss=0.6778, Acc=0.7038\n",
      "Epoch 13: Train Loss=0.6359, Acc=0.6870 | Val Loss=0.6783, Acc=0.7096\n",
      "Epoch 14: Train Loss=0.6302, Acc=0.6947 | Val Loss=0.6798, Acc=0.7135\n",
      "Epoch 15: Train Loss=0.6244, Acc=0.6966 | Val Loss=0.7180, Acc=0.6596\n",
      "Epoch 16: Train Loss=0.6235, Acc=0.6957 | Val Loss=0.6987, Acc=0.6423\n",
      "Early stopping triggered after 16 epochs\n",
      "\n",
      "Fold 2 Test Metrics:\n",
      "  Accuracy:  0.6221\n",
      "  Precision: 0.7177\n",
      "  Recall:    0.6221\n",
      "  F1 Score:  0.6542\n",
      "\n",
      "Fold 2 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8337    0.6524    0.7320       515\n",
      "           1     0.2782    0.5074    0.3594       136\n",
      "\n",
      "    accuracy                         0.6221       651\n",
      "   macro avg     0.5560    0.5799    0.5457       651\n",
      "weighted avg     0.7177    0.6221    0.6542       651\n",
      "\n",
      "\n",
      "===== HybridModel - Fold 3 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=0.6962, Acc=0.5976 | Val Loss=0.6648, Acc=0.7327\n",
      "Epoch 02: Train Loss=0.6621, Acc=0.6639 | Val Loss=0.6619, Acc=0.7077\n",
      "Epoch 03: Train Loss=0.6591, Acc=0.6457 | Val Loss=0.6626, Acc=0.6308\n",
      "Epoch 04: Train Loss=0.6580, Acc=0.6894 | Val Loss=0.6606, Acc=0.7000\n",
      "Epoch 05: Train Loss=0.6519, Acc=0.6822 | Val Loss=0.6732, Acc=0.6865\n",
      "Epoch 06: Train Loss=0.6520, Acc=0.6937 | Val Loss=0.6686, Acc=0.6654\n",
      "Epoch 07: Train Loss=0.6470, Acc=0.6702 | Val Loss=0.6764, Acc=0.6846\n",
      "Epoch 08: Train Loss=0.6409, Acc=0.6779 | Val Loss=0.6779, Acc=0.6615\n",
      "Epoch 09: Train Loss=0.6439, Acc=0.6668 | Val Loss=0.6773, Acc=0.6423\n",
      "Epoch 10: Train Loss=0.6309, Acc=0.6957 | Val Loss=0.6957, Acc=0.6192\n",
      "Epoch 11: Train Loss=0.6346, Acc=0.6740 | Val Loss=0.7146, Acc=0.6038\n",
      "Epoch 12: Train Loss=0.6376, Acc=0.6808 | Val Loss=0.6955, Acc=0.6096\n",
      "Epoch 13: Train Loss=0.6399, Acc=0.6764 | Val Loss=0.6838, Acc=0.6673\n",
      "Epoch 14: Train Loss=0.6253, Acc=0.6966 | Val Loss=0.7134, Acc=0.7135\n",
      "Epoch 15: Train Loss=0.6343, Acc=0.6803 | Val Loss=0.7066, Acc=0.6596\n",
      "Epoch 16: Train Loss=0.6347, Acc=0.6803 | Val Loss=0.7186, Acc=0.6558\n",
      "Epoch 17: Train Loss=0.6247, Acc=0.7010 | Val Loss=0.7177, Acc=0.6615\n",
      "Epoch 18: Train Loss=0.6284, Acc=0.6885 | Val Loss=0.7260, Acc=0.6577\n",
      "Epoch 19: Train Loss=0.6213, Acc=0.6889 | Val Loss=0.7159, Acc=0.6462\n",
      "Early stopping triggered after 19 epochs\n",
      "\n",
      "Fold 3 Test Metrics:\n",
      "  Accuracy:  0.6206\n",
      "  Precision: 0.6973\n",
      "  Recall:    0.6206\n",
      "  F1 Score:  0.6494\n",
      "\n",
      "Fold 3 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8146    0.6738    0.7375       515\n",
      "           1     0.2533    0.4191    0.3158       136\n",
      "\n",
      "    accuracy                         0.6206       651\n",
      "   macro avg     0.5339    0.5465    0.5267       651\n",
      "weighted avg     0.6973    0.6206    0.6494       651\n",
      "\n",
      "\n",
      "===== HybridModel - Fold 4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=0.6996, Acc=0.6337 | Val Loss=0.6759, Acc=0.6942\n",
      "Epoch 02: Train Loss=0.6623, Acc=0.6774 | Val Loss=0.6748, Acc=0.6865\n",
      "Epoch 03: Train Loss=0.6594, Acc=0.6760 | Val Loss=0.6681, Acc=0.6712\n",
      "Epoch 04: Train Loss=0.6577, Acc=0.6702 | Val Loss=0.6756, Acc=0.6808\n",
      "Epoch 05: Train Loss=0.6556, Acc=0.6981 | Val Loss=0.6702, Acc=0.6788\n",
      "Epoch 06: Train Loss=0.6566, Acc=0.6851 | Val Loss=0.6757, Acc=0.6827\n",
      "Epoch 07: Train Loss=0.6443, Acc=0.6937 | Val Loss=0.6807, Acc=0.6788\n",
      "Epoch 08: Train Loss=0.6444, Acc=0.7024 | Val Loss=0.6740, Acc=0.6962\n",
      "Epoch 09: Train Loss=0.6441, Acc=0.6981 | Val Loss=0.6831, Acc=0.6942\n",
      "Epoch 10: Train Loss=0.6430, Acc=0.7053 | Val Loss=0.6906, Acc=0.6846\n",
      "Epoch 11: Train Loss=0.6386, Acc=0.6913 | Val Loss=0.6841, Acc=0.6788\n",
      "Epoch 12: Train Loss=0.6353, Acc=0.7082 | Val Loss=0.7074, Acc=0.6788\n",
      "Epoch 13: Train Loss=0.6391, Acc=0.6846 | Val Loss=0.7034, Acc=0.6442\n",
      "Epoch 14: Train Loss=0.6382, Acc=0.6966 | Val Loss=0.6971, Acc=0.6769\n",
      "Epoch 15: Train Loss=0.6218, Acc=0.7053 | Val Loss=0.6960, Acc=0.6731\n",
      "Epoch 16: Train Loss=0.6320, Acc=0.7053 | Val Loss=0.7095, Acc=0.6808\n",
      "Epoch 17: Train Loss=0.6272, Acc=0.7072 | Val Loss=0.7007, Acc=0.6788\n",
      "Epoch 18: Train Loss=0.6245, Acc=0.6875 | Val Loss=0.7081, Acc=0.6635\n",
      "Early stopping triggered after 18 epochs\n",
      "\n",
      "Fold 4 Test Metrics:\n",
      "  Accuracy:  0.6697\n",
      "  Precision: 0.7148\n",
      "  Recall:    0.6697\n",
      "  F1 Score:  0.6882\n",
      "\n",
      "Fold 4 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8261    0.7379    0.7795       515\n",
      "           1     0.2932    0.4118    0.3425       136\n",
      "\n",
      "    accuracy                         0.6697       651\n",
      "   macro avg     0.5596    0.5748    0.5610       651\n",
      "weighted avg     0.7148    0.6697    0.6882       651\n",
      "\n",
      "\n",
      "===== HybridModel - Fold 5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=0.6965, Acc=0.6245 | Val Loss=0.6642, Acc=0.5577\n",
      "Epoch 02: Train Loss=0.6629, Acc=0.6317 | Val Loss=0.6527, Acc=0.6385\n",
      "Epoch 03: Train Loss=0.6632, Acc=0.6548 | Val Loss=0.6466, Acc=0.6846\n",
      "Epoch 04: Train Loss=0.6620, Acc=0.6567 | Val Loss=0.6494, Acc=0.6692\n",
      "Epoch 05: Train Loss=0.6543, Acc=0.6697 | Val Loss=0.6499, Acc=0.7058\n",
      "Epoch 06: Train Loss=0.6573, Acc=0.6572 | Val Loss=0.6524, Acc=0.6635\n",
      "Epoch 07: Train Loss=0.6559, Acc=0.6620 | Val Loss=0.6473, Acc=0.6769\n",
      "Epoch 08: Train Loss=0.6516, Acc=0.6625 | Val Loss=0.6464, Acc=0.6904\n",
      "Epoch 09: Train Loss=0.6560, Acc=0.6774 | Val Loss=0.6493, Acc=0.7135\n",
      "Epoch 10: Train Loss=0.6487, Acc=0.6760 | Val Loss=0.6564, Acc=0.6846\n",
      "Epoch 11: Train Loss=0.6513, Acc=0.6712 | Val Loss=0.6504, Acc=0.7135\n",
      "Epoch 12: Train Loss=0.6458, Acc=0.6649 | Val Loss=0.6492, Acc=0.7115\n",
      "Epoch 13: Train Loss=0.6499, Acc=0.6716 | Val Loss=0.6558, Acc=0.6673\n",
      "Epoch 14: Train Loss=0.6426, Acc=0.6673 | Val Loss=0.6570, Acc=0.6615\n",
      "Epoch 15: Train Loss=0.6386, Acc=0.6731 | Val Loss=0.6535, Acc=0.7192\n",
      "Epoch 16: Train Loss=0.6451, Acc=0.6793 | Val Loss=0.6539, Acc=0.6827\n",
      "Epoch 17: Train Loss=0.6387, Acc=0.6716 | Val Loss=0.6513, Acc=0.6288\n",
      "Epoch 18: Train Loss=0.6422, Acc=0.6740 | Val Loss=0.6553, Acc=0.6827\n",
      "Epoch 19: Train Loss=0.6427, Acc=0.6817 | Val Loss=0.6579, Acc=0.6673\n",
      "Epoch 20: Train Loss=0.6346, Acc=0.6808 | Val Loss=0.6537, Acc=0.6519\n",
      "Epoch 21: Train Loss=0.6442, Acc=0.6875 | Val Loss=0.6548, Acc=0.6981\n",
      "Epoch 22: Train Loss=0.6408, Acc=0.6813 | Val Loss=0.6545, Acc=0.6423\n",
      "Epoch 23: Train Loss=0.6289, Acc=0.6962 | Val Loss=0.6576, Acc=0.6615\n",
      "Early stopping triggered after 23 epochs\n",
      "\n",
      "Fold 5 Test Metrics:\n",
      "  Accuracy:  0.6283\n",
      "  Precision: 0.7100\n",
      "  Recall:    0.6283\n",
      "  F1 Score:  0.6576\n",
      "\n",
      "Fold 5 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8258    0.6718    0.7409       515\n",
      "           1     0.2716    0.4632    0.3424       136\n",
      "\n",
      "    accuracy                         0.6283       651\n",
      "   macro avg     0.5487    0.5675    0.5416       651\n",
      "weighted avg     0.7100    0.6283    0.6576       651\n",
      "\n",
      "\n",
      "===== Final Combined Results (Majority Vote) for HybridModel =====\n",
      "Final Accuracy:  0.6267\n",
      "Final Precision: 0.7127\n",
      "Final Recall:    0.6267\n",
      "Final F1 Score:  0.6570\n",
      "\n",
      "Final Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8285    0.6660    0.7384       515\n",
      "           1     0.2743    0.4779    0.3485       136\n",
      "\n",
      "    accuracy                         0.6267       651\n",
      "   macro avg     0.5514    0.5720    0.5435       651\n",
      "weighted avg     0.7127    0.6267    0.6570       651\n",
      "\n",
      "\n",
      "HybridModel - Final Majority Vote Results:\n",
      "  Accuracy:  0.6267\n",
      "  Precision: 0.7127\n",
      "  Recall:    0.6267\n",
      "  F1 Score:  0.6570\n",
      "\n",
      "=== Testing RegularizedGNN Model ===\n",
      "\n",
      "===== RegularizedGNN - Fold 1 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=0.6873, Acc=0.6668 | Val Loss=0.6768, Acc=0.6737\n",
      "Epoch 02: Train Loss=0.6754, Acc=0.6937 | Val Loss=0.6674, Acc=0.6276\n",
      "Epoch 03: Train Loss=0.6673, Acc=0.6889 | Val Loss=0.6650, Acc=0.6564\n",
      "Epoch 04: Train Loss=0.6629, Acc=0.6889 | Val Loss=0.6630, Acc=0.6411\n",
      "Epoch 05: Train Loss=0.6671, Acc=0.6813 | Val Loss=0.6631, Acc=0.6449\n",
      "Epoch 06: Train Loss=0.6670, Acc=0.6606 | Val Loss=0.6638, Acc=0.6699\n",
      "Epoch 07: Train Loss=0.6651, Acc=0.6832 | Val Loss=0.6627, Acc=0.6468\n",
      "Epoch 08: Train Loss=0.6669, Acc=0.6803 | Val Loss=0.6625, Acc=0.6564\n",
      "Epoch 09: Train Loss=0.6610, Acc=0.6962 | Val Loss=0.6628, Acc=0.6699\n",
      "Epoch 10: Train Loss=0.6638, Acc=0.6957 | Val Loss=0.6635, Acc=0.6372\n",
      "Epoch 11: Train Loss=0.6578, Acc=0.6788 | Val Loss=0.6625, Acc=0.6545\n",
      "Epoch 12: Train Loss=0.6579, Acc=0.6846 | Val Loss=0.6629, Acc=0.6353\n",
      "Epoch 13: Train Loss=0.6644, Acc=0.6721 | Val Loss=0.6625, Acc=0.6468\n",
      "Epoch 14: Train Loss=0.6631, Acc=0.6813 | Val Loss=0.6622, Acc=0.6545\n",
      "Epoch 15: Train Loss=0.6599, Acc=0.6894 | Val Loss=0.6621, Acc=0.6526\n",
      "Epoch 16: Train Loss=0.6573, Acc=0.6913 | Val Loss=0.6623, Acc=0.6507\n",
      "Epoch 17: Train Loss=0.6601, Acc=0.6803 | Val Loss=0.6624, Acc=0.6488\n",
      "Epoch 18: Train Loss=0.6586, Acc=0.6865 | Val Loss=0.6624, Acc=0.6507\n",
      "Epoch 19: Train Loss=0.6590, Acc=0.6894 | Val Loss=0.6623, Acc=0.6507\n",
      "Epoch 20: Train Loss=0.6574, Acc=0.6870 | Val Loss=0.6622, Acc=0.6526\n",
      "Epoch 21: Train Loss=0.6596, Acc=0.6899 | Val Loss=0.6621, Acc=0.6545\n",
      "Epoch 22: Train Loss=0.6629, Acc=0.6981 | Val Loss=0.6621, Acc=0.6564\n",
      "Epoch 23: Train Loss=0.6638, Acc=0.6947 | Val Loss=0.6621, Acc=0.6564\n",
      "Epoch 24: Train Loss=0.6616, Acc=0.6827 | Val Loss=0.6620, Acc=0.6564\n",
      "Epoch 25: Train Loss=0.6623, Acc=0.6885 | Val Loss=0.6621, Acc=0.6545\n",
      "Epoch 26: Train Loss=0.6602, Acc=0.6885 | Val Loss=0.6622, Acc=0.6564\n",
      "Epoch 27: Train Loss=0.6496, Acc=0.6928 | Val Loss=0.6623, Acc=0.6564\n",
      "Epoch 28: Train Loss=0.6542, Acc=0.6909 | Val Loss=0.6623, Acc=0.6564\n",
      "Epoch 29: Train Loss=0.6528, Acc=0.6962 | Val Loss=0.6624, Acc=0.6488\n",
      "Epoch 30: Train Loss=0.6579, Acc=0.6952 | Val Loss=0.6624, Acc=0.6526\n",
      "Epoch 31: Train Loss=0.6575, Acc=0.6880 | Val Loss=0.6624, Acc=0.6564\n",
      "Epoch 32: Train Loss=0.6626, Acc=0.6933 | Val Loss=0.6624, Acc=0.6564\n",
      "Epoch 33: Train Loss=0.6545, Acc=0.6971 | Val Loss=0.6623, Acc=0.6545\n",
      "Epoch 34: Train Loss=0.6558, Acc=0.6928 | Val Loss=0.6624, Acc=0.6545\n",
      "Epoch 35: Train Loss=0.6531, Acc=0.6889 | Val Loss=0.6624, Acc=0.6545\n",
      "Epoch 36: Train Loss=0.6530, Acc=0.6962 | Val Loss=0.6623, Acc=0.6545\n",
      "Epoch 37: Train Loss=0.6611, Acc=0.6880 | Val Loss=0.6623, Acc=0.6545\n",
      "Epoch 38: Train Loss=0.6607, Acc=0.6962 | Val Loss=0.6623, Acc=0.6545\n",
      "Epoch 39: Train Loss=0.6601, Acc=0.7005 | Val Loss=0.6623, Acc=0.6545\n",
      "Early stopping triggered after 39 epochs\n",
      "\n",
      "Fold 1 Test Metrics:\n",
      "  Accuracy:  0.6406\n",
      "  Precision: 0.7081\n",
      "  Recall:    0.6406\n",
      "  F1 Score:  0.6663\n",
      "\n",
      "Fold 1 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8230    0.6951    0.7537       515\n",
      "           1     0.2731    0.4338    0.3352       136\n",
      "\n",
      "    accuracy                         0.6406       651\n",
      "   macro avg     0.5481    0.5645    0.5445       651\n",
      "weighted avg     0.7081    0.6406    0.6663       651\n",
      "\n",
      "\n",
      "===== RegularizedGNN - Fold 2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=0.6882, Acc=0.5880 | Val Loss=0.6779, Acc=0.6904\n",
      "Epoch 02: Train Loss=0.6768, Acc=0.6716 | Val Loss=0.6649, Acc=0.6865\n",
      "Epoch 03: Train Loss=0.6733, Acc=0.6712 | Val Loss=0.6661, Acc=0.7019\n",
      "Epoch 04: Train Loss=0.6702, Acc=0.6851 | Val Loss=0.6647, Acc=0.6769\n",
      "Epoch 05: Train Loss=0.6690, Acc=0.6745 | Val Loss=0.6626, Acc=0.6846\n",
      "Epoch 06: Train Loss=0.6749, Acc=0.6861 | Val Loss=0.6626, Acc=0.6865\n",
      "Epoch 07: Train Loss=0.6663, Acc=0.6894 | Val Loss=0.6591, Acc=0.6750\n",
      "Epoch 08: Train Loss=0.6658, Acc=0.6760 | Val Loss=0.6608, Acc=0.6923\n",
      "Epoch 09: Train Loss=0.6673, Acc=0.6952 | Val Loss=0.6618, Acc=0.6808\n",
      "Epoch 10: Train Loss=0.6658, Acc=0.6846 | Val Loss=0.6620, Acc=0.6923\n",
      "Epoch 11: Train Loss=0.6656, Acc=0.6899 | Val Loss=0.6620, Acc=0.6885\n",
      "Epoch 12: Train Loss=0.6666, Acc=0.6865 | Val Loss=0.6604, Acc=0.6846\n",
      "Epoch 13: Train Loss=0.6678, Acc=0.6981 | Val Loss=0.6602, Acc=0.6769\n",
      "Epoch 14: Train Loss=0.6622, Acc=0.6750 | Val Loss=0.6602, Acc=0.6846\n",
      "Epoch 15: Train Loss=0.6586, Acc=0.6875 | Val Loss=0.6603, Acc=0.6865\n",
      "Epoch 16: Train Loss=0.6644, Acc=0.7034 | Val Loss=0.6607, Acc=0.6962\n",
      "Epoch 17: Train Loss=0.6626, Acc=0.6856 | Val Loss=0.6611, Acc=0.6865\n",
      "Epoch 18: Train Loss=0.6569, Acc=0.6952 | Val Loss=0.6608, Acc=0.6865\n",
      "Epoch 19: Train Loss=0.6557, Acc=0.6851 | Val Loss=0.6600, Acc=0.6827\n",
      "Epoch 20: Train Loss=0.6637, Acc=0.6817 | Val Loss=0.6601, Acc=0.6769\n",
      "Epoch 21: Train Loss=0.6587, Acc=0.6865 | Val Loss=0.6604, Acc=0.6846\n",
      "Epoch 22: Train Loss=0.6623, Acc=0.6865 | Val Loss=0.6601, Acc=0.6846\n",
      "Early stopping triggered after 22 epochs\n",
      "\n",
      "Fold 2 Test Metrics:\n",
      "  Accuracy:  0.6590\n",
      "  Precision: 0.7136\n",
      "  Recall:    0.6590\n",
      "  F1 Score:  0.6806\n",
      "\n",
      "Fold 2 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8263    0.7204    0.7697       515\n",
      "           1     0.2871    0.4265    0.3432       136\n",
      "\n",
      "    accuracy                         0.6590       651\n",
      "   macro avg     0.5567    0.5734    0.5565       651\n",
      "weighted avg     0.7136    0.6590    0.6806       651\n",
      "\n",
      "\n",
      "===== RegularizedGNN - Fold 3 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=0.6996, Acc=0.3558 | Val Loss=0.6598, Acc=0.6654\n",
      "Epoch 02: Train Loss=0.6836, Acc=0.5130 | Val Loss=0.6516, Acc=0.6577\n",
      "Epoch 03: Train Loss=0.6751, Acc=0.5779 | Val Loss=0.6561, Acc=0.5788\n",
      "Epoch 04: Train Loss=0.6719, Acc=0.5745 | Val Loss=0.6543, Acc=0.5750\n",
      "Epoch 05: Train Loss=0.6691, Acc=0.5760 | Val Loss=0.6559, Acc=0.5942\n",
      "Epoch 06: Train Loss=0.6665, Acc=0.5611 | Val Loss=0.6487, Acc=0.6519\n",
      "Epoch 07: Train Loss=0.6770, Acc=0.5740 | Val Loss=0.6497, Acc=0.6500\n",
      "Epoch 08: Train Loss=0.6713, Acc=0.5938 | Val Loss=0.6480, Acc=0.6558\n",
      "Epoch 09: Train Loss=0.6586, Acc=0.6115 | Val Loss=0.6453, Acc=0.6635\n",
      "Epoch 10: Train Loss=0.6645, Acc=0.6096 | Val Loss=0.6476, Acc=0.6750\n",
      "Epoch 11: Train Loss=0.6683, Acc=0.6197 | Val Loss=0.6529, Acc=0.6346\n",
      "Epoch 12: Train Loss=0.6643, Acc=0.5817 | Val Loss=0.6471, Acc=0.6596\n",
      "Epoch 13: Train Loss=0.6628, Acc=0.6245 | Val Loss=0.6458, Acc=0.6673\n",
      "Epoch 14: Train Loss=0.6656, Acc=0.6019 | Val Loss=0.6511, Acc=0.6288\n",
      "Epoch 15: Train Loss=0.6633, Acc=0.6014 | Val Loss=0.6518, Acc=0.6462\n",
      "Epoch 16: Train Loss=0.6659, Acc=0.5962 | Val Loss=0.6461, Acc=0.6750\n",
      "Epoch 17: Train Loss=0.6675, Acc=0.6236 | Val Loss=0.6467, Acc=0.6692\n",
      "Epoch 18: Train Loss=0.6649, Acc=0.6216 | Val Loss=0.6474, Acc=0.6885\n",
      "Epoch 19: Train Loss=0.6556, Acc=0.6236 | Val Loss=0.6458, Acc=0.6788\n",
      "Epoch 20: Train Loss=0.6631, Acc=0.6351 | Val Loss=0.6464, Acc=0.6769\n",
      "Epoch 21: Train Loss=0.6570, Acc=0.6284 | Val Loss=0.6453, Acc=0.6865\n",
      "Epoch 22: Train Loss=0.6623, Acc=0.6337 | Val Loss=0.6471, Acc=0.6731\n",
      "Epoch 23: Train Loss=0.6579, Acc=0.6423 | Val Loss=0.6454, Acc=0.6846\n",
      "Epoch 24: Train Loss=0.6605, Acc=0.6255 | Val Loss=0.6468, Acc=0.6846\n",
      "Epoch 25: Train Loss=0.6623, Acc=0.6288 | Val Loss=0.6471, Acc=0.6788\n",
      "Epoch 26: Train Loss=0.6649, Acc=0.6250 | Val Loss=0.6475, Acc=0.6827\n",
      "Epoch 27: Train Loss=0.6649, Acc=0.6298 | Val Loss=0.6479, Acc=0.6769\n",
      "Epoch 28: Train Loss=0.6592, Acc=0.6255 | Val Loss=0.6475, Acc=0.6788\n",
      "Epoch 29: Train Loss=0.6605, Acc=0.6207 | Val Loss=0.6469, Acc=0.6769\n",
      "Epoch 30: Train Loss=0.6598, Acc=0.6399 | Val Loss=0.6468, Acc=0.6827\n",
      "Epoch 31: Train Loss=0.6656, Acc=0.6365 | Val Loss=0.6473, Acc=0.6769\n",
      "Epoch 32: Train Loss=0.6656, Acc=0.6178 | Val Loss=0.6476, Acc=0.6769\n",
      "Epoch 33: Train Loss=0.6567, Acc=0.6260 | Val Loss=0.6473, Acc=0.6769\n",
      "Epoch 34: Train Loss=0.6657, Acc=0.6303 | Val Loss=0.6476, Acc=0.6769\n",
      "Epoch 35: Train Loss=0.6573, Acc=0.6221 | Val Loss=0.6473, Acc=0.6769\n",
      "Epoch 36: Train Loss=0.6620, Acc=0.6293 | Val Loss=0.6474, Acc=0.6769\n",
      "Early stopping triggered after 36 epochs\n",
      "\n",
      "Fold 3 Test Metrics:\n",
      "  Accuracy:  0.6283\n",
      "  Precision: 0.7199\n",
      "  Recall:    0.6283\n",
      "  F1 Score:  0.6593\n",
      "\n",
      "Fold 3 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8354    0.6602    0.7375       515\n",
      "           1     0.2828    0.5074    0.3632       136\n",
      "\n",
      "    accuracy                         0.6283       651\n",
      "   macro avg     0.5591    0.5838    0.5503       651\n",
      "weighted avg     0.7199    0.6283    0.6593       651\n",
      "\n",
      "\n",
      "===== RegularizedGNN - Fold 4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=0.7008, Acc=0.4688 | Val Loss=0.6734, Acc=0.6904\n",
      "Epoch 02: Train Loss=0.6732, Acc=0.6043 | Val Loss=0.6754, Acc=0.6846\n",
      "Epoch 03: Train Loss=0.6671, Acc=0.6370 | Val Loss=0.6688, Acc=0.6500\n",
      "Epoch 04: Train Loss=0.6644, Acc=0.6274 | Val Loss=0.6739, Acc=0.6827\n",
      "Epoch 05: Train Loss=0.6638, Acc=0.6452 | Val Loss=0.6672, Acc=0.6269\n",
      "Epoch 06: Train Loss=0.6639, Acc=0.6077 | Val Loss=0.6686, Acc=0.6327\n",
      "Epoch 07: Train Loss=0.6614, Acc=0.6284 | Val Loss=0.6725, Acc=0.6231\n",
      "Epoch 08: Train Loss=0.6582, Acc=0.6380 | Val Loss=0.6692, Acc=0.6692\n",
      "Epoch 09: Train Loss=0.6659, Acc=0.6519 | Val Loss=0.6684, Acc=0.6481\n",
      "Epoch 10: Train Loss=0.6597, Acc=0.6409 | Val Loss=0.6675, Acc=0.6635\n",
      "Epoch 11: Train Loss=0.6590, Acc=0.6601 | Val Loss=0.6680, Acc=0.6673\n",
      "Epoch 12: Train Loss=0.6535, Acc=0.6495 | Val Loss=0.6690, Acc=0.6673\n",
      "Epoch 13: Train Loss=0.6588, Acc=0.6712 | Val Loss=0.6699, Acc=0.6769\n",
      "Epoch 14: Train Loss=0.6582, Acc=0.6707 | Val Loss=0.6696, Acc=0.6731\n",
      "Epoch 15: Train Loss=0.6626, Acc=0.6663 | Val Loss=0.6690, Acc=0.6673\n",
      "Epoch 16: Train Loss=0.6529, Acc=0.6788 | Val Loss=0.6706, Acc=0.6769\n",
      "Epoch 17: Train Loss=0.6631, Acc=0.6635 | Val Loss=0.6687, Acc=0.6654\n",
      "Epoch 18: Train Loss=0.6517, Acc=0.6764 | Val Loss=0.6687, Acc=0.6654\n",
      "Epoch 19: Train Loss=0.6656, Acc=0.6558 | Val Loss=0.6680, Acc=0.6538\n",
      "Epoch 20: Train Loss=0.6595, Acc=0.6620 | Val Loss=0.6677, Acc=0.6577\n",
      "Early stopping triggered after 20 epochs\n",
      "\n",
      "Fold 4 Test Metrics:\n",
      "  Accuracy:  0.6298\n",
      "  Precision: 0.7106\n",
      "  Recall:    0.6298\n",
      "  F1 Score:  0.6589\n",
      "\n",
      "Fold 4 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8262    0.6738    0.7422       515\n",
      "           1     0.2727    0.4632    0.3433       136\n",
      "\n",
      "    accuracy                         0.6298       651\n",
      "   macro avg     0.5495    0.5685    0.5428       651\n",
      "weighted avg     0.7106    0.6298    0.6589       651\n",
      "\n",
      "\n",
      "===== RegularizedGNN - Fold 5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=0.6944, Acc=0.3606 | Val Loss=0.6794, Acc=0.5654\n",
      "Epoch 02: Train Loss=0.6779, Acc=0.5601 | Val Loss=0.6676, Acc=0.6827\n",
      "Epoch 03: Train Loss=0.6760, Acc=0.5543 | Val Loss=0.6634, Acc=0.6731\n",
      "Epoch 04: Train Loss=0.6752, Acc=0.6000 | Val Loss=0.6683, Acc=0.6077\n",
      "Epoch 05: Train Loss=0.6696, Acc=0.5928 | Val Loss=0.6620, Acc=0.6385\n",
      "Epoch 06: Train Loss=0.6721, Acc=0.6111 | Val Loss=0.6606, Acc=0.6385\n",
      "Epoch 07: Train Loss=0.6693, Acc=0.6529 | Val Loss=0.6605, Acc=0.6404\n",
      "Epoch 08: Train Loss=0.6669, Acc=0.6149 | Val Loss=0.6576, Acc=0.6827\n",
      "Epoch 09: Train Loss=0.6668, Acc=0.6385 | Val Loss=0.6582, Acc=0.6385\n",
      "Epoch 10: Train Loss=0.6636, Acc=0.6293 | Val Loss=0.6572, Acc=0.6462\n",
      "Epoch 11: Train Loss=0.6647, Acc=0.6279 | Val Loss=0.6569, Acc=0.6423\n",
      "Epoch 12: Train Loss=0.6623, Acc=0.6370 | Val Loss=0.6562, Acc=0.6442\n",
      "Epoch 13: Train Loss=0.6645, Acc=0.6202 | Val Loss=0.6572, Acc=0.6385\n",
      "Epoch 14: Train Loss=0.6624, Acc=0.6370 | Val Loss=0.6586, Acc=0.6212\n",
      "Epoch 15: Train Loss=0.6688, Acc=0.6245 | Val Loss=0.6583, Acc=0.6250\n",
      "Epoch 16: Train Loss=0.6615, Acc=0.6399 | Val Loss=0.6570, Acc=0.6462\n",
      "Epoch 17: Train Loss=0.6562, Acc=0.6452 | Val Loss=0.6566, Acc=0.6404\n",
      "Epoch 18: Train Loss=0.6590, Acc=0.6428 | Val Loss=0.6569, Acc=0.6404\n",
      "Epoch 19: Train Loss=0.6658, Acc=0.6529 | Val Loss=0.6576, Acc=0.6423\n",
      "Epoch 20: Train Loss=0.6663, Acc=0.6394 | Val Loss=0.6582, Acc=0.6231\n",
      "Epoch 21: Train Loss=0.6608, Acc=0.6481 | Val Loss=0.6573, Acc=0.6442\n",
      "Epoch 22: Train Loss=0.6618, Acc=0.6428 | Val Loss=0.6579, Acc=0.6365\n",
      "Epoch 23: Train Loss=0.6671, Acc=0.6471 | Val Loss=0.6577, Acc=0.6442\n",
      "Epoch 24: Train Loss=0.6646, Acc=0.6409 | Val Loss=0.6580, Acc=0.6365\n",
      "Epoch 25: Train Loss=0.6610, Acc=0.6534 | Val Loss=0.6574, Acc=0.6423\n",
      "Epoch 26: Train Loss=0.6656, Acc=0.6462 | Val Loss=0.6573, Acc=0.6404\n",
      "Epoch 27: Train Loss=0.6524, Acc=0.6538 | Val Loss=0.6573, Acc=0.6442\n",
      "Early stopping triggered after 27 epochs\n",
      "\n",
      "Fold 5 Test Metrics:\n",
      "  Accuracy:  0.6175\n",
      "  Precision: 0.7228\n",
      "  Recall:    0.6175\n",
      "  F1 Score:  0.6511\n",
      "\n",
      "Fold 5 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8393    0.6388    0.7255       515\n",
      "           1     0.2819    0.5368    0.3696       136\n",
      "\n",
      "    accuracy                         0.6175       651\n",
      "   macro avg     0.5606    0.5878    0.5475       651\n",
      "weighted avg     0.7228    0.6175    0.6511       651\n",
      "\n",
      "\n",
      "===== Final Combined Results (Majority Vote) for RegularizedGNN =====\n",
      "Final Accuracy:  0.6298\n",
      "Final Precision: 0.7089\n",
      "Final Recall:    0.6298\n",
      "Final F1 Score:  0.6586\n",
      "\n",
      "Final Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8246    0.6757    0.7428       515\n",
      "           1     0.2707    0.4559    0.3397       136\n",
      "\n",
      "    accuracy                         0.6298       651\n",
      "   macro avg     0.5477    0.5658    0.5413       651\n",
      "weighted avg     0.7089    0.6298    0.6586       651\n",
      "\n",
      "\n",
      "RegularizedGNN - Final Majority Vote Results:\n",
      "  Accuracy:  0.6298\n",
      "  Precision: 0.7089\n",
      "  Recall:    0.6298\n",
      "  F1 Score:  0.6586\n",
      "\n",
      "=== Testing LightweightGCN Model ===\n",
      "\n",
      "===== LightweightGCN - Fold 1 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=0.6743, Acc=0.6221 | Val Loss=0.6674, Acc=0.6411\n",
      "Epoch 02: Train Loss=0.6609, Acc=0.6745 | Val Loss=0.6618, Acc=0.6468\n",
      "Epoch 03: Train Loss=0.6606, Acc=0.6687 | Val Loss=0.6600, Acc=0.6622\n",
      "Epoch 04: Train Loss=0.6577, Acc=0.6813 | Val Loss=0.6603, Acc=0.6430\n",
      "Epoch 05: Train Loss=0.6536, Acc=0.6663 | Val Loss=0.6595, Acc=0.6526\n",
      "Epoch 06: Train Loss=0.6552, Acc=0.6813 | Val Loss=0.6595, Acc=0.6795\n",
      "Epoch 07: Train Loss=0.6550, Acc=0.6764 | Val Loss=0.6598, Acc=0.6449\n",
      "Epoch 08: Train Loss=0.6539, Acc=0.6856 | Val Loss=0.6614, Acc=0.6372\n",
      "Epoch 09: Train Loss=0.6509, Acc=0.6668 | Val Loss=0.6611, Acc=0.6392\n",
      "Epoch 10: Train Loss=0.6535, Acc=0.6683 | Val Loss=0.6640, Acc=0.6276\n",
      "Epoch 11: Train Loss=0.6495, Acc=0.6813 | Val Loss=0.6625, Acc=0.6276\n",
      "Epoch 12: Train Loss=0.6515, Acc=0.6784 | Val Loss=0.6619, Acc=0.6871\n",
      "Epoch 13: Train Loss=0.6509, Acc=0.6745 | Val Loss=0.6635, Acc=0.6871\n",
      "Epoch 14: Train Loss=0.6489, Acc=0.6851 | Val Loss=0.6638, Acc=0.6660\n",
      "Epoch 15: Train Loss=0.6440, Acc=0.6899 | Val Loss=0.6658, Acc=0.6200\n",
      "Epoch 16: Train Loss=0.6499, Acc=0.6764 | Val Loss=0.6620, Acc=0.6449\n",
      "Epoch 17: Train Loss=0.6499, Acc=0.6774 | Val Loss=0.6619, Acc=0.6372\n",
      "Epoch 18: Train Loss=0.6458, Acc=0.6769 | Val Loss=0.6632, Acc=0.7083\n",
      "Epoch 19: Train Loss=0.6459, Acc=0.6981 | Val Loss=0.6618, Acc=0.6468\n",
      "Epoch 20: Train Loss=0.6444, Acc=0.6774 | Val Loss=0.6629, Acc=0.6526\n",
      "Epoch 21: Train Loss=0.6441, Acc=0.6889 | Val Loss=0.6632, Acc=0.6449\n",
      "Early stopping triggered after 21 epochs\n",
      "\n",
      "Fold 1 Test Metrics:\n",
      "  Accuracy:  0.6283\n",
      "  Precision: 0.7051\n",
      "  Recall:    0.6283\n",
      "  F1 Score:  0.6567\n",
      "\n",
      "Fold 1 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8212    0.6777    0.7426       515\n",
      "           1     0.2655    0.4412    0.3315       136\n",
      "\n",
      "    accuracy                         0.6283       651\n",
      "   macro avg     0.5433    0.5594    0.5370       651\n",
      "weighted avg     0.7051    0.6283    0.6567       651\n",
      "\n",
      "\n",
      "===== LightweightGCN - Fold 2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=0.6730, Acc=0.6380 | Val Loss=0.6674, Acc=0.6423\n",
      "Epoch 02: Train Loss=0.6605, Acc=0.6630 | Val Loss=0.6622, Acc=0.6712\n",
      "Epoch 03: Train Loss=0.6584, Acc=0.6635 | Val Loss=0.6656, Acc=0.6308\n",
      "Epoch 04: Train Loss=0.6545, Acc=0.6707 | Val Loss=0.6629, Acc=0.6481\n",
      "Epoch 05: Train Loss=0.6560, Acc=0.6683 | Val Loss=0.6616, Acc=0.6596\n",
      "Epoch 06: Train Loss=0.6557, Acc=0.6654 | Val Loss=0.6607, Acc=0.6673\n",
      "Epoch 07: Train Loss=0.6519, Acc=0.6755 | Val Loss=0.6652, Acc=0.6481\n",
      "Epoch 08: Train Loss=0.6534, Acc=0.6707 | Val Loss=0.6614, Acc=0.6673\n",
      "Epoch 09: Train Loss=0.6491, Acc=0.6654 | Val Loss=0.6622, Acc=0.6615\n",
      "Epoch 10: Train Loss=0.6484, Acc=0.6697 | Val Loss=0.6613, Acc=0.6654\n",
      "Epoch 11: Train Loss=0.6503, Acc=0.6813 | Val Loss=0.6624, Acc=0.6654\n",
      "Epoch 12: Train Loss=0.6499, Acc=0.6712 | Val Loss=0.6620, Acc=0.6731\n",
      "Epoch 13: Train Loss=0.6475, Acc=0.6683 | Val Loss=0.6623, Acc=0.6750\n",
      "Epoch 14: Train Loss=0.6513, Acc=0.6822 | Val Loss=0.6619, Acc=0.6673\n",
      "Epoch 15: Train Loss=0.6480, Acc=0.6779 | Val Loss=0.6633, Acc=0.6577\n",
      "Epoch 16: Train Loss=0.6468, Acc=0.6654 | Val Loss=0.6642, Acc=0.6673\n",
      "Epoch 17: Train Loss=0.6461, Acc=0.6899 | Val Loss=0.6629, Acc=0.6731\n",
      "Epoch 18: Train Loss=0.6473, Acc=0.6760 | Val Loss=0.6629, Acc=0.6654\n",
      "Epoch 19: Train Loss=0.6457, Acc=0.6774 | Val Loss=0.6654, Acc=0.6692\n",
      "Epoch 20: Train Loss=0.6412, Acc=0.6851 | Val Loss=0.6646, Acc=0.6558\n",
      "Epoch 21: Train Loss=0.6439, Acc=0.6745 | Val Loss=0.6651, Acc=0.6635\n",
      "Early stopping triggered after 21 epochs\n",
      "\n",
      "Fold 2 Test Metrics:\n",
      "  Accuracy:  0.6436\n",
      "  Precision: 0.7141\n",
      "  Recall:    0.6436\n",
      "  F1 Score:  0.6698\n",
      "\n",
      "Fold 2 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8283    0.6932    0.7548       515\n",
      "           1     0.2818    0.4559    0.3483       136\n",
      "\n",
      "    accuracy                         0.6436       651\n",
      "   macro avg     0.5551    0.5745    0.5515       651\n",
      "weighted avg     0.7141    0.6436    0.6698       651\n",
      "\n",
      "\n",
      "===== LightweightGCN - Fold 3 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=0.6945, Acc=0.5687 | Val Loss=0.6556, Acc=0.7385\n",
      "Epoch 02: Train Loss=0.6688, Acc=0.6712 | Val Loss=0.6567, Acc=0.6404\n",
      "Epoch 03: Train Loss=0.6650, Acc=0.6567 | Val Loss=0.6511, Acc=0.6692\n",
      "Epoch 04: Train Loss=0.6648, Acc=0.6620 | Val Loss=0.6500, Acc=0.7096\n",
      "Epoch 05: Train Loss=0.6593, Acc=0.6740 | Val Loss=0.6469, Acc=0.6750\n",
      "Epoch 06: Train Loss=0.6590, Acc=0.6615 | Val Loss=0.6481, Acc=0.6981\n",
      "Epoch 07: Train Loss=0.6590, Acc=0.6659 | Val Loss=0.6475, Acc=0.6885\n",
      "Epoch 08: Train Loss=0.6565, Acc=0.6529 | Val Loss=0.6482, Acc=0.6865\n",
      "Epoch 09: Train Loss=0.6553, Acc=0.6668 | Val Loss=0.6504, Acc=0.6712\n",
      "Epoch 10: Train Loss=0.6564, Acc=0.6635 | Val Loss=0.6492, Acc=0.6788\n",
      "Epoch 11: Train Loss=0.6533, Acc=0.6495 | Val Loss=0.6519, Acc=0.7192\n",
      "Epoch 12: Train Loss=0.6514, Acc=0.6760 | Val Loss=0.6529, Acc=0.7154\n",
      "Epoch 13: Train Loss=0.6500, Acc=0.6736 | Val Loss=0.6531, Acc=0.6885\n",
      "Epoch 14: Train Loss=0.6522, Acc=0.6740 | Val Loss=0.6539, Acc=0.6750\n",
      "Epoch 15: Train Loss=0.6528, Acc=0.6784 | Val Loss=0.6538, Acc=0.6788\n",
      "Epoch 16: Train Loss=0.6499, Acc=0.6793 | Val Loss=0.6543, Acc=0.6865\n",
      "Epoch 17: Train Loss=0.6525, Acc=0.6798 | Val Loss=0.6557, Acc=0.6519\n",
      "Epoch 18: Train Loss=0.6510, Acc=0.6692 | Val Loss=0.6550, Acc=0.6692\n",
      "Epoch 19: Train Loss=0.6515, Acc=0.6731 | Val Loss=0.6553, Acc=0.7019\n",
      "Epoch 20: Train Loss=0.6475, Acc=0.6909 | Val Loss=0.6549, Acc=0.7019\n",
      "Early stopping triggered after 20 epochs\n",
      "\n",
      "Fold 3 Test Metrics:\n",
      "  Accuracy:  0.6329\n",
      "  Precision: 0.7052\n",
      "  Recall:    0.6329\n",
      "  F1 Score:  0.6601\n",
      "\n",
      "Fold 3 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8209    0.6854    0.7471       515\n",
      "           1     0.2670    0.4338    0.3305       136\n",
      "\n",
      "    accuracy                         0.6329       651\n",
      "   macro avg     0.5439    0.5596    0.5388       651\n",
      "weighted avg     0.7052    0.6329    0.6601       651\n",
      "\n",
      "\n",
      "===== LightweightGCN - Fold 4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=0.6772, Acc=0.5889 | Val Loss=0.6756, Acc=0.6827\n",
      "Epoch 02: Train Loss=0.6597, Acc=0.6774 | Val Loss=0.6718, Acc=0.6788\n",
      "Epoch 03: Train Loss=0.6596, Acc=0.6745 | Val Loss=0.6710, Acc=0.6615\n",
      "Epoch 04: Train Loss=0.6555, Acc=0.6678 | Val Loss=0.6732, Acc=0.6096\n",
      "Epoch 05: Train Loss=0.6554, Acc=0.6721 | Val Loss=0.6690, Acc=0.6500\n",
      "Epoch 06: Train Loss=0.6539, Acc=0.6779 | Val Loss=0.6703, Acc=0.6615\n",
      "Epoch 07: Train Loss=0.6528, Acc=0.6755 | Val Loss=0.6722, Acc=0.6808\n",
      "Epoch 08: Train Loss=0.6517, Acc=0.6764 | Val Loss=0.6711, Acc=0.6154\n",
      "Epoch 09: Train Loss=0.6481, Acc=0.6538 | Val Loss=0.6749, Acc=0.6827\n",
      "Epoch 10: Train Loss=0.6513, Acc=0.6702 | Val Loss=0.6733, Acc=0.6731\n",
      "Epoch 11: Train Loss=0.6485, Acc=0.6899 | Val Loss=0.6722, Acc=0.6365\n",
      "Epoch 12: Train Loss=0.6470, Acc=0.6798 | Val Loss=0.6737, Acc=0.6712\n",
      "Epoch 13: Train Loss=0.6477, Acc=0.6808 | Val Loss=0.6742, Acc=0.6712\n",
      "Epoch 14: Train Loss=0.6462, Acc=0.6822 | Val Loss=0.6737, Acc=0.6673\n",
      "Epoch 15: Train Loss=0.6459, Acc=0.6779 | Val Loss=0.6727, Acc=0.6635\n",
      "Epoch 16: Train Loss=0.6494, Acc=0.6788 | Val Loss=0.6733, Acc=0.6673\n",
      "Epoch 17: Train Loss=0.6467, Acc=0.6846 | Val Loss=0.6726, Acc=0.6635\n",
      "Epoch 18: Train Loss=0.6432, Acc=0.6846 | Val Loss=0.6734, Acc=0.6673\n",
      "Epoch 19: Train Loss=0.6423, Acc=0.6832 | Val Loss=0.6741, Acc=0.6615\n",
      "Epoch 20: Train Loss=0.6436, Acc=0.6731 | Val Loss=0.6741, Acc=0.6615\n",
      "Early stopping triggered after 20 epochs\n",
      "\n",
      "Fold 4 Test Metrics:\n",
      "  Accuracy:  0.6390\n",
      "  Precision: 0.7092\n",
      "  Recall:    0.6390\n",
      "  F1 Score:  0.6654\n",
      "\n",
      "Fold 4 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8241    0.6913    0.7518       515\n",
      "           1     0.2740    0.4412    0.3380       136\n",
      "\n",
      "    accuracy                         0.6390       651\n",
      "   macro avg     0.5490    0.5662    0.5449       651\n",
      "weighted avg     0.7092    0.6390    0.6654       651\n",
      "\n",
      "\n",
      "===== LightweightGCN - Fold 5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=0.6851, Acc=0.5678 | Val Loss=0.6603, Acc=0.7058\n",
      "Epoch 02: Train Loss=0.6635, Acc=0.6837 | Val Loss=0.6587, Acc=0.6558\n",
      "Epoch 03: Train Loss=0.6591, Acc=0.6745 | Val Loss=0.6569, Acc=0.7192\n",
      "Epoch 04: Train Loss=0.6590, Acc=0.6808 | Val Loss=0.6566, Acc=0.7212\n",
      "Epoch 05: Train Loss=0.6586, Acc=0.6774 | Val Loss=0.6545, Acc=0.6981\n",
      "Epoch 06: Train Loss=0.6550, Acc=0.6928 | Val Loss=0.6568, Acc=0.6442\n",
      "Epoch 07: Train Loss=0.6543, Acc=0.6803 | Val Loss=0.6561, Acc=0.7058\n",
      "Epoch 08: Train Loss=0.6567, Acc=0.6663 | Val Loss=0.6605, Acc=0.6231\n",
      "Epoch 09: Train Loss=0.6546, Acc=0.6784 | Val Loss=0.6541, Acc=0.6481\n",
      "Epoch 10: Train Loss=0.6560, Acc=0.6673 | Val Loss=0.6545, Acc=0.6942\n",
      "Epoch 11: Train Loss=0.6508, Acc=0.6865 | Val Loss=0.6540, Acc=0.6788\n",
      "Epoch 12: Train Loss=0.6504, Acc=0.6769 | Val Loss=0.6548, Acc=0.6692\n",
      "Epoch 13: Train Loss=0.6509, Acc=0.6683 | Val Loss=0.6543, Acc=0.6865\n",
      "Epoch 14: Train Loss=0.6491, Acc=0.6822 | Val Loss=0.6542, Acc=0.6731\n",
      "Epoch 15: Train Loss=0.6488, Acc=0.6764 | Val Loss=0.6544, Acc=0.6923\n",
      "Epoch 16: Train Loss=0.6510, Acc=0.6942 | Val Loss=0.6547, Acc=0.6750\n",
      "Epoch 17: Train Loss=0.6483, Acc=0.6793 | Val Loss=0.6546, Acc=0.6731\n",
      "Epoch 18: Train Loss=0.6469, Acc=0.6937 | Val Loss=0.6544, Acc=0.6731\n",
      "Epoch 19: Train Loss=0.6457, Acc=0.6687 | Val Loss=0.6543, Acc=0.6712\n",
      "Epoch 20: Train Loss=0.6474, Acc=0.6889 | Val Loss=0.6567, Acc=0.6385\n",
      "Epoch 21: Train Loss=0.6477, Acc=0.6687 | Val Loss=0.6541, Acc=0.6731\n",
      "Epoch 22: Train Loss=0.6485, Acc=0.6731 | Val Loss=0.6542, Acc=0.6712\n",
      "Epoch 23: Train Loss=0.6427, Acc=0.6846 | Val Loss=0.6548, Acc=0.6673\n",
      "Epoch 24: Train Loss=0.6470, Acc=0.6692 | Val Loss=0.6545, Acc=0.6692\n",
      "Epoch 25: Train Loss=0.6466, Acc=0.6832 | Val Loss=0.6549, Acc=0.6731\n",
      "Epoch 26: Train Loss=0.6444, Acc=0.6904 | Val Loss=0.6551, Acc=0.6692\n",
      "Early stopping triggered after 26 epochs\n",
      "\n",
      "Fold 5 Test Metrics:\n",
      "  Accuracy:  0.6390\n",
      "  Precision: 0.7075\n",
      "  Recall:    0.6390\n",
      "  F1 Score:  0.6650\n",
      "\n",
      "Fold 5 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8226    0.6932    0.7524       515\n",
      "           1     0.2719    0.4338    0.3343       136\n",
      "\n",
      "    accuracy                         0.6390       651\n",
      "   macro avg     0.5472    0.5635    0.5433       651\n",
      "weighted avg     0.7075    0.6390    0.6650       651\n",
      "\n",
      "\n",
      "===== Final Combined Results (Majority Vote) for LightweightGCN =====\n",
      "Final Accuracy:  0.6344\n",
      "Final Precision: 0.7074\n",
      "Final Recall:    0.6344\n",
      "Final F1 Score:  0.6617\n",
      "\n",
      "Final Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8228    0.6854    0.7479       515\n",
      "           1     0.2703    0.4412    0.3352       136\n",
      "\n",
      "    accuracy                         0.6344       651\n",
      "   macro avg     0.5466    0.5633    0.5415       651\n",
      "weighted avg     0.7074    0.6344    0.6617       651\n",
      "\n",
      "\n",
      "LightweightGCN - Final Majority Vote Results:\n",
      "  Accuracy:  0.6344\n",
      "  Precision: 0.7074\n",
      "  Recall:    0.6344\n",
      "  F1 Score:  0.6617\n",
      "\n",
      "=== Testing BalancedGAT Model ===\n",
      "\n",
      "===== BalancedGAT - Fold 1 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=0.6863, Acc=0.6034 | Val Loss=0.6816, Acc=0.6372\n",
      "Epoch 02: Train Loss=0.6736, Acc=0.6361 | Val Loss=0.6713, Acc=0.6660\n",
      "Epoch 03: Train Loss=0.6803, Acc=0.6596 | Val Loss=0.6763, Acc=0.6392\n",
      "Epoch 04: Train Loss=0.6721, Acc=0.6894 | Val Loss=0.6796, Acc=0.6718\n",
      "Epoch 05: Train Loss=0.6732, Acc=0.6279 | Val Loss=0.6915, Acc=0.6948\n",
      "Epoch 06: Train Loss=0.6682, Acc=0.6630 | Val Loss=0.6837, Acc=0.6910\n",
      "Epoch 07: Train Loss=0.6614, Acc=0.6899 | Val Loss=0.6821, Acc=0.6775\n",
      "Epoch 08: Train Loss=0.6681, Acc=0.6716 | Val Loss=0.6857, Acc=0.6967\n",
      "Epoch 09: Train Loss=0.6611, Acc=0.6981 | Val Loss=0.6682, Acc=0.6718\n",
      "Epoch 10: Train Loss=0.6666, Acc=0.6769 | Val Loss=0.6658, Acc=0.6468\n",
      "Epoch 11: Train Loss=0.6664, Acc=0.6486 | Val Loss=0.6699, Acc=0.6622\n",
      "Epoch 12: Train Loss=0.6636, Acc=0.6899 | Val Loss=0.6724, Acc=0.6392\n",
      "Epoch 13: Train Loss=0.6639, Acc=0.6683 | Val Loss=0.6784, Acc=0.6603\n",
      "Epoch 14: Train Loss=0.6608, Acc=0.6654 | Val Loss=0.6846, Acc=0.6967\n",
      "Epoch 15: Train Loss=0.6647, Acc=0.6976 | Val Loss=0.6716, Acc=0.6353\n",
      "Epoch 16: Train Loss=0.6623, Acc=0.6740 | Val Loss=0.6738, Acc=0.6468\n",
      "Epoch 17: Train Loss=0.6548, Acc=0.6697 | Val Loss=0.6742, Acc=0.6411\n",
      "Epoch 18: Train Loss=0.6566, Acc=0.6673 | Val Loss=0.6768, Acc=0.6468\n",
      "Epoch 19: Train Loss=0.6596, Acc=0.6596 | Val Loss=0.6758, Acc=0.6718\n",
      "Epoch 20: Train Loss=0.6571, Acc=0.6755 | Val Loss=0.6775, Acc=0.6718\n",
      "Epoch 21: Train Loss=0.6556, Acc=0.6880 | Val Loss=0.6737, Acc=0.6660\n",
      "Epoch 22: Train Loss=0.6573, Acc=0.6745 | Val Loss=0.6750, Acc=0.6507\n",
      "Epoch 23: Train Loss=0.6557, Acc=0.6630 | Val Loss=0.6767, Acc=0.6545\n",
      "Epoch 24: Train Loss=0.6551, Acc=0.6716 | Val Loss=0.6773, Acc=0.6507\n",
      "Epoch 25: Train Loss=0.6566, Acc=0.6731 | Val Loss=0.6774, Acc=0.6679\n",
      "Early stopping triggered after 25 epochs\n",
      "\n",
      "Fold 1 Test Metrics:\n",
      "  Accuracy:  0.6467\n",
      "  Precision: 0.7105\n",
      "  Recall:    0.6467\n",
      "  F1 Score:  0.6712\n",
      "\n",
      "Fold 1 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8246    0.7029    0.7589       515\n",
      "           1     0.2783    0.4338    0.3391       136\n",
      "\n",
      "    accuracy                         0.6467       651\n",
      "   macro avg     0.5515    0.5684    0.5490       651\n",
      "weighted avg     0.7105    0.6467    0.6712       651\n",
      "\n",
      "\n",
      "===== BalancedGAT - Fold 2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=0.6788, Acc=0.6072 | Val Loss=0.6750, Acc=0.6346\n",
      "Epoch 02: Train Loss=0.6719, Acc=0.6332 | Val Loss=0.6704, Acc=0.5846\n",
      "Epoch 03: Train Loss=0.6752, Acc=0.6543 | Val Loss=0.6670, Acc=0.6308\n",
      "Epoch 04: Train Loss=0.6705, Acc=0.6356 | Val Loss=0.6678, Acc=0.6808\n",
      "Epoch 05: Train Loss=0.6709, Acc=0.6817 | Val Loss=0.6643, Acc=0.6519\n",
      "Epoch 06: Train Loss=0.6634, Acc=0.6904 | Val Loss=0.6634, Acc=0.6519\n",
      "Epoch 07: Train Loss=0.6648, Acc=0.6731 | Val Loss=0.6615, Acc=0.6481\n",
      "Epoch 08: Train Loss=0.6651, Acc=0.6630 | Val Loss=0.6654, Acc=0.6404\n",
      "Epoch 09: Train Loss=0.6629, Acc=0.6716 | Val Loss=0.6665, Acc=0.6327\n",
      "Epoch 10: Train Loss=0.6640, Acc=0.6625 | Val Loss=0.6719, Acc=0.6019\n",
      "Epoch 11: Train Loss=0.6599, Acc=0.6625 | Val Loss=0.6684, Acc=0.6269\n",
      "Epoch 12: Train Loss=0.6608, Acc=0.6447 | Val Loss=0.6667, Acc=0.6365\n",
      "Epoch 13: Train Loss=0.6618, Acc=0.6630 | Val Loss=0.6694, Acc=0.6365\n",
      "Epoch 14: Train Loss=0.6599, Acc=0.6663 | Val Loss=0.6713, Acc=0.6231\n",
      "Epoch 15: Train Loss=0.6602, Acc=0.6639 | Val Loss=0.6726, Acc=0.6135\n",
      "Epoch 16: Train Loss=0.6577, Acc=0.6601 | Val Loss=0.6711, Acc=0.6231\n",
      "Epoch 17: Train Loss=0.6574, Acc=0.6687 | Val Loss=0.6689, Acc=0.6404\n",
      "Epoch 18: Train Loss=0.6618, Acc=0.6726 | Val Loss=0.6689, Acc=0.6346\n",
      "Epoch 19: Train Loss=0.6574, Acc=0.6788 | Val Loss=0.6691, Acc=0.6327\n",
      "Epoch 20: Train Loss=0.6584, Acc=0.6692 | Val Loss=0.6689, Acc=0.6327\n",
      "Epoch 21: Train Loss=0.6616, Acc=0.6788 | Val Loss=0.6687, Acc=0.6385\n",
      "Epoch 22: Train Loss=0.6616, Acc=0.6644 | Val Loss=0.6691, Acc=0.6327\n",
      "Early stopping triggered after 22 epochs\n",
      "\n",
      "Fold 2 Test Metrics:\n",
      "  Accuracy:  0.5945\n",
      "  Precision: 0.7199\n",
      "  Recall:    0.5945\n",
      "  F1 Score:  0.6317\n",
      "\n",
      "Fold 2 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8383    0.6039    0.7020       515\n",
      "           1     0.2714    0.5588    0.3654       136\n",
      "\n",
      "    accuracy                         0.5945       651\n",
      "   macro avg     0.5549    0.5814    0.5337       651\n",
      "weighted avg     0.7199    0.5945    0.6317       651\n",
      "\n",
      "\n",
      "===== BalancedGAT - Fold 3 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=0.7002, Acc=0.5327 | Val Loss=0.6829, Acc=0.6173\n",
      "Epoch 02: Train Loss=0.6825, Acc=0.6236 | Val Loss=0.6699, Acc=0.6923\n",
      "Epoch 03: Train Loss=0.6780, Acc=0.6582 | Val Loss=0.6682, Acc=0.6885\n",
      "Epoch 04: Train Loss=0.6769, Acc=0.6543 | Val Loss=0.6653, Acc=0.6250\n",
      "Epoch 05: Train Loss=0.6661, Acc=0.6832 | Val Loss=0.6666, Acc=0.5769\n",
      "Epoch 06: Train Loss=0.6702, Acc=0.6668 | Val Loss=0.6629, Acc=0.6269\n",
      "Epoch 07: Train Loss=0.6686, Acc=0.6577 | Val Loss=0.6711, Acc=0.5635\n",
      "Epoch 08: Train Loss=0.6678, Acc=0.6909 | Val Loss=0.6730, Acc=0.5115\n",
      "Epoch 09: Train Loss=0.6632, Acc=0.6764 | Val Loss=0.6673, Acc=0.5615\n",
      "Epoch 10: Train Loss=0.6669, Acc=0.6745 | Val Loss=0.6733, Acc=0.4365\n",
      "Epoch 11: Train Loss=0.6638, Acc=0.6721 | Val Loss=0.6684, Acc=0.5731\n",
      "Epoch 12: Train Loss=0.6642, Acc=0.6702 | Val Loss=0.6704, Acc=0.6077\n",
      "Epoch 13: Train Loss=0.6647, Acc=0.6885 | Val Loss=0.6666, Acc=0.6519\n",
      "Epoch 14: Train Loss=0.6604, Acc=0.7000 | Val Loss=0.6665, Acc=0.6154\n",
      "Epoch 15: Train Loss=0.6645, Acc=0.6894 | Val Loss=0.6626, Acc=0.6596\n",
      "Epoch 16: Train Loss=0.6615, Acc=0.6856 | Val Loss=0.6624, Acc=0.6712\n",
      "Epoch 17: Train Loss=0.6576, Acc=0.6894 | Val Loss=0.6649, Acc=0.6173\n",
      "Epoch 18: Train Loss=0.6629, Acc=0.6933 | Val Loss=0.6673, Acc=0.6019\n",
      "Epoch 19: Train Loss=0.6599, Acc=0.6793 | Val Loss=0.6674, Acc=0.5942\n",
      "Epoch 20: Train Loss=0.6637, Acc=0.6774 | Val Loss=0.6687, Acc=0.5692\n",
      "Epoch 21: Train Loss=0.6578, Acc=0.6721 | Val Loss=0.6692, Acc=0.5596\n",
      "Epoch 22: Train Loss=0.6562, Acc=0.6769 | Val Loss=0.6675, Acc=0.5769\n",
      "Epoch 23: Train Loss=0.6601, Acc=0.6716 | Val Loss=0.6676, Acc=0.5769\n",
      "Epoch 24: Train Loss=0.6592, Acc=0.6668 | Val Loss=0.6688, Acc=0.5712\n",
      "Epoch 25: Train Loss=0.6630, Acc=0.6764 | Val Loss=0.6678, Acc=0.5846\n",
      "Epoch 26: Train Loss=0.6555, Acc=0.6817 | Val Loss=0.6690, Acc=0.5788\n",
      "Epoch 27: Train Loss=0.6602, Acc=0.6774 | Val Loss=0.6694, Acc=0.5692\n",
      "Epoch 28: Train Loss=0.6582, Acc=0.6817 | Val Loss=0.6695, Acc=0.5712\n",
      "Epoch 29: Train Loss=0.6614, Acc=0.6596 | Val Loss=0.6703, Acc=0.5596\n",
      "Epoch 30: Train Loss=0.6599, Acc=0.6817 | Val Loss=0.6703, Acc=0.5654\n",
      "Epoch 31: Train Loss=0.6600, Acc=0.6673 | Val Loss=0.6692, Acc=0.5731\n",
      "Early stopping triggered after 31 epochs\n",
      "\n",
      "Fold 3 Test Metrics:\n",
      "  Accuracy:  0.5561\n",
      "  Precision: 0.7250\n",
      "  Recall:    0.5561\n",
      "  F1 Score:  0.5972\n",
      "\n",
      "Fold 3 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8466    0.5359    0.6564       515\n",
      "           1     0.2646    0.6324    0.3731       136\n",
      "\n",
      "    accuracy                         0.5561       651\n",
      "   macro avg     0.5556    0.5841    0.5147       651\n",
      "weighted avg     0.7250    0.5561    0.5972       651\n",
      "\n",
      "\n",
      "===== BalancedGAT - Fold 4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=0.6932, Acc=0.6087 | Val Loss=0.6815, Acc=0.6096\n",
      "Epoch 02: Train Loss=0.6712, Acc=0.6788 | Val Loss=0.6836, Acc=0.6481\n",
      "Epoch 03: Train Loss=0.6697, Acc=0.6937 | Val Loss=0.6801, Acc=0.6577\n",
      "Epoch 04: Train Loss=0.6692, Acc=0.6990 | Val Loss=0.6778, Acc=0.6500\n",
      "Epoch 05: Train Loss=0.6717, Acc=0.6942 | Val Loss=0.6789, Acc=0.6865\n",
      "Epoch 06: Train Loss=0.6601, Acc=0.7014 | Val Loss=0.6798, Acc=0.6423\n",
      "Epoch 07: Train Loss=0.6608, Acc=0.6971 | Val Loss=0.6714, Acc=0.6596\n",
      "Epoch 08: Train Loss=0.6654, Acc=0.6952 | Val Loss=0.6778, Acc=0.6058\n",
      "Epoch 09: Train Loss=0.6622, Acc=0.7159 | Val Loss=0.6752, Acc=0.6077\n",
      "Epoch 10: Train Loss=0.6616, Acc=0.7091 | Val Loss=0.6717, Acc=0.6154\n",
      "Epoch 11: Train Loss=0.6632, Acc=0.7077 | Val Loss=0.6761, Acc=0.6096\n",
      "Epoch 12: Train Loss=0.6646, Acc=0.7173 | Val Loss=0.6748, Acc=0.6077\n",
      "Epoch 13: Train Loss=0.6623, Acc=0.7154 | Val Loss=0.6731, Acc=0.6212\n",
      "Epoch 14: Train Loss=0.6578, Acc=0.7212 | Val Loss=0.6794, Acc=0.6000\n",
      "Epoch 15: Train Loss=0.6567, Acc=0.7000 | Val Loss=0.6713, Acc=0.6327\n",
      "Epoch 16: Train Loss=0.6592, Acc=0.7144 | Val Loss=0.6735, Acc=0.6538\n",
      "Epoch 17: Train Loss=0.6547, Acc=0.7130 | Val Loss=0.6727, Acc=0.6673\n",
      "Epoch 18: Train Loss=0.6524, Acc=0.7077 | Val Loss=0.6723, Acc=0.6615\n",
      "Epoch 19: Train Loss=0.6573, Acc=0.7188 | Val Loss=0.6728, Acc=0.6769\n",
      "Epoch 20: Train Loss=0.6621, Acc=0.7101 | Val Loss=0.6712, Acc=0.6808\n",
      "Epoch 21: Train Loss=0.6521, Acc=0.7072 | Val Loss=0.6726, Acc=0.6500\n",
      "Epoch 22: Train Loss=0.6554, Acc=0.7010 | Val Loss=0.6708, Acc=0.6500\n",
      "Epoch 23: Train Loss=0.6542, Acc=0.7014 | Val Loss=0.6696, Acc=0.6519\n",
      "Epoch 24: Train Loss=0.6554, Acc=0.7072 | Val Loss=0.6693, Acc=0.6538\n",
      "Epoch 25: Train Loss=0.6541, Acc=0.7043 | Val Loss=0.6701, Acc=0.6404\n",
      "Epoch 26: Train Loss=0.6595, Acc=0.6870 | Val Loss=0.6703, Acc=0.6500\n",
      "Epoch 27: Train Loss=0.6562, Acc=0.6933 | Val Loss=0.6701, Acc=0.6538\n",
      "Epoch 28: Train Loss=0.6604, Acc=0.6913 | Val Loss=0.6691, Acc=0.6500\n",
      "Epoch 29: Train Loss=0.6559, Acc=0.6990 | Val Loss=0.6691, Acc=0.6481\n",
      "Epoch 30: Train Loss=0.6563, Acc=0.6976 | Val Loss=0.6691, Acc=0.6462\n",
      "Epoch 31: Train Loss=0.6578, Acc=0.6923 | Val Loss=0.6689, Acc=0.6481\n",
      "Epoch 32: Train Loss=0.6528, Acc=0.7000 | Val Loss=0.6687, Acc=0.6481\n",
      "Epoch 33: Train Loss=0.6462, Acc=0.6942 | Val Loss=0.6686, Acc=0.6481\n",
      "Epoch 34: Train Loss=0.6608, Acc=0.6909 | Val Loss=0.6687, Acc=0.6481\n",
      "Epoch 35: Train Loss=0.6552, Acc=0.6947 | Val Loss=0.6683, Acc=0.6500\n",
      "Epoch 36: Train Loss=0.6520, Acc=0.6937 | Val Loss=0.6683, Acc=0.6519\n",
      "Epoch 37: Train Loss=0.6510, Acc=0.6894 | Val Loss=0.6682, Acc=0.6538\n",
      "Epoch 38: Train Loss=0.6528, Acc=0.6957 | Val Loss=0.6680, Acc=0.6519\n",
      "Epoch 39: Train Loss=0.6540, Acc=0.6937 | Val Loss=0.6679, Acc=0.6538\n",
      "Epoch 40: Train Loss=0.6520, Acc=0.6937 | Val Loss=0.6676, Acc=0.6596\n",
      "Epoch 41: Train Loss=0.6505, Acc=0.6966 | Val Loss=0.6678, Acc=0.6577\n",
      "Epoch 42: Train Loss=0.6515, Acc=0.6880 | Val Loss=0.6679, Acc=0.6519\n",
      "Epoch 43: Train Loss=0.6493, Acc=0.6986 | Val Loss=0.6679, Acc=0.6577\n",
      "Epoch 44: Train Loss=0.6572, Acc=0.6899 | Val Loss=0.6681, Acc=0.6558\n",
      "Epoch 45: Train Loss=0.6561, Acc=0.6947 | Val Loss=0.6681, Acc=0.6577\n",
      "Epoch 46: Train Loss=0.6544, Acc=0.6918 | Val Loss=0.6680, Acc=0.6538\n",
      "Epoch 47: Train Loss=0.6566, Acc=0.6909 | Val Loss=0.6679, Acc=0.6519\n",
      "Epoch 48: Train Loss=0.6546, Acc=0.6933 | Val Loss=0.6679, Acc=0.6519\n",
      "Epoch 49: Train Loss=0.6502, Acc=0.6894 | Val Loss=0.6679, Acc=0.6519\n",
      "Epoch 50: Train Loss=0.6540, Acc=0.6971 | Val Loss=0.6679, Acc=0.6538\n",
      "Epoch 51: Train Loss=0.6567, Acc=0.7005 | Val Loss=0.6679, Acc=0.6519\n",
      "Epoch 52: Train Loss=0.6557, Acc=0.6918 | Val Loss=0.6679, Acc=0.6519\n",
      "Epoch 53: Train Loss=0.6545, Acc=0.6923 | Val Loss=0.6679, Acc=0.6519\n",
      "Epoch 54: Train Loss=0.6558, Acc=0.7000 | Val Loss=0.6679, Acc=0.6519\n",
      "Epoch 55: Train Loss=0.6549, Acc=0.6933 | Val Loss=0.6678, Acc=0.6519\n",
      "Early stopping triggered after 55 epochs\n",
      "\n",
      "Fold 4 Test Metrics:\n",
      "  Accuracy:  0.6313\n",
      "  Precision: 0.7144\n",
      "  Recall:    0.6313\n",
      "  F1 Score:  0.6608\n",
      "\n",
      "Fold 4 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8297    0.6718    0.7425       515\n",
      "           1     0.2778    0.4779    0.3514       136\n",
      "\n",
      "    accuracy                         0.6313       651\n",
      "   macro avg     0.5538    0.5749    0.5469       651\n",
      "weighted avg     0.7144    0.6313    0.6608       651\n",
      "\n",
      "\n",
      "===== BalancedGAT - Fold 5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=0.7083, Acc=0.5846 | Val Loss=0.7088, Acc=0.2308\n",
      "Epoch 02: Train Loss=0.6764, Acc=0.6413 | Val Loss=0.6752, Acc=0.6442\n",
      "Epoch 03: Train Loss=0.6743, Acc=0.6731 | Val Loss=0.6755, Acc=0.6288\n",
      "Epoch 04: Train Loss=0.6734, Acc=0.6457 | Val Loss=0.6619, Acc=0.7019\n",
      "Epoch 05: Train Loss=0.6719, Acc=0.6846 | Val Loss=0.6754, Acc=0.6000\n",
      "Epoch 06: Train Loss=0.6657, Acc=0.6649 | Val Loss=0.6688, Acc=0.6558\n",
      "Epoch 07: Train Loss=0.6633, Acc=0.6779 | Val Loss=0.6650, Acc=0.6500\n",
      "Epoch 08: Train Loss=0.6660, Acc=0.6721 | Val Loss=0.6950, Acc=0.5365\n",
      "Epoch 09: Train Loss=0.6645, Acc=0.6471 | Val Loss=0.6622, Acc=0.6692\n",
      "Epoch 10: Train Loss=0.6558, Acc=0.6543 | Val Loss=0.6693, Acc=0.6269\n",
      "Epoch 11: Train Loss=0.6618, Acc=0.6178 | Val Loss=0.6680, Acc=0.6269\n",
      "Epoch 12: Train Loss=0.6640, Acc=0.6433 | Val Loss=0.6638, Acc=0.6481\n",
      "Epoch 13: Train Loss=0.6629, Acc=0.6707 | Val Loss=0.6637, Acc=0.6462\n",
      "Epoch 14: Train Loss=0.6626, Acc=0.6486 | Val Loss=0.6685, Acc=0.6135\n",
      "Epoch 15: Train Loss=0.6625, Acc=0.6361 | Val Loss=0.6671, Acc=0.6212\n",
      "Epoch 16: Train Loss=0.6594, Acc=0.6587 | Val Loss=0.6667, Acc=0.6154\n",
      "Epoch 17: Train Loss=0.6584, Acc=0.6495 | Val Loss=0.6638, Acc=0.6250\n",
      "Epoch 18: Train Loss=0.6617, Acc=0.6553 | Val Loss=0.6648, Acc=0.6250\n",
      "Epoch 19: Train Loss=0.6589, Acc=0.6543 | Val Loss=0.6642, Acc=0.6385\n",
      "Early stopping triggered after 19 epochs\n",
      "\n",
      "Fold 5 Test Metrics:\n",
      "  Accuracy:  0.6252\n",
      "  Precision: 0.7290\n",
      "  Recall:    0.6252\n",
      "  F1 Score:  0.6580\n",
      "\n",
      "Fold 5 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8448    0.6447    0.7313       515\n",
      "           1     0.2907    0.5515    0.3807       136\n",
      "\n",
      "    accuracy                         0.6252       651\n",
      "   macro avg     0.5677    0.5981    0.5560       651\n",
      "weighted avg     0.7290    0.6252    0.6580       651\n",
      "\n",
      "\n",
      "===== Final Combined Results (Majority Vote) for BalancedGAT =====\n",
      "Final Accuracy:  0.6190\n",
      "Final Precision: 0.7251\n",
      "Final Recall:    0.6190\n",
      "Final F1 Score:  0.6526\n",
      "\n",
      "Final Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8414    0.6388    0.7263       515\n",
      "           1     0.2846    0.5441    0.3737       136\n",
      "\n",
      "    accuracy                         0.6190       651\n",
      "   macro avg     0.5630    0.5915    0.5500       651\n",
      "weighted avg     0.7251    0.6190    0.6526       651\n",
      "\n",
      "\n",
      "BalancedGAT - Final Majority Vote Results:\n",
      "  Accuracy:  0.6190\n",
      "  Precision: 0.7251\n",
      "  Recall:    0.6190\n",
      "  F1 Score:  0.6526\n",
      "\n",
      "=== Testing MultiGNN Model ===\n",
      "\n",
      "===== MultiGNN - Fold 1 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=1.6892, Acc=0.4062 | Val Loss=0.7091, Acc=0.2265\n",
      "Epoch 02: Train Loss=0.7237, Acc=0.3889 | Val Loss=0.6910, Acc=0.3455\n",
      "Epoch 03: Train Loss=0.7172, Acc=0.3625 | Val Loss=0.6824, Acc=0.5720\n",
      "Epoch 04: Train Loss=0.6948, Acc=0.3928 | Val Loss=0.6799, Acc=0.5873\n",
      "Epoch 05: Train Loss=0.7049, Acc=0.4250 | Val Loss=0.6869, Acc=0.4702\n",
      "Epoch 06: Train Loss=0.7101, Acc=0.4298 | Val Loss=0.6794, Acc=0.5873\n",
      "Epoch 07: Train Loss=0.7126, Acc=0.4236 | Val Loss=0.6806, Acc=0.5259\n",
      "Epoch 08: Train Loss=0.6942, Acc=0.4500 | Val Loss=0.6775, Acc=0.6372\n",
      "Epoch 09: Train Loss=0.6952, Acc=0.4332 | Val Loss=0.6803, Acc=0.6180\n",
      "Epoch 10: Train Loss=0.7113, Acc=0.4957 | Val Loss=0.6765, Acc=0.6468\n",
      "Epoch 11: Train Loss=0.6931, Acc=0.4865 | Val Loss=0.6781, Acc=0.5797\n",
      "Epoch 12: Train Loss=0.6930, Acc=0.4981 | Val Loss=0.6794, Acc=0.5931\n",
      "Epoch 13: Train Loss=0.6874, Acc=0.4788 | Val Loss=0.6772, Acc=0.6449\n",
      "Epoch 14: Train Loss=0.6992, Acc=0.4736 | Val Loss=0.6833, Acc=0.5797\n",
      "Epoch 15: Train Loss=0.6891, Acc=0.5101 | Val Loss=0.6811, Acc=0.6257\n",
      "Epoch 16: Train Loss=0.7928, Acc=0.5433 | Val Loss=0.6868, Acc=0.4357\n",
      "Epoch 17: Train Loss=0.6756, Acc=0.5375 | Val Loss=0.6832, Acc=0.5317\n",
      "Epoch 18: Train Loss=0.7157, Acc=0.5168 | Val Loss=0.6853, Acc=0.6392\n",
      "Epoch 19: Train Loss=0.6766, Acc=0.5572 | Val Loss=0.6787, Acc=0.5643\n",
      "Epoch 20: Train Loss=0.6816, Acc=0.5567 | Val Loss=0.6787, Acc=0.5931\n",
      "Epoch 21: Train Loss=0.6859, Acc=0.5702 | Val Loss=0.6728, Acc=0.6315\n",
      "Epoch 22: Train Loss=0.6789, Acc=0.5370 | Val Loss=0.6719, Acc=0.6353\n",
      "Epoch 23: Train Loss=0.6859, Acc=0.5692 | Val Loss=0.6742, Acc=0.5816\n",
      "Epoch 24: Train Loss=0.6849, Acc=0.6077 | Val Loss=0.6760, Acc=0.5777\n",
      "Epoch 25: Train Loss=0.6876, Acc=0.5591 | Val Loss=0.6751, Acc=0.5758\n",
      "Epoch 26: Train Loss=0.6709, Acc=0.5510 | Val Loss=0.6766, Acc=0.5681\n",
      "Epoch 27: Train Loss=0.6812, Acc=0.5611 | Val Loss=0.6756, Acc=0.5720\n",
      "Epoch 28: Train Loss=0.6728, Acc=0.6005 | Val Loss=0.6765, Acc=0.5720\n",
      "Epoch 29: Train Loss=0.6750, Acc=0.5813 | Val Loss=0.6767, Acc=0.5643\n",
      "Epoch 30: Train Loss=0.6736, Acc=0.5870 | Val Loss=0.6796, Acc=0.5029\n",
      "Epoch 31: Train Loss=0.6809, Acc=0.5409 | Val Loss=0.6772, Acc=0.5585\n",
      "Epoch 32: Train Loss=0.6700, Acc=0.5817 | Val Loss=0.6763, Acc=0.5739\n",
      "Epoch 33: Train Loss=0.6879, Acc=0.5808 | Val Loss=0.6763, Acc=0.5797\n",
      "Epoch 34: Train Loss=0.6848, Acc=0.5784 | Val Loss=0.6757, Acc=0.5969\n",
      "Epoch 35: Train Loss=0.6799, Acc=0.5986 | Val Loss=0.6758, Acc=0.5835\n",
      "Epoch 36: Train Loss=0.6763, Acc=0.5760 | Val Loss=0.6769, Acc=0.5701\n",
      "Epoch 37: Train Loss=0.6818, Acc=0.5813 | Val Loss=0.6759, Acc=0.5816\n",
      "Early stopping triggered after 37 epochs\n",
      "\n",
      "Fold 1 Test Metrics:\n",
      "  Accuracy:  0.5730\n",
      "  Precision: 0.7121\n",
      "  Recall:    0.5730\n",
      "  F1 Score:  0.6127\n",
      "\n",
      "Fold 1 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8319    0.5767    0.6812       515\n",
      "           1     0.2585    0.5588    0.3535       136\n",
      "\n",
      "    accuracy                         0.5730       651\n",
      "   macro avg     0.5452    0.5678    0.5173       651\n",
      "weighted avg     0.7121    0.5730    0.6127       651\n",
      "\n",
      "\n",
      "===== MultiGNN - Fold 2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=2.6970, Acc=0.3447 | Val Loss=0.6869, Acc=0.2173\n",
      "Epoch 02: Train Loss=0.7103, Acc=0.3231 | Val Loss=0.6844, Acc=0.6058\n",
      "Epoch 03: Train Loss=0.6954, Acc=0.4933 | Val Loss=0.6848, Acc=0.7135\n",
      "Epoch 04: Train Loss=0.6899, Acc=0.6394 | Val Loss=0.6801, Acc=0.7192\n",
      "Epoch 05: Train Loss=0.6837, Acc=0.6865 | Val Loss=0.6749, Acc=0.7788\n",
      "Epoch 06: Train Loss=0.6915, Acc=0.6543 | Val Loss=0.6829, Acc=0.7615\n",
      "Epoch 07: Train Loss=0.6801, Acc=0.7010 | Val Loss=0.6785, Acc=0.7750\n",
      "Epoch 08: Train Loss=0.6758, Acc=0.6736 | Val Loss=0.6765, Acc=0.7385\n",
      "Epoch 09: Train Loss=0.6814, Acc=0.7221 | Val Loss=0.6755, Acc=0.7404\n",
      "Epoch 10: Train Loss=0.6735, Acc=0.7168 | Val Loss=0.6741, Acc=0.7231\n",
      "Epoch 11: Train Loss=0.6713, Acc=0.7245 | Val Loss=0.6725, Acc=0.7346\n",
      "Epoch 12: Train Loss=0.6589, Acc=0.7322 | Val Loss=0.6665, Acc=0.7096\n",
      "Epoch 13: Train Loss=0.6736, Acc=0.7269 | Val Loss=0.6754, Acc=0.7519\n",
      "Epoch 14: Train Loss=0.6731, Acc=0.7529 | Val Loss=0.6729, Acc=0.7231\n",
      "Epoch 15: Train Loss=0.6729, Acc=0.7260 | Val Loss=0.6667, Acc=0.7231\n",
      "Epoch 16: Train Loss=0.6741, Acc=0.7337 | Val Loss=0.6730, Acc=0.7288\n",
      "Epoch 17: Train Loss=0.6687, Acc=0.7361 | Val Loss=0.6699, Acc=0.7019\n",
      "Epoch 18: Train Loss=0.6704, Acc=0.7063 | Val Loss=0.6714, Acc=0.7154\n",
      "Epoch 19: Train Loss=0.6635, Acc=0.7274 | Val Loss=0.6754, Acc=0.7212\n",
      "Epoch 20: Train Loss=0.6709, Acc=0.7135 | Val Loss=0.6744, Acc=0.7192\n",
      "Epoch 21: Train Loss=0.6615, Acc=0.7236 | Val Loss=0.6750, Acc=0.7115\n",
      "Epoch 22: Train Loss=0.6754, Acc=0.7250 | Val Loss=0.6775, Acc=0.7327\n",
      "Epoch 23: Train Loss=0.6631, Acc=0.7279 | Val Loss=0.6717, Acc=0.7135\n",
      "Epoch 24: Train Loss=0.6690, Acc=0.7255 | Val Loss=0.6720, Acc=0.7231\n",
      "Epoch 25: Train Loss=0.6715, Acc=0.7197 | Val Loss=0.6727, Acc=0.7346\n",
      "Epoch 26: Train Loss=0.6688, Acc=0.7216 | Val Loss=0.6724, Acc=0.7404\n",
      "Epoch 27: Train Loss=0.6719, Acc=0.7231 | Val Loss=0.6721, Acc=0.7288\n",
      "Early stopping triggered after 27 epochs\n",
      "\n",
      "Fold 2 Test Metrics:\n",
      "  Accuracy:  0.6882\n",
      "  Precision: 0.6923\n",
      "  Recall:    0.6882\n",
      "  F1 Score:  0.6902\n",
      "\n",
      "Fold 2 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8059    0.7981    0.8020       515\n",
      "           1     0.2624    0.2721    0.2671       136\n",
      "\n",
      "    accuracy                         0.6882       651\n",
      "   macro avg     0.5341    0.5351    0.5345       651\n",
      "weighted avg     0.6923    0.6882    0.6902       651\n",
      "\n",
      "\n",
      "===== MultiGNN - Fold 3 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=1.3693, Acc=0.5428 | Val Loss=0.6830, Acc=0.6846\n",
      "Epoch 02: Train Loss=0.7122, Acc=0.6529 | Val Loss=0.6666, Acc=0.6154\n",
      "Epoch 03: Train Loss=0.7109, Acc=0.6769 | Val Loss=0.6678, Acc=0.7462\n",
      "Epoch 04: Train Loss=0.7014, Acc=0.6875 | Val Loss=0.6914, Acc=0.7846\n",
      "Epoch 05: Train Loss=0.6913, Acc=0.7053 | Val Loss=0.6657, Acc=0.6788\n",
      "Epoch 06: Train Loss=0.6917, Acc=0.6913 | Val Loss=0.6663, Acc=0.6154\n",
      "Epoch 07: Train Loss=0.6883, Acc=0.6817 | Val Loss=0.6711, Acc=0.7192\n",
      "Epoch 08: Train Loss=0.6945, Acc=0.7370 | Val Loss=0.6649, Acc=0.6865\n",
      "Epoch 09: Train Loss=0.6981, Acc=0.7260 | Val Loss=0.6622, Acc=0.7462\n",
      "Epoch 10: Train Loss=0.7074, Acc=0.6760 | Val Loss=0.6668, Acc=0.6538\n",
      "Epoch 11: Train Loss=0.6822, Acc=0.6721 | Val Loss=0.6699, Acc=0.7731\n",
      "Epoch 12: Train Loss=0.6949, Acc=0.7087 | Val Loss=0.6631, Acc=0.7423\n",
      "Epoch 13: Train Loss=0.6786, Acc=0.7053 | Val Loss=0.6590, Acc=0.6404\n",
      "Epoch 14: Train Loss=0.6858, Acc=0.6721 | Val Loss=0.6616, Acc=0.7365\n",
      "Epoch 15: Train Loss=0.6744, Acc=0.7096 | Val Loss=0.6546, Acc=0.7135\n",
      "Epoch 16: Train Loss=0.6757, Acc=0.6899 | Val Loss=0.6610, Acc=0.6942\n",
      "Epoch 17: Train Loss=0.6734, Acc=0.6885 | Val Loss=0.6605, Acc=0.6904\n",
      "Epoch 18: Train Loss=0.6684, Acc=0.6937 | Val Loss=0.6580, Acc=0.7212\n",
      "Epoch 19: Train Loss=0.6718, Acc=0.7063 | Val Loss=0.6555, Acc=0.7288\n",
      "Epoch 20: Train Loss=0.6715, Acc=0.7014 | Val Loss=0.6618, Acc=0.7173\n",
      "Epoch 21: Train Loss=0.6769, Acc=0.7106 | Val Loss=0.6630, Acc=0.7346\n",
      "Epoch 22: Train Loss=0.6685, Acc=0.7000 | Val Loss=0.6598, Acc=0.6942\n",
      "Epoch 23: Train Loss=0.6667, Acc=0.6928 | Val Loss=0.6586, Acc=0.7038\n",
      "Epoch 24: Train Loss=0.6739, Acc=0.6995 | Val Loss=0.6591, Acc=0.7250\n",
      "Epoch 25: Train Loss=0.6679, Acc=0.7034 | Val Loss=0.6609, Acc=0.7077\n",
      "Epoch 26: Train Loss=0.6674, Acc=0.7034 | Val Loss=0.6597, Acc=0.7058\n",
      "Epoch 27: Train Loss=0.6727, Acc=0.7034 | Val Loss=0.6603, Acc=0.7038\n",
      "Epoch 28: Train Loss=0.6613, Acc=0.7101 | Val Loss=0.6579, Acc=0.7231\n",
      "Epoch 29: Train Loss=0.6775, Acc=0.7058 | Val Loss=0.6582, Acc=0.7212\n",
      "Epoch 30: Train Loss=0.6708, Acc=0.6803 | Val Loss=0.6584, Acc=0.7096\n",
      "Early stopping triggered after 30 epochs\n",
      "\n",
      "Fold 3 Test Metrics:\n",
      "  Accuracy:  0.6436\n",
      "  Precision: 0.7013\n",
      "  Recall:    0.6436\n",
      "  F1 Score:  0.6667\n",
      "\n",
      "Fold 3 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8166    0.7087    0.7588       515\n",
      "           1     0.2647    0.3971    0.3176       136\n",
      "\n",
      "    accuracy                         0.6436       651\n",
      "   macro avg     0.5406    0.5529    0.5382       651\n",
      "weighted avg     0.7013    0.6436    0.6667       651\n",
      "\n",
      "\n",
      "===== MultiGNN - Fold 4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=1.1288, Acc=0.6053 | Val Loss=0.6938, Acc=0.7827\n",
      "Epoch 02: Train Loss=0.7024, Acc=0.7322 | Val Loss=0.6929, Acc=0.7808\n",
      "Epoch 03: Train Loss=0.7215, Acc=0.7298 | Val Loss=0.6930, Acc=0.7827\n",
      "Epoch 04: Train Loss=0.6959, Acc=0.7216 | Val Loss=0.6926, Acc=0.7827\n",
      "Epoch 05: Train Loss=0.6908, Acc=0.7048 | Val Loss=0.6907, Acc=0.7827\n",
      "Epoch 06: Train Loss=0.7093, Acc=0.7442 | Val Loss=0.6929, Acc=0.7808\n",
      "Epoch 07: Train Loss=0.6910, Acc=0.7793 | Val Loss=0.6927, Acc=0.7827\n",
      "Epoch 08: Train Loss=0.7166, Acc=0.7538 | Val Loss=0.6928, Acc=0.7827\n",
      "Epoch 09: Train Loss=0.7131, Acc=0.7683 | Val Loss=0.6907, Acc=0.7846\n",
      "Epoch 10: Train Loss=0.7111, Acc=0.7615 | Val Loss=0.6834, Acc=0.7673\n",
      "Epoch 11: Train Loss=0.7008, Acc=0.7106 | Val Loss=0.6859, Acc=0.7615\n",
      "Epoch 12: Train Loss=0.6974, Acc=0.7288 | Val Loss=0.6786, Acc=0.7538\n",
      "Epoch 13: Train Loss=0.6789, Acc=0.7476 | Val Loss=0.6826, Acc=0.6942\n",
      "Epoch 14: Train Loss=0.6898, Acc=0.7279 | Val Loss=0.6852, Acc=0.7462\n",
      "Epoch 15: Train Loss=0.6841, Acc=0.7337 | Val Loss=0.6894, Acc=0.7692\n",
      "Epoch 16: Train Loss=0.6808, Acc=0.7615 | Val Loss=0.6835, Acc=0.7365\n",
      "Epoch 17: Train Loss=0.6827, Acc=0.7370 | Val Loss=0.6772, Acc=0.7308\n",
      "Epoch 18: Train Loss=0.6867, Acc=0.7288 | Val Loss=0.6800, Acc=0.7173\n",
      "Epoch 19: Train Loss=0.6738, Acc=0.7380 | Val Loss=0.6808, Acc=0.7500\n",
      "Epoch 20: Train Loss=0.6827, Acc=0.7500 | Val Loss=0.6818, Acc=0.7538\n",
      "Epoch 21: Train Loss=0.6816, Acc=0.7567 | Val Loss=0.6800, Acc=0.7308\n",
      "Epoch 22: Train Loss=0.6649, Acc=0.7688 | Val Loss=0.6778, Acc=0.7250\n",
      "Epoch 23: Train Loss=0.6695, Acc=0.7346 | Val Loss=0.6796, Acc=0.7288\n",
      "Epoch 24: Train Loss=0.6706, Acc=0.7495 | Val Loss=0.6812, Acc=0.7365\n",
      "Epoch 25: Train Loss=0.6785, Acc=0.7510 | Val Loss=0.6812, Acc=0.7481\n",
      "Epoch 26: Train Loss=0.6757, Acc=0.7413 | Val Loss=0.6803, Acc=0.7346\n",
      "Epoch 27: Train Loss=0.6804, Acc=0.7423 | Val Loss=0.6825, Acc=0.7442\n",
      "Epoch 28: Train Loss=0.6751, Acc=0.7462 | Val Loss=0.6827, Acc=0.7462\n",
      "Epoch 29: Train Loss=0.6737, Acc=0.7476 | Val Loss=0.6824, Acc=0.7442\n",
      "Epoch 30: Train Loss=0.6660, Acc=0.7413 | Val Loss=0.6812, Acc=0.7327\n",
      "Epoch 31: Train Loss=0.6677, Acc=0.7404 | Val Loss=0.6797, Acc=0.7135\n",
      "Epoch 32: Train Loss=0.6721, Acc=0.7322 | Val Loss=0.6793, Acc=0.7173\n",
      "Early stopping triggered after 32 epochs\n",
      "\n",
      "Fold 4 Test Metrics:\n",
      "  Accuracy:  0.7066\n",
      "  Precision: 0.7010\n",
      "  Recall:    0.7066\n",
      "  F1 Score:  0.7037\n",
      "\n",
      "Fold 4 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8103    0.8214    0.8158       515\n",
      "           1     0.2868    0.2721    0.2792       136\n",
      "\n",
      "    accuracy                         0.7066       651\n",
      "   macro avg     0.5486    0.5467    0.5475       651\n",
      "weighted avg     0.7010    0.7066    0.7037       651\n",
      "\n",
      "\n",
      "===== MultiGNN - Fold 5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=2.2236, Acc=0.4726 | Val Loss=0.6985, Acc=0.6019\n",
      "Epoch 02: Train Loss=0.7164, Acc=0.3490 | Val Loss=0.6869, Acc=0.5096\n",
      "Epoch 03: Train Loss=0.7142, Acc=0.3534 | Val Loss=0.7290, Acc=0.7558\n",
      "Epoch 04: Train Loss=0.7210, Acc=0.3274 | Val Loss=0.6931, Acc=0.2942\n",
      "Epoch 05: Train Loss=0.7046, Acc=0.4005 | Val Loss=0.6856, Acc=0.5788\n",
      "Epoch 06: Train Loss=0.7104, Acc=0.3942 | Val Loss=0.6937, Acc=0.6846\n",
      "Epoch 07: Train Loss=0.7088, Acc=0.4582 | Val Loss=0.6885, Acc=0.5038\n",
      "Epoch 08: Train Loss=0.7093, Acc=0.5207 | Val Loss=0.6786, Acc=0.6462\n",
      "Epoch 09: Train Loss=0.7078, Acc=0.5317 | Val Loss=0.6818, Acc=0.6135\n",
      "Epoch 10: Train Loss=0.6956, Acc=0.4745 | Val Loss=0.6829, Acc=0.5904\n",
      "Epoch 11: Train Loss=0.6813, Acc=0.5514 | Val Loss=0.6778, Acc=0.6462\n",
      "Epoch 12: Train Loss=0.7153, Acc=0.5010 | Val Loss=0.6915, Acc=0.3269\n",
      "Epoch 13: Train Loss=0.6925, Acc=0.4885 | Val Loss=0.6806, Acc=0.6308\n",
      "Epoch 14: Train Loss=0.6783, Acc=0.5380 | Val Loss=0.6798, Acc=0.6442\n",
      "Epoch 15: Train Loss=0.6873, Acc=0.5034 | Val Loss=0.6867, Acc=0.4788\n",
      "Epoch 16: Train Loss=0.6766, Acc=0.4875 | Val Loss=0.6842, Acc=0.6173\n",
      "Epoch 17: Train Loss=0.6890, Acc=0.5125 | Val Loss=0.6822, Acc=0.6231\n",
      "Epoch 18: Train Loss=0.6941, Acc=0.5279 | Val Loss=0.6800, Acc=0.6269\n",
      "Epoch 19: Train Loss=0.6733, Acc=0.5490 | Val Loss=0.6785, Acc=0.6385\n",
      "Epoch 20: Train Loss=0.6807, Acc=0.5803 | Val Loss=0.6771, Acc=0.6231\n",
      "Epoch 21: Train Loss=0.6804, Acc=0.5668 | Val Loss=0.6792, Acc=0.6096\n",
      "Epoch 22: Train Loss=0.6813, Acc=0.5303 | Val Loss=0.6779, Acc=0.6173\n",
      "Epoch 23: Train Loss=0.6726, Acc=0.5769 | Val Loss=0.6775, Acc=0.6231\n",
      "Epoch 24: Train Loss=0.6726, Acc=0.5712 | Val Loss=0.6786, Acc=0.6212\n",
      "Epoch 25: Train Loss=0.6826, Acc=0.5736 | Val Loss=0.6784, Acc=0.6250\n",
      "Epoch 26: Train Loss=0.6807, Acc=0.5808 | Val Loss=0.6787, Acc=0.6288\n",
      "Epoch 27: Train Loss=0.6873, Acc=0.5687 | Val Loss=0.6788, Acc=0.5942\n",
      "Epoch 28: Train Loss=0.6818, Acc=0.5365 | Val Loss=0.6792, Acc=0.5923\n",
      "Epoch 29: Train Loss=0.6864, Acc=0.5264 | Val Loss=0.6800, Acc=0.5788\n",
      "Epoch 30: Train Loss=0.6731, Acc=0.5274 | Val Loss=0.6794, Acc=0.5846\n",
      "Epoch 31: Train Loss=0.6705, Acc=0.5548 | Val Loss=0.6786, Acc=0.5981\n",
      "Epoch 32: Train Loss=0.6734, Acc=0.5476 | Val Loss=0.6788, Acc=0.5981\n",
      "Epoch 33: Train Loss=0.6790, Acc=0.5572 | Val Loss=0.6786, Acc=0.6019\n",
      "Epoch 34: Train Loss=0.6779, Acc=0.5553 | Val Loss=0.6785, Acc=0.6019\n",
      "Epoch 35: Train Loss=0.6721, Acc=0.5519 | Val Loss=0.6784, Acc=0.6096\n",
      "Early stopping triggered after 35 epochs\n",
      "\n",
      "Fold 5 Test Metrics:\n",
      "  Accuracy:  0.5899\n",
      "  Precision: 0.7200\n",
      "  Recall:    0.5899\n",
      "  F1 Score:  0.6278\n",
      "\n",
      "Fold 5 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8388    0.5961    0.6969       515\n",
      "           1     0.2702    0.5662    0.3658       136\n",
      "\n",
      "    accuracy                         0.5899       651\n",
      "   macro avg     0.5545    0.5811    0.5314       651\n",
      "weighted avg     0.7200    0.5899    0.6278       651\n",
      "\n",
      "\n",
      "===== Final Combined Results (Majority Vote) for MultiGNN =====\n",
      "Final Accuracy:  0.6436\n",
      "Final Precision: 0.7029\n",
      "Final Recall:    0.6436\n",
      "Final F1 Score:  0.6671\n",
      "\n",
      "Final Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8180    0.7068    0.7583       515\n",
      "           1     0.2670    0.4044    0.3216       136\n",
      "\n",
      "    accuracy                         0.6436       651\n",
      "   macro avg     0.5425    0.5556    0.5400       651\n",
      "weighted avg     0.7029    0.6436    0.6671       651\n",
      "\n",
      "\n",
      "MultiGNN - Final Majority Vote Results:\n",
      "  Accuracy:  0.6436\n",
      "  Precision: 0.7029\n",
      "  Recall:    0.6436\n",
      "  F1 Score:  0.6671\n",
      "\n",
      "=== Testing ResidualAttentionGNN Model ===\n",
      "\n",
      "===== ResidualAttentionGNN - Fold 1 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=0.6979, Acc=0.7837 | Val Loss=0.6921, Acc=0.7831\n",
      "Epoch 02: Train Loss=0.6847, Acc=0.7490 | Val Loss=0.6697, Acc=0.7006\n",
      "Epoch 03: Train Loss=0.6716, Acc=0.7125 | Val Loss=0.6675, Acc=0.7390\n",
      "Epoch 04: Train Loss=0.6747, Acc=0.7298 | Val Loss=0.6658, Acc=0.7102\n",
      "Epoch 05: Train Loss=0.6723, Acc=0.7192 | Val Loss=0.6684, Acc=0.7063\n",
      "Epoch 06: Train Loss=0.6697, Acc=0.7014 | Val Loss=0.6632, Acc=0.7083\n",
      "Epoch 07: Train Loss=0.6730, Acc=0.7327 | Val Loss=0.6703, Acc=0.6910\n",
      "Epoch 08: Train Loss=0.6647, Acc=0.7139 | Val Loss=0.6693, Acc=0.6929\n",
      "Epoch 09: Train Loss=0.6689, Acc=0.7192 | Val Loss=0.6696, Acc=0.6699\n",
      "Epoch 10: Train Loss=0.6655, Acc=0.6913 | Val Loss=0.6690, Acc=0.6660\n",
      "Epoch 11: Train Loss=0.6618, Acc=0.7072 | Val Loss=0.6665, Acc=0.6718\n",
      "Epoch 12: Train Loss=0.6636, Acc=0.7087 | Val Loss=0.6657, Acc=0.6814\n",
      "Epoch 13: Train Loss=0.6636, Acc=0.7058 | Val Loss=0.6670, Acc=0.6910\n",
      "Epoch 14: Train Loss=0.6664, Acc=0.7135 | Val Loss=0.6656, Acc=0.6775\n",
      "Epoch 15: Train Loss=0.6625, Acc=0.7048 | Val Loss=0.6693, Acc=0.6699\n",
      "Epoch 16: Train Loss=0.6596, Acc=0.7005 | Val Loss=0.6669, Acc=0.6679\n",
      "Epoch 17: Train Loss=0.6645, Acc=0.7077 | Val Loss=0.6680, Acc=0.6699\n",
      "Epoch 18: Train Loss=0.6622, Acc=0.7096 | Val Loss=0.6678, Acc=0.6756\n",
      "Epoch 19: Train Loss=0.6639, Acc=0.7072 | Val Loss=0.6682, Acc=0.6699\n",
      "Epoch 20: Train Loss=0.6643, Acc=0.7019 | Val Loss=0.6679, Acc=0.6660\n",
      "Epoch 21: Train Loss=0.6637, Acc=0.6942 | Val Loss=0.6676, Acc=0.6622\n",
      "Early stopping triggered after 21 epochs\n",
      "\n",
      "Fold 1 Test Metrics:\n",
      "  Accuracy:  0.6498\n",
      "  Precision: 0.7133\n",
      "  Recall:    0.6498\n",
      "  F1 Score:  0.6741\n",
      "\n",
      "Fold 1 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8269    0.7049    0.7610       515\n",
      "           1     0.2830    0.4412    0.3448       136\n",
      "\n",
      "    accuracy                         0.6498       651\n",
      "   macro avg     0.5549    0.5730    0.5529       651\n",
      "weighted avg     0.7133    0.6498    0.6741       651\n",
      "\n",
      "\n",
      "===== ResidualAttentionGNN - Fold 2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=0.6980, Acc=0.7755 | Val Loss=0.6828, Acc=0.7827\n",
      "Epoch 02: Train Loss=0.6837, Acc=0.7087 | Val Loss=0.6649, Acc=0.6692\n",
      "Epoch 03: Train Loss=0.6788, Acc=0.6986 | Val Loss=0.6602, Acc=0.6577\n",
      "Epoch 04: Train Loss=0.6717, Acc=0.6966 | Val Loss=0.6622, Acc=0.6635\n",
      "Epoch 05: Train Loss=0.6710, Acc=0.6760 | Val Loss=0.6620, Acc=0.6673\n",
      "Epoch 06: Train Loss=0.6665, Acc=0.7087 | Val Loss=0.6613, Acc=0.6635\n",
      "Epoch 07: Train Loss=0.6669, Acc=0.6779 | Val Loss=0.6631, Acc=0.6558\n",
      "Epoch 08: Train Loss=0.6645, Acc=0.6774 | Val Loss=0.6617, Acc=0.6673\n",
      "Epoch 09: Train Loss=0.6659, Acc=0.6986 | Val Loss=0.6592, Acc=0.6885\n",
      "Epoch 10: Train Loss=0.6612, Acc=0.7043 | Val Loss=0.6568, Acc=0.6808\n",
      "Epoch 11: Train Loss=0.6624, Acc=0.7111 | Val Loss=0.6555, Acc=0.6750\n",
      "Epoch 12: Train Loss=0.6681, Acc=0.6937 | Val Loss=0.6561, Acc=0.6769\n",
      "Epoch 13: Train Loss=0.6610, Acc=0.6966 | Val Loss=0.6572, Acc=0.6712\n",
      "Epoch 14: Train Loss=0.6644, Acc=0.6885 | Val Loss=0.6566, Acc=0.6692\n",
      "Epoch 15: Train Loss=0.6556, Acc=0.7038 | Val Loss=0.6565, Acc=0.6731\n",
      "Epoch 16: Train Loss=0.6605, Acc=0.6933 | Val Loss=0.6559, Acc=0.6692\n",
      "Epoch 17: Train Loss=0.6628, Acc=0.6947 | Val Loss=0.6558, Acc=0.6635\n",
      "Epoch 18: Train Loss=0.6630, Acc=0.6957 | Val Loss=0.6564, Acc=0.6673\n",
      "Epoch 19: Train Loss=0.6597, Acc=0.6966 | Val Loss=0.6566, Acc=0.6596\n",
      "Epoch 20: Train Loss=0.6659, Acc=0.6856 | Val Loss=0.6559, Acc=0.6615\n",
      "Epoch 21: Train Loss=0.6613, Acc=0.6889 | Val Loss=0.6565, Acc=0.6596\n",
      "Epoch 22: Train Loss=0.6658, Acc=0.6894 | Val Loss=0.6568, Acc=0.6538\n",
      "Epoch 23: Train Loss=0.6657, Acc=0.6870 | Val Loss=0.6566, Acc=0.6577\n",
      "Epoch 24: Train Loss=0.6629, Acc=0.6885 | Val Loss=0.6560, Acc=0.6596\n",
      "Epoch 25: Train Loss=0.6598, Acc=0.6851 | Val Loss=0.6559, Acc=0.6596\n",
      "Epoch 26: Train Loss=0.6673, Acc=0.6779 | Val Loss=0.6562, Acc=0.6596\n",
      "Early stopping triggered after 26 epochs\n",
      "\n",
      "Fold 2 Test Metrics:\n",
      "  Accuracy:  0.6344\n",
      "  Precision: 0.7189\n",
      "  Recall:    0.6344\n",
      "  F1 Score:  0.6639\n",
      "\n",
      "Fold 2 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8337    0.6718    0.7441       515\n",
      "           1     0.2839    0.4926    0.3602       136\n",
      "\n",
      "    accuracy                         0.6344       651\n",
      "   macro avg     0.5588    0.5822    0.5522       651\n",
      "weighted avg     0.7189    0.6344    0.6639       651\n",
      "\n",
      "\n",
      "===== ResidualAttentionGNN - Fold 3 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=0.6964, Acc=0.2702 | Val Loss=0.6918, Acc=0.2192\n",
      "Epoch 02: Train Loss=0.6917, Acc=0.4389 | Val Loss=0.6848, Acc=0.6731\n",
      "Epoch 03: Train Loss=0.6807, Acc=0.6394 | Val Loss=0.6672, Acc=0.6788\n",
      "Epoch 04: Train Loss=0.6772, Acc=0.6404 | Val Loss=0.6502, Acc=0.7192\n",
      "Epoch 05: Train Loss=0.6775, Acc=0.6423 | Val Loss=0.6574, Acc=0.7019\n",
      "Epoch 06: Train Loss=0.6748, Acc=0.6764 | Val Loss=0.6562, Acc=0.6865\n",
      "Epoch 07: Train Loss=0.6725, Acc=0.6207 | Val Loss=0.6523, Acc=0.7000\n",
      "Epoch 08: Train Loss=0.6719, Acc=0.6755 | Val Loss=0.6524, Acc=0.7250\n",
      "Epoch 09: Train Loss=0.6650, Acc=0.6577 | Val Loss=0.6534, Acc=0.6942\n",
      "Epoch 10: Train Loss=0.6701, Acc=0.6481 | Val Loss=0.6526, Acc=0.7154\n",
      "Epoch 11: Train Loss=0.6619, Acc=0.6673 | Val Loss=0.6476, Acc=0.6904\n",
      "Epoch 12: Train Loss=0.6661, Acc=0.6514 | Val Loss=0.6506, Acc=0.6731\n",
      "Epoch 13: Train Loss=0.6636, Acc=0.6413 | Val Loss=0.6520, Acc=0.7462\n",
      "Epoch 14: Train Loss=0.6661, Acc=0.6654 | Val Loss=0.6506, Acc=0.6808\n",
      "Epoch 15: Train Loss=0.6644, Acc=0.6529 | Val Loss=0.6491, Acc=0.7058\n",
      "Epoch 16: Train Loss=0.6646, Acc=0.6832 | Val Loss=0.6481, Acc=0.7058\n",
      "Epoch 17: Train Loss=0.6688, Acc=0.6548 | Val Loss=0.6518, Acc=0.7077\n",
      "Epoch 18: Train Loss=0.6680, Acc=0.6740 | Val Loss=0.6498, Acc=0.6846\n",
      "Epoch 19: Train Loss=0.6664, Acc=0.6620 | Val Loss=0.6499, Acc=0.6942\n",
      "Epoch 20: Train Loss=0.6651, Acc=0.6495 | Val Loss=0.6486, Acc=0.7019\n",
      "Epoch 21: Train Loss=0.6622, Acc=0.6639 | Val Loss=0.6487, Acc=0.7077\n",
      "Epoch 22: Train Loss=0.6689, Acc=0.6793 | Val Loss=0.6482, Acc=0.7115\n",
      "Epoch 23: Train Loss=0.6634, Acc=0.6644 | Val Loss=0.6468, Acc=0.6923\n",
      "Epoch 24: Train Loss=0.6608, Acc=0.6615 | Val Loss=0.6464, Acc=0.6885\n",
      "Epoch 25: Train Loss=0.6629, Acc=0.6601 | Val Loss=0.6474, Acc=0.6865\n",
      "Epoch 26: Train Loss=0.6566, Acc=0.6630 | Val Loss=0.6471, Acc=0.6846\n",
      "Epoch 27: Train Loss=0.6641, Acc=0.6505 | Val Loss=0.6480, Acc=0.6846\n",
      "Epoch 28: Train Loss=0.6638, Acc=0.6644 | Val Loss=0.6493, Acc=0.6827\n",
      "Epoch 29: Train Loss=0.6691, Acc=0.6591 | Val Loss=0.6472, Acc=0.6750\n",
      "Epoch 30: Train Loss=0.6593, Acc=0.6553 | Val Loss=0.6486, Acc=0.6750\n",
      "Epoch 31: Train Loss=0.6640, Acc=0.6466 | Val Loss=0.6493, Acc=0.6712\n",
      "Epoch 32: Train Loss=0.6560, Acc=0.6490 | Val Loss=0.6485, Acc=0.6712\n",
      "Epoch 33: Train Loss=0.6656, Acc=0.6409 | Val Loss=0.6482, Acc=0.6712\n",
      "Epoch 34: Train Loss=0.6642, Acc=0.6423 | Val Loss=0.6485, Acc=0.6712\n",
      "Epoch 35: Train Loss=0.6603, Acc=0.6442 | Val Loss=0.6483, Acc=0.6788\n",
      "Epoch 36: Train Loss=0.6578, Acc=0.6538 | Val Loss=0.6476, Acc=0.6808\n",
      "Epoch 37: Train Loss=0.6635, Acc=0.6620 | Val Loss=0.6470, Acc=0.6808\n",
      "Epoch 38: Train Loss=0.6615, Acc=0.6596 | Val Loss=0.6471, Acc=0.6846\n",
      "Epoch 39: Train Loss=0.6541, Acc=0.6630 | Val Loss=0.6469, Acc=0.6827\n",
      "Early stopping triggered after 39 epochs\n",
      "\n",
      "Fold 3 Test Metrics:\n",
      "  Accuracy:  0.6283\n",
      "  Precision: 0.7133\n",
      "  Recall:    0.6283\n",
      "  F1 Score:  0.6582\n",
      "\n",
      "Fold 3 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8289    0.6680    0.7398       515\n",
      "           1     0.2754    0.4779    0.3495       136\n",
      "\n",
      "    accuracy                         0.6283       651\n",
      "   macro avg     0.5522    0.5730    0.5446       651\n",
      "weighted avg     0.7133    0.6283    0.6582       651\n",
      "\n",
      "\n",
      "===== ResidualAttentionGNN - Fold 4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=0.6914, Acc=0.5962 | Val Loss=0.6884, Acc=0.7212\n",
      "Epoch 02: Train Loss=0.6828, Acc=0.6365 | Val Loss=0.6816, Acc=0.6827\n",
      "Epoch 03: Train Loss=0.6718, Acc=0.6486 | Val Loss=0.6798, Acc=0.6962\n",
      "Epoch 04: Train Loss=0.6722, Acc=0.7077 | Val Loss=0.6787, Acc=0.6942\n",
      "Epoch 05: Train Loss=0.6714, Acc=0.6990 | Val Loss=0.6779, Acc=0.6962\n",
      "Epoch 06: Train Loss=0.6668, Acc=0.7091 | Val Loss=0.6743, Acc=0.6942\n",
      "Epoch 07: Train Loss=0.6688, Acc=0.7101 | Val Loss=0.6745, Acc=0.6846\n",
      "Epoch 08: Train Loss=0.6666, Acc=0.6865 | Val Loss=0.6761, Acc=0.7000\n",
      "Epoch 09: Train Loss=0.6600, Acc=0.7149 | Val Loss=0.6759, Acc=0.6942\n",
      "Epoch 10: Train Loss=0.6596, Acc=0.7024 | Val Loss=0.6761, Acc=0.6923\n",
      "Epoch 11: Train Loss=0.6646, Acc=0.6952 | Val Loss=0.6731, Acc=0.6904\n",
      "Epoch 12: Train Loss=0.6610, Acc=0.7005 | Val Loss=0.6752, Acc=0.6923\n",
      "Epoch 13: Train Loss=0.6625, Acc=0.6904 | Val Loss=0.6762, Acc=0.6712\n",
      "Epoch 14: Train Loss=0.6582, Acc=0.6913 | Val Loss=0.6754, Acc=0.6808\n",
      "Epoch 15: Train Loss=0.6655, Acc=0.7082 | Val Loss=0.6726, Acc=0.6827\n",
      "Epoch 16: Train Loss=0.6612, Acc=0.6957 | Val Loss=0.6733, Acc=0.6769\n",
      "Epoch 17: Train Loss=0.6631, Acc=0.7082 | Val Loss=0.6753, Acc=0.6923\n",
      "Epoch 18: Train Loss=0.6660, Acc=0.7019 | Val Loss=0.6746, Acc=0.6904\n",
      "Epoch 19: Train Loss=0.6666, Acc=0.6923 | Val Loss=0.6731, Acc=0.6808\n",
      "Epoch 20: Train Loss=0.6644, Acc=0.7010 | Val Loss=0.6733, Acc=0.6808\n",
      "Epoch 21: Train Loss=0.6626, Acc=0.6942 | Val Loss=0.6731, Acc=0.6904\n",
      "Epoch 22: Train Loss=0.6593, Acc=0.6885 | Val Loss=0.6737, Acc=0.6769\n",
      "Epoch 23: Train Loss=0.6676, Acc=0.6952 | Val Loss=0.6736, Acc=0.6788\n",
      "Epoch 24: Train Loss=0.6588, Acc=0.6947 | Val Loss=0.6735, Acc=0.6750\n",
      "Epoch 25: Train Loss=0.6625, Acc=0.6885 | Val Loss=0.6736, Acc=0.6750\n",
      "Epoch 26: Train Loss=0.6563, Acc=0.6913 | Val Loss=0.6738, Acc=0.6769\n",
      "Epoch 27: Train Loss=0.6637, Acc=0.6894 | Val Loss=0.6742, Acc=0.6865\n",
      "Epoch 28: Train Loss=0.6573, Acc=0.6962 | Val Loss=0.6745, Acc=0.6846\n",
      "Epoch 29: Train Loss=0.6639, Acc=0.6918 | Val Loss=0.6743, Acc=0.6865\n",
      "Epoch 30: Train Loss=0.6623, Acc=0.7029 | Val Loss=0.6742, Acc=0.6827\n",
      "Early stopping triggered after 30 epochs\n",
      "\n",
      "Fold 4 Test Metrics:\n",
      "  Accuracy:  0.6528\n",
      "  Precision: 0.7112\n",
      "  Recall:    0.6528\n",
      "  F1 Score:  0.6757\n",
      "\n",
      "Fold 4 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8247    0.7126    0.7646       515\n",
      "           1     0.2816    0.4265    0.3392       136\n",
      "\n",
      "    accuracy                         0.6528       651\n",
      "   macro avg     0.5531    0.5695    0.5519       651\n",
      "weighted avg     0.7112    0.6528    0.6757       651\n",
      "\n",
      "\n",
      "===== ResidualAttentionGNN - Fold 5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=0.6954, Acc=0.7817 | Val Loss=0.6864, Acc=0.7827\n",
      "Epoch 02: Train Loss=0.6844, Acc=0.7188 | Val Loss=0.6731, Acc=0.7077\n",
      "Epoch 03: Train Loss=0.6745, Acc=0.7115 | Val Loss=0.6677, Acc=0.7135\n",
      "Epoch 04: Train Loss=0.6683, Acc=0.7221 | Val Loss=0.6672, Acc=0.7115\n",
      "Epoch 05: Train Loss=0.6721, Acc=0.6990 | Val Loss=0.6629, Acc=0.7115\n",
      "Epoch 06: Train Loss=0.6751, Acc=0.7058 | Val Loss=0.6613, Acc=0.6923\n",
      "Epoch 07: Train Loss=0.6680, Acc=0.7130 | Val Loss=0.6643, Acc=0.7288\n",
      "Epoch 08: Train Loss=0.6722, Acc=0.7216 | Val Loss=0.6629, Acc=0.7269\n",
      "Epoch 09: Train Loss=0.6717, Acc=0.7115 | Val Loss=0.6619, Acc=0.7173\n",
      "Epoch 10: Train Loss=0.6678, Acc=0.7096 | Val Loss=0.6597, Acc=0.7115\n",
      "Epoch 11: Train Loss=0.6688, Acc=0.7135 | Val Loss=0.6610, Acc=0.7250\n",
      "Epoch 12: Train Loss=0.6671, Acc=0.7082 | Val Loss=0.6613, Acc=0.7077\n",
      "Epoch 13: Train Loss=0.6685, Acc=0.7067 | Val Loss=0.6615, Acc=0.7308\n",
      "Epoch 14: Train Loss=0.6667, Acc=0.7096 | Val Loss=0.6615, Acc=0.7269\n",
      "Epoch 15: Train Loss=0.6654, Acc=0.7111 | Val Loss=0.6609, Acc=0.7192\n",
      "Epoch 16: Train Loss=0.6667, Acc=0.7067 | Val Loss=0.6609, Acc=0.7308\n",
      "Epoch 17: Train Loss=0.6618, Acc=0.7101 | Val Loss=0.6600, Acc=0.7173\n",
      "Epoch 18: Train Loss=0.6703, Acc=0.6870 | Val Loss=0.6609, Acc=0.7038\n",
      "Epoch 19: Train Loss=0.6642, Acc=0.6986 | Val Loss=0.6610, Acc=0.7096\n",
      "Epoch 20: Train Loss=0.6689, Acc=0.6942 | Val Loss=0.6607, Acc=0.7115\n",
      "Epoch 21: Train Loss=0.6658, Acc=0.6995 | Val Loss=0.6608, Acc=0.7115\n",
      "Epoch 22: Train Loss=0.6646, Acc=0.7014 | Val Loss=0.6605, Acc=0.7135\n",
      "Epoch 23: Train Loss=0.6666, Acc=0.6937 | Val Loss=0.6607, Acc=0.7115\n",
      "Epoch 24: Train Loss=0.6644, Acc=0.7005 | Val Loss=0.6607, Acc=0.7154\n",
      "Epoch 25: Train Loss=0.6684, Acc=0.7010 | Val Loss=0.6611, Acc=0.7135\n",
      "Early stopping triggered after 25 epochs\n",
      "\n",
      "Fold 5 Test Metrics:\n",
      "  Accuracy:  0.6774\n",
      "  Precision: 0.6974\n",
      "  Recall:    0.6774\n",
      "  F1 Score:  0.6866\n",
      "\n",
      "Fold 5 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8106    0.7728    0.7913       515\n",
      "           1     0.2687    0.3162    0.2905       136\n",
      "\n",
      "    accuracy                         0.6774       651\n",
      "   macro avg     0.5397    0.5445    0.5409       651\n",
      "weighted avg     0.6974    0.6774    0.6866       651\n",
      "\n",
      "\n",
      "===== Final Combined Results (Majority Vote) for ResidualAttentionGNN =====\n",
      "Final Accuracy:  0.6436\n",
      "Final Precision: 0.7109\n",
      "Final Recall:    0.6436\n",
      "Final F1 Score:  0.6691\n",
      "\n",
      "Final Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8253    0.6971    0.7558       515\n",
      "           1     0.2778    0.4412    0.3409       136\n",
      "\n",
      "    accuracy                         0.6436       651\n",
      "   macro avg     0.5515    0.5691    0.5483       651\n",
      "weighted avg     0.7109    0.6436    0.6691       651\n",
      "\n",
      "\n",
      "ResidualAttentionGNN - Final Majority Vote Results:\n",
      "  Accuracy:  0.6436\n",
      "  Precision: 0.7109\n",
      "  Recall:    0.6436\n",
      "  F1 Score:  0.6691\n",
      "\n",
      "=== Testing DeepGNN Model ===\n",
      "\n",
      "===== DeepGNN - Fold 1 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=0.6916, Acc=0.4034 | Val Loss=0.6809, Acc=0.6967\n",
      "Epoch 02: Train Loss=0.6866, Acc=0.5178 | Val Loss=0.6728, Acc=0.6622\n",
      "Epoch 03: Train Loss=0.6682, Acc=0.6332 | Val Loss=0.6742, Acc=0.6967\n",
      "Epoch 04: Train Loss=0.6736, Acc=0.6010 | Val Loss=0.6783, Acc=0.4626\n",
      "Epoch 05: Train Loss=0.6775, Acc=0.5971 | Val Loss=0.6658, Acc=0.6449\n",
      "Epoch 06: Train Loss=0.6659, Acc=0.6389 | Val Loss=0.6646, Acc=0.6967\n",
      "Epoch 07: Train Loss=0.6654, Acc=0.6625 | Val Loss=0.6624, Acc=0.6449\n",
      "Epoch 08: Train Loss=0.6628, Acc=0.6654 | Val Loss=0.6628, Acc=0.6622\n",
      "Epoch 09: Train Loss=0.6601, Acc=0.6817 | Val Loss=0.6631, Acc=0.6334\n",
      "Epoch 10: Train Loss=0.6554, Acc=0.6803 | Val Loss=0.6646, Acc=0.6008\n",
      "Epoch 11: Train Loss=0.6616, Acc=0.6774 | Val Loss=0.6684, Acc=0.7140\n",
      "Epoch 12: Train Loss=0.6491, Acc=0.7043 | Val Loss=0.6691, Acc=0.6814\n",
      "Epoch 13: Train Loss=0.6378, Acc=0.7245 | Val Loss=0.6739, Acc=0.6296\n",
      "Epoch 14: Train Loss=0.6402, Acc=0.6981 | Val Loss=0.6719, Acc=0.6699\n",
      "Epoch 15: Train Loss=0.6323, Acc=0.7442 | Val Loss=0.6897, Acc=0.5835\n",
      "Epoch 16: Train Loss=0.6330, Acc=0.7538 | Val Loss=0.7008, Acc=0.5739\n",
      "Epoch 17: Train Loss=0.6217, Acc=0.7202 | Val Loss=0.6891, Acc=0.6795\n",
      "Epoch 18: Train Loss=0.6112, Acc=0.7279 | Val Loss=0.7038, Acc=0.7006\n",
      "Epoch 19: Train Loss=0.6003, Acc=0.7567 | Val Loss=0.7056, Acc=0.6910\n",
      "Epoch 20: Train Loss=0.5964, Acc=0.7721 | Val Loss=0.7208, Acc=0.6660\n",
      "Epoch 21: Train Loss=0.5855, Acc=0.7625 | Val Loss=0.7353, Acc=0.6583\n",
      "Epoch 22: Train Loss=0.5767, Acc=0.7697 | Val Loss=0.7363, Acc=0.6967\n",
      "Early stopping triggered after 22 epochs\n",
      "\n",
      "Fold 1 Test Metrics:\n",
      "  Accuracy:  0.6636\n",
      "  Precision: 0.6948\n",
      "  Recall:    0.6636\n",
      "  F1 Score:  0.6775\n",
      "\n",
      "Fold 1 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8096    0.7515    0.7795       515\n",
      "           1     0.2601    0.3309    0.2913       136\n",
      "\n",
      "    accuracy                         0.6636       651\n",
      "   macro avg     0.5349    0.5412    0.5354       651\n",
      "weighted avg     0.6948    0.6636    0.6775       651\n",
      "\n",
      "\n",
      "===== DeepGNN - Fold 2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=0.6938, Acc=0.7236 | Val Loss=0.6693, Acc=0.7250\n",
      "Epoch 02: Train Loss=0.6750, Acc=0.7053 | Val Loss=0.6639, Acc=0.6885\n",
      "Epoch 03: Train Loss=0.6785, Acc=0.7072 | Val Loss=0.6695, Acc=0.6615\n",
      "Epoch 04: Train Loss=0.6822, Acc=0.6861 | Val Loss=0.6631, Acc=0.6865\n",
      "Epoch 05: Train Loss=0.6724, Acc=0.6966 | Val Loss=0.6644, Acc=0.7192\n",
      "Epoch 06: Train Loss=0.6760, Acc=0.7000 | Val Loss=0.6690, Acc=0.7115\n",
      "Epoch 07: Train Loss=0.6762, Acc=0.7091 | Val Loss=0.6626, Acc=0.6865\n",
      "Epoch 08: Train Loss=0.6638, Acc=0.7115 | Val Loss=0.6628, Acc=0.6904\n",
      "Epoch 09: Train Loss=0.6591, Acc=0.7120 | Val Loss=0.6645, Acc=0.6423\n",
      "Epoch 10: Train Loss=0.6666, Acc=0.7168 | Val Loss=0.6657, Acc=0.6558\n",
      "Epoch 11: Train Loss=0.6663, Acc=0.7010 | Val Loss=0.6704, Acc=0.7115\n",
      "Epoch 12: Train Loss=0.6567, Acc=0.7183 | Val Loss=0.6695, Acc=0.6308\n",
      "Epoch 13: Train Loss=0.6579, Acc=0.7192 | Val Loss=0.6699, Acc=0.6615\n",
      "Epoch 14: Train Loss=0.6433, Acc=0.7236 | Val Loss=0.6686, Acc=0.7000\n",
      "Epoch 15: Train Loss=0.6536, Acc=0.7433 | Val Loss=0.6696, Acc=0.6942\n",
      "Epoch 16: Train Loss=0.6409, Acc=0.7495 | Val Loss=0.6797, Acc=0.6673\n",
      "Epoch 17: Train Loss=0.6417, Acc=0.7279 | Val Loss=0.6782, Acc=0.6904\n",
      "Epoch 18: Train Loss=0.6321, Acc=0.7596 | Val Loss=0.6837, Acc=0.6577\n",
      "Epoch 19: Train Loss=0.6211, Acc=0.7611 | Val Loss=0.6900, Acc=0.6769\n",
      "Epoch 20: Train Loss=0.6136, Acc=0.7601 | Val Loss=0.6960, Acc=0.6904\n",
      "Epoch 21: Train Loss=0.6165, Acc=0.7659 | Val Loss=0.7004, Acc=0.6712\n",
      "Epoch 22: Train Loss=0.6031, Acc=0.7639 | Val Loss=0.7040, Acc=0.6904\n",
      "Early stopping triggered after 22 epochs\n",
      "\n",
      "Fold 2 Test Metrics:\n",
      "  Accuracy:  0.6805\n",
      "  Precision: 0.6987\n",
      "  Recall:    0.6805\n",
      "  F1 Score:  0.6890\n",
      "\n",
      "Fold 2 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8114    0.7767    0.7937       515\n",
      "           1     0.2722    0.3162    0.2925       136\n",
      "\n",
      "    accuracy                         0.6805       651\n",
      "   macro avg     0.5418    0.5464    0.5431       651\n",
      "weighted avg     0.6987    0.6805    0.6890       651\n",
      "\n",
      "\n",
      "===== DeepGNN - Fold 3 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=0.6907, Acc=0.5183 | Val Loss=0.6572, Acc=0.7077\n",
      "Epoch 02: Train Loss=0.6742, Acc=0.7024 | Val Loss=0.6640, Acc=0.6538\n",
      "Epoch 03: Train Loss=0.6713, Acc=0.6822 | Val Loss=0.6504, Acc=0.6885\n",
      "Epoch 04: Train Loss=0.6707, Acc=0.6841 | Val Loss=0.6476, Acc=0.7308\n",
      "Epoch 05: Train Loss=0.6756, Acc=0.6861 | Val Loss=0.6621, Acc=0.7538\n",
      "Epoch 06: Train Loss=0.6692, Acc=0.7005 | Val Loss=0.6485, Acc=0.7327\n",
      "Epoch 07: Train Loss=0.6656, Acc=0.6707 | Val Loss=0.6577, Acc=0.6615\n",
      "Epoch 08: Train Loss=0.6631, Acc=0.6875 | Val Loss=0.6556, Acc=0.7154\n",
      "Epoch 09: Train Loss=0.6673, Acc=0.6981 | Val Loss=0.6549, Acc=0.6846\n",
      "Epoch 10: Train Loss=0.6680, Acc=0.7096 | Val Loss=0.6618, Acc=0.7038\n",
      "Epoch 11: Train Loss=0.6637, Acc=0.6788 | Val Loss=0.6518, Acc=0.7308\n",
      "Epoch 12: Train Loss=0.6494, Acc=0.7019 | Val Loss=0.6653, Acc=0.6635\n",
      "Epoch 13: Train Loss=0.6508, Acc=0.7264 | Val Loss=0.6629, Acc=0.7635\n",
      "Epoch 14: Train Loss=0.6507, Acc=0.7192 | Val Loss=0.6662, Acc=0.7269\n",
      "Epoch 15: Train Loss=0.6422, Acc=0.7385 | Val Loss=0.6605, Acc=0.7154\n",
      "Epoch 16: Train Loss=0.6537, Acc=0.7130 | Val Loss=0.6661, Acc=0.7115\n",
      "Epoch 17: Train Loss=0.6527, Acc=0.7274 | Val Loss=0.6594, Acc=0.6942\n",
      "Epoch 18: Train Loss=0.6528, Acc=0.7284 | Val Loss=0.6644, Acc=0.7231\n",
      "Epoch 19: Train Loss=0.6342, Acc=0.7341 | Val Loss=0.6931, Acc=0.6135\n",
      "Early stopping triggered after 19 epochs\n",
      "\n",
      "Fold 3 Test Metrics:\n",
      "  Accuracy:  0.5653\n",
      "  Precision: 0.6865\n",
      "  Recall:    0.5653\n",
      "  F1 Score:  0.6047\n",
      "\n",
      "Fold 3 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8069    0.5922    0.6831       515\n",
      "           1     0.2308    0.4632    0.3081       136\n",
      "\n",
      "    accuracy                         0.5653       651\n",
      "   macro avg     0.5188    0.5277    0.4956       651\n",
      "weighted avg     0.6865    0.5653    0.6047       651\n",
      "\n",
      "\n",
      "===== DeepGNN - Fold 4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=0.6872, Acc=0.6178 | Val Loss=0.6815, Acc=0.7038\n",
      "Epoch 02: Train Loss=0.6809, Acc=0.7029 | Val Loss=0.6778, Acc=0.6942\n",
      "Epoch 03: Train Loss=0.6738, Acc=0.6966 | Val Loss=0.6759, Acc=0.6923\n",
      "Epoch 04: Train Loss=0.6705, Acc=0.7236 | Val Loss=0.6756, Acc=0.6692\n",
      "Epoch 05: Train Loss=0.6682, Acc=0.6880 | Val Loss=0.6770, Acc=0.6981\n",
      "Epoch 06: Train Loss=0.6637, Acc=0.7067 | Val Loss=0.6726, Acc=0.6654\n",
      "Epoch 07: Train Loss=0.6649, Acc=0.6764 | Val Loss=0.6702, Acc=0.6654\n",
      "Epoch 08: Train Loss=0.6553, Acc=0.6918 | Val Loss=0.6695, Acc=0.6712\n",
      "Epoch 09: Train Loss=0.6586, Acc=0.7091 | Val Loss=0.6743, Acc=0.6731\n",
      "Epoch 10: Train Loss=0.6570, Acc=0.6990 | Val Loss=0.6740, Acc=0.6827\n",
      "Epoch 11: Train Loss=0.6562, Acc=0.7154 | Val Loss=0.6774, Acc=0.7000\n",
      "Epoch 12: Train Loss=0.6595, Acc=0.7130 | Val Loss=0.6727, Acc=0.6865\n",
      "Epoch 13: Train Loss=0.6516, Acc=0.6923 | Val Loss=0.6783, Acc=0.6942\n",
      "Epoch 14: Train Loss=0.6541, Acc=0.7288 | Val Loss=0.6763, Acc=0.6577\n",
      "Epoch 15: Train Loss=0.6501, Acc=0.7274 | Val Loss=0.6776, Acc=0.6712\n",
      "Epoch 16: Train Loss=0.6489, Acc=0.7063 | Val Loss=0.6802, Acc=0.6846\n",
      "Epoch 17: Train Loss=0.6373, Acc=0.7255 | Val Loss=0.6821, Acc=0.6692\n",
      "Epoch 18: Train Loss=0.6368, Acc=0.7409 | Val Loss=0.6924, Acc=0.6462\n",
      "Epoch 19: Train Loss=0.6364, Acc=0.7221 | Val Loss=0.6931, Acc=0.6981\n",
      "Epoch 20: Train Loss=0.6257, Acc=0.7510 | Val Loss=0.6931, Acc=0.6365\n",
      "Epoch 21: Train Loss=0.6303, Acc=0.7486 | Val Loss=0.6907, Acc=0.6788\n",
      "Epoch 22: Train Loss=0.6236, Acc=0.7375 | Val Loss=0.6960, Acc=0.7000\n",
      "Epoch 23: Train Loss=0.6166, Acc=0.7639 | Val Loss=0.6991, Acc=0.6731\n",
      "Early stopping triggered after 23 epochs\n",
      "\n",
      "Fold 4 Test Metrics:\n",
      "  Accuracy:  0.6452\n",
      "  Precision: 0.6939\n",
      "  Recall:    0.6452\n",
      "  F1 Score:  0.6655\n",
      "\n",
      "Fold 4 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8100    0.7204    0.7626       515\n",
      "           1     0.2539    0.3603    0.2979       136\n",
      "\n",
      "    accuracy                         0.6452       651\n",
      "   macro avg     0.5320    0.5403    0.5302       651\n",
      "weighted avg     0.6939    0.6452    0.6655       651\n",
      "\n",
      "\n",
      "===== DeepGNN - Fold 5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\newenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=0.6954, Acc=0.3611 | Val Loss=0.6806, Acc=0.2519\n",
      "Epoch 02: Train Loss=0.6874, Acc=0.4702 | Val Loss=0.6768, Acc=0.5615\n",
      "Epoch 03: Train Loss=0.6756, Acc=0.5663 | Val Loss=0.6806, Acc=0.7788\n",
      "Epoch 04: Train Loss=0.6773, Acc=0.6447 | Val Loss=0.6677, Acc=0.7404\n",
      "Epoch 05: Train Loss=0.6767, Acc=0.6058 | Val Loss=0.6587, Acc=0.6962\n",
      "Epoch 06: Train Loss=0.6749, Acc=0.6308 | Val Loss=0.6582, Acc=0.6981\n",
      "Epoch 07: Train Loss=0.6735, Acc=0.6587 | Val Loss=0.6560, Acc=0.7096\n",
      "Epoch 08: Train Loss=0.6725, Acc=0.6827 | Val Loss=0.6579, Acc=0.7135\n",
      "Epoch 09: Train Loss=0.6695, Acc=0.6813 | Val Loss=0.6534, Acc=0.6750\n",
      "Epoch 10: Train Loss=0.6618, Acc=0.6740 | Val Loss=0.6531, Acc=0.6846\n",
      "Epoch 11: Train Loss=0.6602, Acc=0.6957 | Val Loss=0.6555, Acc=0.6962\n",
      "Epoch 12: Train Loss=0.6583, Acc=0.7034 | Val Loss=0.6542, Acc=0.6788\n",
      "Epoch 13: Train Loss=0.6533, Acc=0.7106 | Val Loss=0.6614, Acc=0.7385\n",
      "Epoch 14: Train Loss=0.6418, Acc=0.7284 | Val Loss=0.6569, Acc=0.7000\n",
      "Epoch 15: Train Loss=0.6383, Acc=0.7163 | Val Loss=0.6636, Acc=0.7346\n",
      "Epoch 16: Train Loss=0.6308, Acc=0.7351 | Val Loss=0.6670, Acc=0.6904\n",
      "Epoch 17: Train Loss=0.6278, Acc=0.7495 | Val Loss=0.6710, Acc=0.6577\n",
      "Epoch 18: Train Loss=0.6169, Acc=0.7490 | Val Loss=0.6840, Acc=0.7673\n",
      "Epoch 19: Train Loss=0.6222, Acc=0.7553 | Val Loss=0.6736, Acc=0.6942\n",
      "Epoch 20: Train Loss=0.6085, Acc=0.7394 | Val Loss=0.6860, Acc=0.6981\n",
      "Epoch 21: Train Loss=0.6068, Acc=0.7548 | Val Loss=0.6862, Acc=0.6962\n",
      "Epoch 22: Train Loss=0.5903, Acc=0.7822 | Val Loss=0.6917, Acc=0.6846\n",
      "Epoch 23: Train Loss=0.5879, Acc=0.7577 | Val Loss=0.6903, Acc=0.7096\n",
      "Epoch 24: Train Loss=0.5767, Acc=0.7846 | Val Loss=0.6991, Acc=0.6846\n",
      "Epoch 25: Train Loss=0.5739, Acc=0.7764 | Val Loss=0.7008, Acc=0.6827\n",
      "Early stopping triggered after 25 epochs\n",
      "\n",
      "Fold 5 Test Metrics:\n",
      "  Accuracy:  0.6528\n",
      "  Precision: 0.6969\n",
      "  Recall:    0.6528\n",
      "  F1 Score:  0.6715\n",
      "\n",
      "Fold 5 Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8121    0.7301    0.7689       515\n",
      "           1     0.2606    0.3603    0.3025       136\n",
      "\n",
      "    accuracy                         0.6528       651\n",
      "   macro avg     0.5364    0.5452    0.5357       651\n",
      "weighted avg     0.6969    0.6528    0.6715       651\n",
      "\n",
      "\n",
      "===== Final Combined Results (Majority Vote) for DeepGNN =====\n",
      "Final Accuracy:  0.6774\n",
      "Final Precision: 0.7038\n",
      "Final Recall:    0.6774\n",
      "Final F1 Score:  0.6892\n",
      "\n",
      "Final Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8157    0.7650    0.7896       515\n",
      "           1     0.2798    0.3456    0.3092       136\n",
      "\n",
      "    accuracy                         0.6774       651\n",
      "   macro avg     0.5477    0.5553    0.5494       651\n",
      "weighted avg     0.7038    0.6774    0.6892       651\n",
      "\n",
      "\n",
      "DeepGNN - Final Majority Vote Results:\n",
      "  Accuracy:  0.6774\n",
      "  Precision: 0.7038\n",
      "  Recall:    0.6774\n",
      "  F1 Score:  0.6892\n",
      "\n",
      "================================================================================\n",
      "FINAL SUMMARY TABLE - ALL MODELS\n",
      "================================================================================\n",
      "Model                     Accuracy   Precision  Recall     F1-Score  \n",
      "--------------------------------------------------------------------------------\n",
      "BaselineGCN               0.6375     0.7037     0.6375     0.6630    \n",
      "BaselineGAT               0.6313     0.7079     0.6313     0.6595    \n",
      "BaselineSAGE              0.6283     0.7100     0.6283     0.6576    \n",
      "ResidualGCN               0.6912     0.6887     0.6912     0.6900    \n",
      "HybridModel               0.6267     0.7127     0.6267     0.6570    \n",
      "RegularizedGNN            0.6298     0.7089     0.6298     0.6586    \n",
      "LightweightGCN            0.6344     0.7074     0.6344     0.6617    \n",
      "BalancedGAT               0.6190     0.7251     0.6190     0.6526    \n",
      "MultiGNN                  0.6436     0.7029     0.6436     0.6671    \n",
      "ResidualAttentionGNN      0.6436     0.7109     0.6436     0.6691    \n",
      "DeepGNN                   0.6774     0.7038     0.6774     0.6892    \n",
      "================================================================================\n",
      "\n",
      "Results saved to GNN_Models_Summary.csv\n",
      "Total models evaluated: 11\n"
     ]
    }
   ],
   "source": [
    "def calculate_class_weights(labels):\n",
    "    \"\"\"Calculate class weights for imbalanced dataset\"\"\"\n",
    "    class_counts = Counter(labels)\n",
    "    total_samples = len(labels)\n",
    "    weights = {cls: total_samples / (len(class_counts) * count) \n",
    "              for cls, count in class_counts.items()}\n",
    "    return torch.tensor([weights[i] for i in range(len(class_counts))])\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, device, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            loss = criterion(out, data.y)\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "            \n",
    "            pred = out.argmax(dim=1)\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_labels.extend(data.y.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision_per_class = precision_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "    recall_per_class = recall_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "    f1_per_class = f1_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(data_loader.dataset),\n",
    "        'accuracy': accuracy,\n",
    "        'precision_per_class': precision_per_class,\n",
    "        'recall_per_class': recall_per_class,\n",
    "        'f1_per_class': f1_per_class,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "def print_class_metrics(results, model_name, fold):\n",
    "    print(f\"\\nDetailed Class Metrics for {model_name} - Fold {fold + 1}:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Overall Accuracy: {results['accuracy']:.4f}\")\n",
    "    print(\"\\nPer-Class Metrics:\")\n",
    "    print(\"-\" * 30)\n",
    "    for i in range(len(results['precision_per_class'])):\n",
    "        print(f\"Class {i}:\")\n",
    "        print(f\"  Precision: {results['precision_per_class'][i]:.4f}\")\n",
    "        print(f\"  Recall:    {results['recall_per_class'][i]:.4f}\")\n",
    "        print(f\"  F1 Score:  {results['f1_per_class'][i]:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(results['confusion_matrix'])\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "\n",
    "def train_and_evaluate(model_class, model_name, train_graphs, test_graphs, device, epochs=100, patience=15):\n",
    "    graphs = [item['graph'] for item in train_graphs]\n",
    "    labels = [item['label'] for item in train_graphs]\n",
    "    for g, y in zip(graphs, labels):\n",
    "        g.y = torch.tensor([y], dtype=torch.long)\n",
    "\n",
    "    test_graphs_copy = []\n",
    "    for item in test_graphs:\n",
    "        g = item['graph']\n",
    "        g.y = torch.tensor([item['label']], dtype=torch.long)\n",
    "        test_graphs_copy.append(g)\n",
    "    test_loader_full = DataLoader(test_graphs_copy, batch_size=32, shuffle=False)\n",
    "    true_test_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader_full:\n",
    "            true_test_labels.extend(data.y.cpu().numpy())\n",
    "    true_test_labels = np.array(true_test_labels)\n",
    "\n",
    "    class_weights = calculate_class_weights(labels).to(device)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_predictions = []  \n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(graphs)), labels)):\n",
    "        print(f\"\\n===== {model_name} - Fold {fold + 1} =====\")\n",
    "\n",
    "        train_subset = [graphs[i] for i in train_idx]\n",
    "        val_subset = [graphs[i] for i in val_idx]\n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=32, shuffle=True, drop_last=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "        test_loader = DataLoader(test_graphs_copy, batch_size=32, shuffle=False)\n",
    "\n",
    "        in_channels = graphs[0].x.size(1)\n",
    "        out_channels = len(set(labels))\n",
    "        model = model_class(in_channels=in_channels, hidden_channels=32, out_channels=out_channels, dropout=0.5).to(device)\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)\n",
    "        criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        best_model_state = None\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            for data in train_loader:\n",
    "                data = data.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(data.x, data.edge_index, data.batch)\n",
    "                loss = criterion(out, data.y)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item() * data.num_graphs\n",
    "                pred = out.argmax(dim=1)\n",
    "                correct += (pred == data.y).sum().item()\n",
    "                total += data.num_graphs\n",
    "\n",
    "            train_loss = total_loss / len(train_loader.dataset)\n",
    "            train_acc = correct / total\n",
    "\n",
    "            val_results = evaluate_model(model, val_loader, device, criterion)\n",
    "            scheduler.step(val_results['accuracy'])\n",
    "\n",
    "            print(f\"Epoch {epoch+1:02d}: Train Loss={train_loss:.4f}, Acc={train_acc:.4f} | \"\n",
    "                  f\"Val Loss={val_results['loss']:.4f}, Acc={val_results['accuracy']:.4f}\")\n",
    "\n",
    "            if val_results['loss'] < best_val_loss:\n",
    "                best_val_loss = val_results['loss']\n",
    "                best_model_state = model.state_dict()\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "\n",
    "        model.load_state_dict(best_model_state)\n",
    "        model.eval()\n",
    "        preds_fold = []\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                data = data.to(device)\n",
    "                out = model(data.x, data.edge_index, data.batch)\n",
    "                preds = out.argmax(dim=1)\n",
    "                preds_fold.extend(preds.cpu().numpy())\n",
    "\n",
    "        preds_fold = np.array(preds_fold)\n",
    "        fold_predictions.append(preds_fold)\n",
    "\n",
    "        fold_acc = accuracy_score(true_test_labels, preds_fold)\n",
    "        fold_prec = precision_score(true_test_labels, preds_fold, average='weighted', zero_division=0)\n",
    "        fold_rec = recall_score(true_test_labels, preds_fold, average='weighted', zero_division=0)\n",
    "        fold_f1 = f1_score(true_test_labels, preds_fold, average='weighted', zero_division=0)\n",
    "\n",
    "        print(f\"\\nFold {fold + 1} Test Metrics:\")\n",
    "        print(f\"  Accuracy:  {fold_acc:.4f}\")\n",
    "        print(f\"  Precision: {fold_prec:.4f}\")\n",
    "        print(f\"  Recall:    {fold_rec:.4f}\")\n",
    "        print(f\"  F1 Score:  {fold_f1:.4f}\")\n",
    "        print(f\"\\nFold {fold + 1} Test Classification Report:\")\n",
    "        print(classification_report(true_test_labels, preds_fold, digits=4))\n",
    "\n",
    "    pred_matrix = np.vstack(fold_predictions)           \n",
    "    def majority_vote_col(col):\n",
    "        return np.bincount(col).argmax()\n",
    "    majority_preds = np.apply_along_axis(majority_vote_col, 0, pred_matrix)\n",
    "\n",
    "    final_acc = accuracy_score(true_test_labels, majority_preds)\n",
    "    final_prec = precision_score(true_test_labels, majority_preds, average='weighted', zero_division=0)\n",
    "    final_rec = recall_score(true_test_labels, majority_preds, average='weighted', zero_division=0)\n",
    "    final_f1 = f1_score(true_test_labels, majority_preds, average='weighted', zero_division=0)\n",
    "\n",
    "    print(f\"\\n===== Final Combined Results (Majority Vote) for {model_name} =====\")\n",
    "    print(f\"Final Accuracy:  {final_acc:.4f}\")\n",
    "    print(f\"Final Precision: {final_prec:.4f}\")\n",
    "    print(f\"Final Recall:    {final_rec:.4f}\")\n",
    "    print(f\"Final F1 Score:  {final_f1:.4f}\")\n",
    "    print(f\"\\nFinal Classification Report:\")\n",
    "    print(classification_report(true_test_labels, majority_preds, digits=4))\n",
    "\n",
    "    return majority_preds.tolist(), true_test_labels.tolist()\n",
    "\n",
    "def main():\n",
    "    print(\"\\n=== Loading Data ===\")\n",
    "    graph_data = torch.load(r\"eeg_graphs_ordered.pt\", weights_only=False)\n",
    "    Xtrain = pd.read_csv(r\"X_train.csv\")\n",
    "    train_indices = set(Xtrain['index'])\n",
    "\n",
    "    train_graphs = [graph_data[i] for i in train_indices]\n",
    "    test_graphs = [graph_data[i] for i in range(len(graph_data)) if i not in train_indices]\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    models = {\n",
    "        'BaselineGCN': BaselineGCN,\n",
    "        'BaselineGAT': BaselineGAT,\n",
    "        'BaselineSAGE': BaselineSAGE,\n",
    "        'ResidualGCN': ResidualGCN,\n",
    "        'HybridModel': HybridModel,\n",
    "        'RegularizedGNN': RegularizedGNN,\n",
    "        'LightweightGCN': LightweightGCN,\n",
    "        'BalancedGAT': BalancedGAT,\n",
    "        'MultiGNN': MultiGNN,\n",
    "        'ResidualAttentionGNN': ResidualAttentionGNN,\n",
    "        'DeepGNN': DeepGNN\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    summary_results = []\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY OF ALL MODELS - MAJORITY VOTE RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for model_name, model_class in models.items():\n",
    "        print(f\"\\n=== Testing {model_name} Model ===\")\n",
    "        preds_majority, trues = train_and_evaluate(model_class, model_name, train_graphs, test_graphs, device)\n",
    "        results[model_name] = {\n",
    "            'majority_preds': preds_majority,\n",
    "            'true': trues\n",
    "        }\n",
    "\n",
    "        acc = accuracy_score(trues, preds_majority)\n",
    "        prec = precision_score(trues, preds_majority, average='weighted', zero_division=0)\n",
    "        rec = recall_score(trues, preds_majority, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(trues, preds_majority, average='weighted', zero_division=0)\n",
    "        report = classification_report(trues, preds_majority, digits=4, zero_division=0)\n",
    "\n",
    "        print(f\"\\n{model_name} - Final Majority Vote Results:\")\n",
    "        print(f\"  Accuracy:  {acc:.4f}\")\n",
    "        print(f\"  Precision: {prec:.4f}\")\n",
    "        print(f\"  Recall:    {rec:.4f}\")\n",
    "        print(f\"  F1 Score:  {f1:.4f}\")\n",
    "\n",
    "        summary_results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Voting\": \"majority_5fold\",\n",
    "            \"Test Accuracy\": acc,\n",
    "            \"Test Precision\": prec,\n",
    "            \"Test Recall\": rec,\n",
    "            \"Test F1\": f1,\n",
    "            \"Classification Report\": report\n",
    "        })\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL SUMMARY TABLE - ALL MODELS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Model':<25} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    for result in summary_results:\n",
    "        print(f\"{result['Model']:<25} {result['Test Accuracy']:<10.4f} {result['Test Precision']:<10.4f} {result['Test Recall']:<10.4f} {result['Test F1']:<10.4f}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    df_summary = pd.DataFrame(summary_results)\n",
    "    df_summary.to_csv(\"GNN_Models_Summary.csv\", index=False)\n",
    "    print(f\"\\nResults saved to GNN_Models_Summary.csv\")\n",
    "    print(f\"Total models evaluated: {len(summary_results)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6007fa4d-9120-4752-90e9-6e8f3227bbcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (newenv)",
   "language": "python",
   "name": "newenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
